"""
Data Processor for Control Groups
ëŒ€ì¡°êµ° ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì²˜ë¦¬
"""

import os
import json
import torch
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import AutoTokenizer, AutoProcessor
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from typing import Dict, List, Tuple, Optional, Union
import numpy as np
from PIL import Image
import librosa
import pandas as pd
from collections import defaultdict, Counter

from config import ControlGroupConfig

class ControlGroupDataset(Dataset):
    """ëŒ€ì¡°êµ° ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 data: List[Dict], 
                 config: ControlGroupConfig,
                 tokenizer: Optional[AutoTokenizer] = None,
                 processor: Optional[AutoProcessor] = None,
                 mode: str = "multimodal"):
        """
        Args:
            data: ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            config: ì„¤ì •
            tokenizer: í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì € (Text-only, Concatì—ì„œ ì‚¬ìš©)
            processor: ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ (Audio-only, Concatì—ì„œ ì‚¬ìš©)
            mode: "audio_only", "text_only", "multimodal"
        """
        self.data = data
        self.config = config
        self.tokenizer = tokenizer
        self.processor = processor
        self.mode = mode
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        item = self.data[idx]
        
        result = {
            'label': torch.tensor(item['label'], dtype=torch.long),
            'language': item['language'],
            'patient_id': item.get('patient_id', item['file_id']),
            'file_id': item['file_id']
        }
        
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ (Audio-only, Multimodal)
        if self.mode in ["audio_only", "multimodal"] and self.processor is not None:
            try:
                # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ ë¡œë“œ
                spectrogram_path = item['spectrogram_path']
                if os.path.exists(spectrogram_path):
                    image = Image.open(spectrogram_path).convert('RGB')
                    pixel_values = self.processor(images=image, return_tensors="pt")['pixel_values'][0]
                    result['pixel_values'] = pixel_values
                else:
                    # ë”ë¯¸ ì´ë¯¸ì§€ (224x224x3)
                    result['pixel_values'] = torch.zeros((3, 224, 224), dtype=torch.float32)
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                result['pixel_values'] = torch.zeros((3, 224, 224), dtype=torch.float32)
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Text-only, Multimodal)
        if self.mode in ["text_only", "multimodal"] and self.tokenizer is not None:
            try:
                text = item['text']
                if text and text.strip():
                    # í† í°í™”
                    encoded = self.tokenizer(
                        text,
                        max_length=self.config.max_seq_length,
                        padding='max_length',
                        truncation=True,
                        return_tensors='pt'
                    )
                    result['input_ids'] = encoded['input_ids'][0]
                    result['attention_mask'] = encoded['attention_mask'][0]
                else:
                    # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                    result['input_ids'] = torch.zeros(self.config.max_seq_length, dtype=torch.long)
                    result['attention_mask'] = torch.zeros(self.config.max_seq_length, dtype=torch.long)
            except Exception as e:
                print(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                result['input_ids'] = torch.zeros(self.config.max_seq_length, dtype=torch.long)
                result['attention_mask'] = torch.zeros(self.config.max_seq_length, dtype=torch.long)
        
        return result

def load_multilingual_data(data_dir: str, languages: List[str]) -> List[Dict]:
    """ë©€í‹°ë§ê¶ ë°ì´í„° ë¡œë“œ"""
    
    print(f"ğŸ“‚ ë‹¤êµ­ì–´ ë°ì´í„° ë¡œë“œ ì¤‘: {languages}")
    all_data = []
    
    for language in languages:
        lang_dir = os.path.join(data_dir, language)
        if not os.path.exists(lang_dir):
            print(f"âš ï¸ ì–¸ì–´ ë””ë ‰í† ë¦¬ ì—†ìŒ: {lang_dir}")
            continue
        
        print(f"  ğŸ“ {language} ë°ì´í„° ë¡œë“œ ì¤‘...")
        lang_data = load_language_data(lang_dir, language)
        all_data.extend(lang_data)
        print(f"    âœ… {len(lang_data)}ê°œ ìƒ˜í”Œ ë¡œë“œë¨")
    
    print(f"ğŸ“Š ì „ì²´ ë¡œë“œëœ ë°ì´í„°: {len(all_data)}ê°œ ìƒ˜í”Œ")
    
    # ì–¸ì–´ë³„ ë¶„í¬
    lang_counts = Counter([item['language'] for item in all_data])
    print("ğŸ“ˆ ì–¸ì–´ë³„ ë¶„í¬:")
    for lang, count in lang_counts.items():
        print(f"  {lang}: {count}ê°œ")
    
    # ë¼ë²¨ë³„ ë¶„í¬
    label_counts = Counter([item['label'] for item in all_data])
    print("ğŸ“ˆ ë¼ë²¨ë³„ ë¶„í¬:")
    for label, count in label_counts.items():
        label_name = "ì •ìƒ" if label == 0 else "ì¹˜ë§¤"
        print(f"  {label_name}: {count}ê°œ")
    
    return all_data

def load_language_data(lang_dir: str, language: str) -> List[Dict]:
    """íŠ¹ì • ì–¸ì–´ ë°ì´í„° ë¡œë“œ"""
    
    data = []
    
    # í…ìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
    text_dir = os.path.join(lang_dir, 'textdata')
    voice_dir = os.path.join(lang_dir, 'voicedata')
    
    if not os.path.exists(text_dir):
        print(f"    âš ï¸ í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {text_dir}")
        return data
    
    # HC (ì •ìƒ) ë° AD (ì¹˜ë§¤) ë°ì´í„° ë¡œë“œ
    for label, label_name in [(0, 'HC'), (1, 'AD')]:
        text_label_dir = os.path.join(text_dir, label_name)
        voice_label_dir = os.path.join(voice_dir, label_name) if os.path.exists(voice_dir) else None
        
        if not os.path.exists(text_label_dir):
            continue
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
        for filename in os.listdir(text_label_dir):
            if not filename.endswith('.txt'):
                continue
            
            file_id = filename.replace('.txt', '')
            text_path = os.path.join(text_label_dir, filename)
            
            # í…ìŠ¤íŠ¸ ì½ê¸°
            try:
                with open(text_path, 'r', encoding='utf-8') as f:
                    text = f.read().strip()
            except:
                continue
            
            if not text:
                continue
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            spectrogram_path = None
            if voice_label_dir and os.path.exists(voice_label_dir):
                # ì—¬ëŸ¬ í™•ì¥ì ì‹œë„
                for ext in ['.wav', '.mp3', '.mp4', '.png', '.jpg']:
                    audio_file = os.path.join(voice_label_dir, f"{file_id}{ext}")
                    if os.path.exists(audio_file):
                        # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì €ì¥
                        if ext in ['.png', '.jpg']:
                            spectrogram_path = audio_file
                        else:
                            # ì˜¤ë””ì˜¤ íŒŒì¼ì„ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•œ ê²½ë¡œ (ê°€ì •)
                            spectrogram_path = audio_file.replace(ext, '_spectrogram.png')
                        break
            
            # í™˜ì ID ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
            patient_id = file_id.split('_')[0] if '_' in file_id else file_id
            
            data.append({
                'file_id': file_id,
                'patient_id': patient_id,
                'language': language,
                'label': label,
                'text': text,
                'text_path': text_path,
                'spectrogram_path': spectrogram_path or "",
                'audio_available': spectrogram_path is not None and os.path.exists(spectrogram_path)
            })
    
    return data

def create_stratified_split(
    dataset: List[Dict], 
    train_split: float = 0.7, 
    val_split: float = 0.1, 
    test_split: float = 0.2,
    random_seed: int = 42,
    split_by_patient: bool = True
) -> Tuple[List[int], List[int], List[int]]:
    """ê³„ì¸µí™”ëœ ë°ì´í„° ë¶„í• """
    
    if split_by_patient:
        print("ğŸ‘¥ í™˜ì ë‹¨ìœ„ Stratified Split ìˆ˜í–‰ ì¤‘...")
        return create_patient_based_split(dataset, train_split, val_split, test_split, random_seed)
    else:
        print("ğŸ“„ ìƒ˜í”Œ ë‹¨ìœ„ Stratified Split ìˆ˜í–‰ ì¤‘...")
        return create_sample_based_split(dataset, train_split, val_split, test_split, random_seed)

def create_patient_based_split(
    dataset: List[Dict], 
    train_split: float = 0.7, 
    val_split: float = 0.1, 
    test_split: float = 0.2,
    random_seed: int = 42
) -> Tuple[List[int], List[int], List[int]]:
    """í™˜ì ë‹¨ìœ„ ê³„ì¸µí™” ë¶„í• """
    
    # í™˜ìë³„ ë°ì´í„° ê·¸ë£¹í•‘
    patient_groups = defaultdict(list)
    for idx, item in enumerate(dataset):
        key = f"{item['patient_id']}_{item['language']}_{item['label']}"
        patient_groups[key].append(idx)
    
    # ê³„ì¸µí™” í‚¤ ìƒì„±
    stratify_keys = []
    patient_indices = []
    
    for key, indices in patient_groups.items():
        _, language, label = key.rsplit('_', 2)
        stratify_key = f"{language}_{label}"
        stratify_keys.append(stratify_key)
        patient_indices.append(indices)
    
    print(f"  ì „ì²´ í™˜ì ê·¸ë£¹: {len(patient_indices)}ê°œ")
    print(f"  Stratify í‚¤ ë¶„í¬: {Counter(stratify_keys)}")
    
    # í™˜ì ê·¸ë£¹ ë¶„í• 
    if test_split > 0:
        # 3-way split
        train_patient_idx, temp_patient_idx = train_test_split(
            range(len(patient_indices)),
            test_size=(val_split + test_split),
            stratify=stratify_keys,
            random_state=random_seed
        )
        
        # val/test ë¶„í• ì„ ìœ„í•œ ìƒˆë¡œìš´ stratify í‚¤
        temp_stratify_keys = [stratify_keys[i] for i in temp_patient_idx]
        val_ratio = val_split / (val_split + test_split)
        
        val_patient_idx, test_patient_idx = train_test_split(
            temp_patient_idx,
            test_size=(1 - val_ratio),
            stratify=temp_stratify_keys,
            random_state=random_seed
        )
    else:
        # 2-way split (cross-lingual ëª¨ë“œ)
        train_patient_idx, val_patient_idx = train_test_split(
            range(len(patient_indices)),
            test_size=val_split,
            stratify=stratify_keys,
            random_state=random_seed
        )
        test_patient_idx = []
    
    # ìƒ˜í”Œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    train_indices = []
    for i in train_patient_idx:
        train_indices.extend(patient_indices[i])
    
    val_indices = []
    for i in val_patient_idx:
        val_indices.extend(patient_indices[i])
    
    test_indices = []
    for i in test_patient_idx:
        test_indices.extend(patient_indices[i])
    
    print(f"ğŸ“ˆ ë¶„í•  ê²°ê³¼:")
    print(f"  í›ˆë ¨: {len(train_indices)}ê°œ ìƒ˜í”Œ")
    print(f"  ê²€ì¦: {len(val_indices)}ê°œ ìƒ˜í”Œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ ìƒ˜í”Œ")
    
    return train_indices, val_indices, test_indices

def create_sample_based_split(
    dataset: List[Dict], 
    train_split: float = 0.7, 
    val_split: float = 0.1, 
    test_split: float = 0.2,
    random_seed: int = 42
) -> Tuple[List[int], List[int], List[int]]:
    """ìƒ˜í”Œ ë‹¨ìœ„ ê³„ì¸µí™” ë¶„í• """
    
    # ê³„ì¸µí™” í‚¤ ìƒì„±
    stratify_keys = []
    for item in dataset:
        stratify_key = f"{item['language']}_{item['label']}"
        stratify_keys.append(stratify_key)
    
    print(f"  ì „ì²´ ìƒ˜í”Œ: {len(dataset)}ê°œ")
    print(f"  Stratify í‚¤ ë¶„í¬: {Counter(stratify_keys)}")
    
    indices = list(range(len(dataset)))
    
    if test_split > 0:
        # 3-way split
        train_indices, temp_indices = train_test_split(
            indices,
            test_size=(val_split + test_split),
            stratify=stratify_keys,
            random_state=random_seed
        )
        
        temp_stratify_keys = [stratify_keys[i] for i in temp_indices]
        val_ratio = val_split / (val_split + test_split)
        
        val_indices, test_indices = train_test_split(
            temp_indices,
            test_size=(1 - val_ratio),
            stratify=temp_stratify_keys,
            random_state=random_seed
        )
    else:
        # 2-way split
        train_indices, val_indices = train_test_split(
            indices,
            test_size=val_split,
            stratify=stratify_keys,
            random_state=random_seed
        )
        test_indices = []
    
    print(f"ğŸ“ˆ ë¶„í•  ê²°ê³¼:")
    print(f"  í›ˆë ¨: {len(train_indices)}ê°œ ìƒ˜í”Œ")
    print(f"  ê²€ì¦: {len(val_indices)}ê°œ ìƒ˜í”Œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ ìƒ˜í”Œ")
    
    return train_indices, val_indices, test_indices

def compute_class_weights(dataset: Union[List[Dict], Subset], config: ControlGroupConfig) -> np.ndarray:
    """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
    
    if isinstance(dataset, Subset):
        labels = [dataset.dataset.data[i]['label'] for i in dataset.indices]
    else:
        labels = [item['label'] for item in dataset]
    
    unique_labels = np.unique(labels)
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=unique_labels,
        y=labels
    )
    
    print(f"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°:")
    for label, weight in zip(unique_labels, class_weights):
        label_name = "ì •ìƒ(HC)" if label == 0 else "ì¹˜ë§¤(AD)"
        print(f"  {label_name}: {weight:.4f}")
    
    return class_weights

def create_dataloaders(
    config: ControlGroupConfig,
    mode: str = "multimodal",
    tokenizer: Optional[AutoTokenizer] = None,
    processor: Optional[AutoProcessor] = None
) -> Tuple[DataLoader, DataLoader, Optional[DataLoader]]:
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    
    # ë°ì´í„° ë¡œë“œ
    all_data = load_multilingual_data(config.data_dir, config.languages)
    
    if len(all_data) == 0:
        raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    # ë°ì´í„° ë¶„í• 
    train_indices, val_indices, test_indices = create_stratified_split(
        all_data,
        split_by_patient=config.split_by_patient,
        random_seed=config.random_seed
    )
    
    # ë°ì´í„°ì…‹ ìƒì„±
    full_dataset = ControlGroupDataset(all_data, config, tokenizer, processor, mode)
    
    train_dataset = Subset(full_dataset, train_indices)
    val_dataset = Subset(full_dataset, val_indices)
    test_dataset = Subset(full_dataset, test_indices) if test_indices else None
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    test_loader = None
    if test_dataset:
        test_loader = DataLoader(
            test_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
    
    return train_loader, val_loader, test_loader
