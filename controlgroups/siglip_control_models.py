"""
SigLIP ê¸°ë°˜ ëŒ€ì¡°êµ° ëª¨ë¸ë“¤
SigLIPì˜ ê°œë³„ ì»´í¬ë„ŒíŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ ê³µì •í•œ ë¹„êµ ì œê³µ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple
from transformers import AutoModel, AutoTokenizer, AutoProcessor
from config import ControlGroupConfig, AudioOnlyConfig, TextOnlyConfig, ConcatConfig

class FocalLoss(nn.Module):
    """Focal Loss for imbalanced classification"""
    
    def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets.float(), reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class SigLIPAudioOnlyModel(nn.Module):
    """
    SigLIP-Audio-Only: SigLIPì˜ ì´ë¯¸ì§€ ì¸ì½”ë”ë§Œ ì‚¬ìš©
    ë©œìŠ¤í™í† ê·¸ë¨ â†’ SigLIP Vision Encoder â†’ ë¶„ë¥˜ê¸°
    """
    
    def __init__(self, config: AudioOnlyConfig):
        super().__init__()
        self.config = config
        
        # SigLIP ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ì§€ ì¸ì½”ë”ë§Œ ì‚¬ìš©)
        print(f"ğŸ”„ SigLIP ëª¨ë¸ ë¡œë“œ: {config.siglip_model}")
        self.siglip_model = AutoModel.from_pretrained(config.siglip_model)
        print(f"âœ… SigLIP ë¡œë“œ ì™„ë£Œ!")
        
        # SigLIP ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ
        self.processor = AutoProcessor.from_pretrained(config.siglip_model)
        
        # SigLIPì˜ ì‹¤ì œ ì´ë¯¸ì§€ ì„ë² ë”© ì°¨ì› í™•ì¸
        self.hidden_size = self.siglip_model.config.vision_config.hidden_size
        print(f"ğŸ“Š SigLIP ì´ë¯¸ì§€ ì„ë² ë”© ì°¨ì›: {self.hidden_size}")
        
        # ë¶„ë¥˜ê¸° (SigLIP ì´ë¯¸ì§€ íŠ¹ì§• â†’ ë¶„ë¥˜)
        self.classifier = nn.Sequential(
            nn.Dropout(config.dropout),
            nn.Linear(self.hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(512, config.num_classes)
        )
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.setup_loss_function()
    
    def setup_loss_function(self, class_weights=None):
        """ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •"""
        if self.config.loss_type == "focal":
            alpha = class_weights[1] if class_weights is not None else self.config.focal_alpha
            self.criterion = FocalLoss(
                alpha=alpha, 
                gamma=self.config.focal_gamma, 
                reduction='mean'
            )
            print(f"ğŸ“Š Focal Loss ì‚¬ìš© (alpha={alpha:.3f}, gamma={self.config.focal_gamma})")
        else:
            if class_weights is not None:
                self.criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights))
                print(f"ğŸ“Š Weighted Cross Entropy Loss ì‚¬ìš©")
            else:
                self.criterion = nn.CrossEntropyLoss()
                print("ğŸ“Š Cross Entropy Loss ì‚¬ìš©")
    
    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ - ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬"""
        # SigLIP ì´ë¯¸ì§€ ì¸ì½”ë”ë§Œ ì‚¬ìš©
        with torch.no_grad():
            # SigLIPì˜ ì´ë¯¸ì§€ ì¸ì½”ë” í†µê³¼
            outputs = self.siglip_model.get_image_features(pixel_values=pixel_values)
        
        # ë¶„ë¥˜ê¸° í†µê³¼
        logits = self.classifier(outputs)
        return logits
    
    def compute_loss(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """ì†ì‹¤ ê³„ì‚°"""
        if self.config.num_classes == 2:
            # ì´ì§„ ë¶„ë¥˜: logitsì„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
            logits = logits[:, 1] - logits[:, 0]  # AD - HC
            return self.criterion(logits, labels.float())
        else:
            return self.criterion(logits, labels)

class SigLIPTextOnlyModel(nn.Module):
    """
    SigLIP-Text-Only: SigLIPì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë” + Gemma í† í¬ë‚˜ì´ì €
    í…ìŠ¤íŠ¸ â†’ Gemma í† í¬ë‚˜ì´ì € â†’ SigLIP Text Encoder â†’ ë¶„ë¥˜ê¸°
    """
    
    def __init__(self, config: TextOnlyConfig):
        super().__init__()
        self.config = config
        
        # SigLIP ëª¨ë¸ ë¡œë“œ (í…ìŠ¤íŠ¸ ì¸ì½”ë”ë§Œ ì‚¬ìš©)
        print(f"ğŸ”„ SigLIP ëª¨ë¸ ë¡œë“œ: {config.siglip_model}")
        self.siglip_model = AutoModel.from_pretrained(config.siglip_model)
        print(f"âœ… SigLIP ë¡œë“œ ì™„ë£Œ!")
        
        # Gemma í† í¬ë‚˜ì´ì € (SigLIPê³¼ ë™ì¼)
        print(f"ğŸ”„ Gemma í† í¬ë‚˜ì´ì € ë¡œë“œ: {config.text_tokenizer}")
        self.tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        print(f"âœ… Gemma í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ! (vocab_size: {self.tokenizer.vocab_size})")
        
        # SigLIPì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì› í™•ì¸
        self.hidden_size = self.siglip_model.config.text_config.hidden_size
        print(f"ğŸ“Š SigLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {self.hidden_size}")
        
        # ë¶„ë¥˜ê¸° (SigLIP í…ìŠ¤íŠ¸ íŠ¹ì§• â†’ ë¶„ë¥˜)
        self.classifier = nn.Sequential(
            nn.Dropout(config.dropout),
            nn.Linear(self.hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(512, config.num_classes)
        )
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.setup_loss_function()
    
    def setup_loss_function(self, class_weights=None):
        """ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •"""
        if self.config.loss_type == "focal":
            alpha = class_weights[1] if class_weights is not None else self.config.focal_alpha
            self.criterion = FocalLoss(
                alpha=alpha, 
                gamma=self.config.focal_gamma, 
                reduction='mean'
            )
            print(f"ğŸ“Š Focal Loss ì‚¬ìš© (alpha={alpha:.3f}, gamma={self.config.focal_gamma})")
        else:
            if class_weights is not None:
                self.criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights))
                print(f"ğŸ“Š Weighted Cross Entropy Loss ì‚¬ìš©")
            else:
                self.criterion = nn.CrossEntropyLoss()
                print("ğŸ“Š Cross Entropy Loss ì‚¬ìš©")
    
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ - í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬"""
        # SigLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”ë§Œ ì‚¬ìš©
        with torch.no_grad():
            # SigLIPì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë” í†µê³¼
            outputs = self.siglip_model.get_text_features(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
        
        # ë¶„ë¥˜ê¸° í†µê³¼
        logits = self.classifier(outputs)
        return logits
    
    def compute_loss(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """ì†ì‹¤ ê³„ì‚°"""
        if self.config.num_classes == 2:
            # ì´ì§„ ë¶„ë¥˜: logitsì„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
            logits = logits[:, 1] - logits[:, 0]  # AD - HC
            return self.criterion(logits, labels.float())
        else:
            return self.criterion(logits, labels)

class SigLIPConcatModel(nn.Module):
    """
    SigLIP-Concat: SigLIPì˜ ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¶„ë¦¬ í›„ ì—°ê²°
    ë©œìŠ¤í™í† ê·¸ë¨ â†’ SigLIP Vision + í…ìŠ¤íŠ¸ â†’ SigLIP Text â†’ Concat â†’ ë¶„ë¥˜ê¸°
    """
    
    def __init__(self, config: ConcatConfig):
        super().__init__()
        self.config = config
        
        # SigLIP ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì¸ì½”ë” ëª¨ë‘ ì‚¬ìš©)
        print(f"ğŸ”„ SigLIP ëª¨ë¸ ë¡œë“œ: {config.siglip_model}")
        self.siglip_model = AutoModel.from_pretrained(config.siglip_model)
        print(f"âœ… SigLIP ë¡œë“œ ì™„ë£Œ!")
        
        # SigLIP ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ
        self.processor = AutoProcessor.from_pretrained(config.siglip_model)
        
        # Gemma í† í¬ë‚˜ì´ì € (SigLIPê³¼ ë™ì¼)
        print(f"ğŸ”„ Gemma í† í¬ë‚˜ì´ì € ë¡œë“œ: {config.text_tokenizer}")
        self.tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        print(f"âœ… Gemma í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ! (vocab_size: {self.tokenizer.vocab_size})")
        
        # SigLIPì˜ ì‹¤ì œ ì„ë² ë”© ì°¨ì›ë“¤
        self.image_hidden_size = self.siglip_model.config.vision_config.hidden_size
        self.text_hidden_size = self.siglip_model.config.text_config.hidden_size
        self.fused_feature_dim = self.image_hidden_size + self.text_hidden_size
        
        print(f"ğŸ“Š SigLIP ì´ë¯¸ì§€ ì„ë² ë”© ì°¨ì›: {self.image_hidden_size}")
        print(f"ğŸ“Š SigLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {self.text_hidden_size}")
        print(f"ğŸ“Š ìœµí•©ëœ íŠ¹ì§• ì°¨ì›: {self.fused_feature_dim}")
        
        # ìœµí•©ëœ íŠ¹ì§•ì„ ìœ„í•œ ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Dropout(config.dropout),
            nn.Linear(self.fused_feature_dim, 1024),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(512, config.num_classes)
        )
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.setup_loss_function()
    
    def setup_loss_function(self, class_weights=None):
        """ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •"""
        if self.config.loss_type == "focal":
            alpha = class_weights[1] if class_weights is not None else self.config.focal_alpha
            self.criterion = FocalLoss(
                alpha=alpha, 
                gamma=self.config.focal_gamma, 
                reduction='mean'
            )
            print(f"ğŸ“Š Focal Loss ì‚¬ìš© (alpha={alpha:.3f}, gamma={self.config.focal_gamma})")
        else:
            if class_weights is not None:
                self.criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights))
                print(f"ğŸ“Š Weighted Cross Entropy Loss ì‚¬ìš©")
            else:
                self.criterion = nn.CrossEntropyLoss()
                print("ğŸ“Š Cross Entropy Loss ì‚¬ìš©")
    
    def forward(self, pixel_values: torch.Tensor, input_ids: torch.Tensor, 
                attention_mask: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ - ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì²˜ë¦¬ í›„ ì—°ê²°"""
        # SigLIP ì´ë¯¸ì§€ ì¸ì½”ë”
        with torch.no_grad():
            image_features = self.siglip_model.get_image_features(pixel_values=pixel_values)
        
        # SigLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”
        with torch.no_grad():
            text_features = self.siglip_model.get_text_features(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
        
        # íŠ¹ì§• ì—°ê²° (Late Fusion)
        fused_features = torch.cat([image_features, text_features], dim=1)
        
        # ë¶„ë¥˜ê¸° í†µê³¼
        logits = self.classifier(fused_features)
        return logits
    
    def compute_loss(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """ì†ì‹¤ ê³„ì‚°"""
        if self.config.num_classes == 2:
            # ì´ì§„ ë¶„ë¥˜: logitsì„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
            logits = logits[:, 1] - logits[:, 0]  # AD - HC
            return self.criterion(logits, labels.float())
        else:
            return self.criterion(logits, labels)

def compute_metrics(predictions: np.ndarray, labels: np.ndarray, probabilities: np.ndarray) -> Dict[str, float]:
    """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve
    
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)
    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)
    
    # AUC ê³„ì‚° (ì´ì§„ ë¶„ë¥˜ë§Œ)
    auc = 0.0
    optimal_threshold = 0.5
    if len(set(labels)) == 2:
        try:
            auc = roc_auc_score(labels, probabilities)
            fpr, tpr, thresholds = roc_curve(labels, probabilities)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
        except Exception as e:
            print(f"âš ï¸ AUC ê³„ì‚° ì‹¤íŒ¨: {e}")
            auc = 0.0
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'auc': auc,
        'optimal_threshold': optimal_threshold
    }

def compute_language_specific_metrics(predictions: np.ndarray, labels: np.ndarray, 
                                    probabilities: np.ndarray, languages: List[str],
                                    optimal_threshold: float = 0.5) -> Dict[str, Dict[str, float]]:
    """ì–¸ì–´ë³„ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
    
    lang_metrics = {}
    unique_languages = list(set(languages))
    
    for lang in unique_languages:
        lang_indices = [i for i, l in enumerate(languages) if l == lang]
        if len(lang_indices) == 0:
            continue
        
        lang_preds = predictions[lang_indices]
        lang_labels = labels[lang_indices]
        lang_probs = probabilities[lang_indices]
        
        # í•´ë‹¹ ì–¸ì–´ì— ë‘ í´ë˜ìŠ¤ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
        if len(set(lang_labels)) < 2:
            print(f"âš ï¸ {lang}: ë‹¨ì¼ í´ë˜ìŠ¤ë§Œ ì¡´ì¬, ì§€í‘œ ê³„ì‚° ì œí•œ")
            lang_metrics[lang] = {
                'accuracy': accuracy_score(lang_labels, lang_preds),
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'macro_f1': 0.0,
                'auc': 0.0
            }
            continue
        
        accuracy = accuracy_score(lang_labels, lang_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(lang_labels, lang_preds, average='weighted', zero_division=0)
        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(lang_labels, lang_preds, average='macro', zero_division=0)
        
        # AUC ê³„ì‚°
        auc = 0.0
        try:
            if len(set(lang_labels)) == 2:
                auc = roc_auc_score(lang_labels, lang_probs)
        except Exception as e:
            print(f"âš ï¸ {lang} AUC ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        lang_metrics[lang] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'macro_f1': macro_f1,
            'auc': auc
        }
    
    return lang_metrics
