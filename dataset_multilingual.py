import torch
from torch.utils.data import Dataset
import numpy as np
from pathlib import Path
from transformers import BertTokenizer
from collections import Counter, defaultdict
from sklearn.model_selection import train_test_split
import random

class MultilingualDementiaDataset(Dataset):
    def __init__(self, data):
        """
        ë‹¤êµ­ì–´ ì¹˜ë§¤ ë°ì´í„°ì…‹
        data: list of dicts with keys: 'text', 'audio_path', 'label', 'language', 'patient_id'
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë¡œë“œ
        try:
            audio_specs = np.load(item['audio_path'])
            # í¬ê¸° ì¡°ì •: (3, 128, 250)ìœ¼ë¡œ ë§ì¶”ê¸°
            if audio_specs.shape != (3, 128, 250):
                # íŒ¨ë”© ë˜ëŠ” í¬ë¡­ìœ¼ë¡œ í¬ê¸° ì¡°ì •
                audio_specs = self._resize_audio(audio_specs)
        except:
            # ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
            audio_specs = np.zeros((3, 128, 250), dtype=np.float32)
        
        return {
            'input_ids': item['input_ids'].squeeze(0),  # (max_seq_len,)
            'attention_mask': item['attention_mask'].squeeze(0),  # (max_seq_len,)
            'audio': torch.tensor(audio_specs, dtype=torch.float32),  # (3, 128, 250)
            'label': item['label'],
            'language': item['language'],
            'patient_id': item['patient_id']
        }
    
    def _resize_audio(self, audio_specs):
        """ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í¬ê¸° ì¡°ì •"""
        target_shape = (3, 128, 250)
        
        if len(audio_specs.shape) == 2:
            # (H, W) -> (3, H, W) ë³€í™˜
            audio_specs = np.stack([audio_specs] * 3, axis=0)
        
        current_shape = audio_specs.shape
        resized = np.zeros(target_shape, dtype=np.float32)
        
        # ê° ì°¨ì›ë³„ë¡œ í¬ê¸° ì¡°ì •
        h_min = min(current_shape[1], target_shape[1])
        w_min = min(current_shape[2], target_shape[2])
        
        resized[:, :h_min, :w_min] = audio_specs[:, :h_min, :w_min]
        
        return resized

def read_multilingual_data(data_dir, languages=None):
    """
    ë‹¤êµ­ì–´ ë°ì´í„° ë¡œë“œ
    data_dir: training_dset ê²½ë¡œ
    languages: ë¡œë“œí•  ì–¸ì–´ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì–¸ì–´)
    """
    if languages is None:
        languages = ['English', 'Greek', 'Spanish', 'Mandarin']
    
    all_data = []
    data_dir = Path(data_dir)
    
    print(f"ğŸ“‚ ë‹¤êµ­ì–´ ë°ì´í„° ë¡œë“œ ì¤‘: {languages}")
    
    for language in languages:
        lang_dir = data_dir / language
        if not lang_dir.exists():
            print(f"âš ï¸ ì–¸ì–´ ë””ë ‰í† ë¦¬ ì—†ìŒ: {lang_dir}")
            continue
            
        print(f"  ğŸ“ {language} ë°ì´í„° ë¡œë“œ ì¤‘...")
        lang_data = load_language_data(lang_dir, language)
        all_data.extend(lang_data)
        print(f"    âœ… {len(lang_data)}ê°œ ìƒ˜í”Œ ë¡œë“œë¨")
    
    print(f"ğŸ“Š ì „ì²´ ë¡œë“œëœ ë°ì´í„°: {len(all_data)}ê°œ ìƒ˜í”Œ")
    
    # ì–¸ì–´ë³„ í†µê³„
    lang_stats = Counter([item['language'] for item in all_data])
    label_stats = Counter([item['label'] for item in all_data])
    
    print(f"ğŸ“ˆ ì–¸ì–´ë³„ ë¶„í¬:")
    for lang, count in lang_stats.items():
        print(f"  {lang}: {count}ê°œ")
    
    print(f"ğŸ“ˆ ë¼ë²¨ë³„ ë¶„í¬:")
    label_names = {0: 'ì •ìƒ', 1: 'ì¹˜ë§¤'}
    for label, count in label_stats.items():
        print(f"  {label_names[label]}: {count}ê°œ")
    
    return all_data

def load_language_data(lang_dir, language):
    """íŠ¹ì • ì–¸ì–´ì˜ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # ì–¸ì–´ë³„ ë°ì´í„° êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
    if language == 'English':
        data = load_english_data(lang_dir, language)
    elif language == 'Greek':
        data = load_greek_data(lang_dir, language)
    elif language == 'Spanish':
        data = load_spanish_data(lang_dir, language)
    elif language == 'Mandarin':
        data = load_mandarin_data(lang_dir, language)
    
    return data

def load_english_data(lang_dir, language):
    """ì˜ì–´ ë°ì´í„° ë¡œë“œ (Pitt ì½”í¼ìŠ¤ ê¸°ë°˜)"""
    data = []
    
    # Pitt ë””ë ‰í† ë¦¬ íƒìƒ‰
    pitt_dir = lang_dir / 'Pitt'
    if pitt_dir.exists():
        data.extend(load_pitt_data(pitt_dir, language))
    
    # ë‹¤ë¥¸ ì˜ì–´ ë°ì´í„° ë””ë ‰í† ë¦¬ë“¤ë„ íƒìƒ‰
    for subdir in lang_dir.iterdir():
        if subdir.is_dir() and subdir.name != 'Pitt':
            # ì¼ë°˜ì ì¸ êµ¬ì¡°ë¡œ ë¡œë“œ ì‹œë„
            subdir_data = load_generic_data(subdir, language)
            data.extend(subdir_data)
    
    return data

def load_pitt_data(pitt_dir, language):
    """Pitt ì½”í¼ìŠ¤ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    text_dir = pitt_dir / 'textdata'
    voice_dir = pitt_dir / 'voicedata'
    
    if not (text_dir.exists() and voice_dir.exists()):
        return data
    
    # Controlê³¼ Dementia ë°ì´í„° ë¡œë“œ
    categories = [
        ('Control', 0),
        ('Dementia', 1)
    ]
    
    for cat_name, label in categories:
        cat_text_dir = text_dir / cat_name
        cat_voice_dir = voice_dir / cat_name
        
        if not (cat_text_dir.exists() and cat_voice_dir.exists()):
            continue
            
        # íŒŒì¼ ë§¤ì¹­
        for text_file in cat_text_dir.glob('**/*.txt'):
            # ìƒëŒ€ ê²½ë¡œ êµ¬ì„±
            rel_path = text_file.relative_to(cat_text_dir)
            audio_file = cat_voice_dir / rel_path.with_suffix('.npy')
            
            if audio_file.exists():
                # í…ìŠ¤íŠ¸ ì½ê¸°
                try:
                    with open(text_file, 'r', encoding='utf-8') as f:
                        text = f.read().strip()
                    
                    # í™˜ì ID ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
                    patient_id = f"{language}_{text_file.stem}"
                    
                    data.append({
                        'text': text,
                        'audio_path': str(audio_file),
                        'label': label,
                        'language': language,
                        'patient_id': patient_id
                    })
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    return data

def load_greek_data(lang_dir, language):
    """ê·¸ë¦¬ìŠ¤ì–´ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # Dem@Care ë””ë ‰í† ë¦¬ íƒìƒ‰
    demcare_dir = lang_dir / 'Dem@Care'
    if demcare_dir.exists():
        data.extend(load_generic_data(demcare_dir, language))
    
    # ë‹¤ë¥¸ í•˜ìœ„ ë””ë ‰í† ë¦¬ë“¤ë„ íƒìƒ‰
    for subdir in lang_dir.iterdir():
        if subdir.is_dir() and subdir.name != 'Dem@Care':
            subdir_data = load_generic_data(subdir, language)
            data.extend(subdir_data)
    
    return data

def load_spanish_data(lang_dir, language):
    """ìŠ¤í˜ì¸ì–´ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # Ivanova ë””ë ‰í† ë¦¬ íƒìƒ‰
    ivanova_dir = lang_dir / 'Ivanova'
    if ivanova_dir.exists():
        data.extend(load_generic_data(ivanova_dir, language))
    
    # AD, HC, MCI ë””ë ‰í† ë¦¬ë“¤ íƒìƒ‰
    categories = [
        ('HC', 0),      # Healthy Control
        ('AD', 1),      # Alzheimer's Disease
        ('MCI', 1)      # Mild Cognitive Impairment (ì¹˜ë§¤ë¡œ ë¶„ë¥˜)
    ]
    
    for cat_name, label in categories:
        cat_dir = lang_dir / cat_name
        if cat_dir.exists():
            cat_data = load_generic_data(cat_dir, language, default_label=label)
            data.extend(cat_data)
    
    return data

def load_mandarin_data(lang_dir, language):
    """ë§Œë‹¤ë¦° ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # ì—¬ëŸ¬ í•˜ìœ„ ë””ë ‰í† ë¦¬ íƒìƒ‰
    for subdir in lang_dir.iterdir():
        if subdir.is_dir():
            subdir_data = load_generic_data(subdir, language)
            data.extend(subdir_data)
    
    return data

def load_generic_data(data_dir, language, default_label=None):
    """ì¼ë°˜ì ì¸ ë°ì´í„° êµ¬ì¡° ë¡œë“œ"""
    data = []
    
    # textdataì™€ voicedata ë””ë ‰í† ë¦¬ ì°¾ê¸°
    text_dir = None
    voice_dir = None
    
    if (data_dir / 'textdata').exists():
        text_dir = data_dir / 'textdata'
    if (data_dir / 'voicedata').exists():
        voice_dir = data_dir / 'voicedata'
    
    # textdataì™€ voicedataê°€ ì—†ìœ¼ë©´ ì§ì ‘ íƒìƒ‰
    if text_dir is None or voice_dir is None:
        # ì§ì ‘ txtì™€ npy íŒŒì¼ ë§¤ì¹­
        txt_files = list(data_dir.glob('**/*.txt'))
        npy_files = list(data_dir.glob('**/*.npy'))
        
        # íŒŒì¼ëª… ê¸°ë°˜ ë§¤ì¹­
        txt_dict = {f.stem: f for f in txt_files}
        npy_dict = {f.stem: f for f in npy_files}
        
        for stem in txt_dict:
            if stem in npy_dict:
                try:
                    with open(txt_dict[stem], 'r', encoding='utf-8') as f:
                        text = f.read().strip()
                    
                    # ë¼ë²¨ ì¶”ì • (íŒŒì¼ ê²½ë¡œë‚˜ ì´ë¦„ì—ì„œ)
                    label = estimate_label(txt_dict[stem], default_label)
                    
                    # í™˜ì ID ìƒì„±
                    patient_id = f"{language}_{stem}"
                    
                    data.append({
                        'text': text,
                        'audio_path': str(npy_dict[stem]),
                        'label': label,
                        'language': language,
                        'patient_id': patient_id
                    })
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {txt_dict[stem]} - {e}")
    
    else:
        # textdata/voicedata êµ¬ì¡°ë¡œ ë¡œë“œ
        for text_file in text_dir.glob('**/*.txt'):
            rel_path = text_file.relative_to(text_dir)
            audio_file = voice_dir / rel_path.with_suffix('.npy')
            
            if audio_file.exists():
                try:
                    with open(text_file, 'r', encoding='utf-8') as f:
                        text = f.read().strip()
                    
                    label = estimate_label(text_file, default_label)
                    patient_id = f"{language}_{text_file.stem}"
                    
                    data.append({
                        'text': text,
                        'audio_path': str(audio_file),
                        'label': label,
                        'language': language,
                        'patient_id': patient_id
                    })
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    return data

def estimate_label(file_path, default_label=None):
    """íŒŒì¼ ê²½ë¡œì—ì„œ ë¼ë²¨ ì¶”ì •"""
    if default_label is not None:
        return default_label
    
    path_str = str(file_path).lower()
    
    # ì¹˜ë§¤ ê´€ë ¨ í‚¤ì›Œë“œ
    dementia_keywords = ['dementia', 'alzheimer', 'ad', 'mci', 'impaired']
    control_keywords = ['control', 'healthy', 'hc', 'normal']
    
    for keyword in dementia_keywords:
        if keyword in path_str:
            return 1
    
    for keyword in control_keywords:
        if keyword in path_str:
            return 0
    
    # ê¸°ë³¸ê°’: 0 (ì •ìƒ)
    return 0

def create_stratified_split_multilingual(dataset, train_split=0.7, val_split=0.15, test_split=0.15, random_seed=42):
    """
    í™˜ì ë‹¨ìœ„ stratified split (ì–¸ì–´ì™€ ë¼ë²¨ì„ ëª¨ë‘ ê³ ë ¤)
    """
    # í™˜ìë³„ ë°ì´í„° ê·¸ë£¹í™”
    patient_groups = defaultdict(list)
    for idx, item in enumerate(dataset):
        patient_id = item['patient_id']
        patient_groups[patient_id].append(idx)
    
    # í™˜ìë³„ ë©”íƒ€ë°ì´í„° ìƒì„±
    patient_metadata = []
    for patient_id, indices in patient_groups.items():
        # ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        first_item = dataset[indices[0]]
        patient_metadata.append({
            'patient_id': patient_id,
            'language': first_item['language'],
            'label': first_item['label'],
            'indices': indices,
            'sample_count': len(indices)
        })
    
    # Stratify í‚¤ ìƒì„± (ì–¸ì–´-ë¼ë²¨ ì¡°í•©)
    patient_stratify_keys = [f"{p['language']}_{p['label']}" for p in patient_metadata]
    patient_indices_list = list(range(len(patient_metadata)))
    
    print(f"\nğŸ“Š í™˜ì ë‹¨ìœ„ Stratified Split:")
    print(f"  ì „ì²´ í™˜ì ìˆ˜: {len(patient_metadata)}")
    print(f"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
    
    # í™˜ìë³„ stratify í‚¤ ë¶„í¬ í™•ì¸
    stratify_dist = Counter(patient_stratify_keys)
    print(f"  Stratify í‚¤ ë¶„í¬:")
    for key, count in stratify_dist.items():
        print(f"    {key}: {count}ëª…")
    
    # ì²« ë²ˆì§¸ ë¶„í• : train vs (val + test)
    train_patient_indices, temp_patient_indices = train_test_split(
        patient_indices_list,
        test_size=val_split + test_split,
        stratify=patient_stratify_keys,
        random_state=random_seed
    )
    
    # ë‘ ë²ˆì§¸ ë¶„í• : val vs test
    temp_patient_stratify_keys = [patient_stratify_keys[i] for i in temp_patient_indices]
    val_patient_indices, test_patient_indices = train_test_split(
        temp_patient_indices,
        test_size=test_split / (val_split + test_split),
        stratify=temp_patient_stratify_keys,
        random_state=random_seed
    )
    
    # í™˜ì ì¸ë±ìŠ¤ë¥¼ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    train_indices = []
    val_indices = []
    test_indices = []
    
    for patient_idx in train_patient_indices:
        train_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in val_patient_indices:
        val_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in test_patient_indices:
        test_indices.extend(patient_metadata[patient_idx]['indices'])
    
    # ê²°ê³¼ í†µê³„
    print(f"\nğŸ“ˆ ë¶„í•  ê²°ê³¼:")
    print(f"  í›ˆë ¨: {len(train_indices)}ê°œ ìƒ˜í”Œ, {len(train_patient_indices)}ëª… í™˜ì")
    print(f"  ê²€ì¦: {len(val_indices)}ê°œ ìƒ˜í”Œ, {len(val_patient_indices)}ëª… í™˜ì")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ ìƒ˜í”Œ, {len(test_patient_indices)}ëª… í™˜ì")
    
    # ì–¸ì–´ë³„ ë¶„í¬ í™•ì¸
    train_lang_dist = Counter([dataset[i]['language'] for i in train_indices])
    val_lang_dist = Counter([dataset[i]['language'] for i in val_indices])
    test_lang_dist = Counter([dataset[i]['language'] for i in test_indices])
    
    print(f"\nğŸ“Š ì–¸ì–´ë³„ ë¶„í¬:")
    all_languages = set(train_lang_dist.keys()) | set(val_lang_dist.keys()) | set(test_lang_dist.keys())
    for lang in all_languages:
        train_count = train_lang_dist.get(lang, 0)
        val_count = val_lang_dist.get(lang, 0)
        test_count = test_lang_dist.get(lang, 0)
        total = train_count + val_count + test_count
        print(f"  {lang}: í›ˆë ¨ {train_count}({train_count/total*100:.1f}%), "
              f"ê²€ì¦ {val_count}({val_count/total*100:.1f}%), "
              f"í…ŒìŠ¤íŠ¸ {test_count}({test_count/total*100:.1f}%)")
    
    return train_indices, val_indices, test_indices

def create_cross_lingual_split(dataset, train_languages, test_languages, val_split=0.2, random_seed=42):
    """
    Cross-lingual split: í›ˆë ¨ ì–¸ì–´ì™€ í…ŒìŠ¤íŠ¸ ì–¸ì–´ë¥¼ ë¶„ë¦¬
    """
    print(f"\nğŸŒ Cross-lingual Split:")
    print(f"  í›ˆë ¨ ì–¸ì–´: {train_languages}")
    print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {test_languages}")
    
    # ì–¸ì–´ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
    train_data_indices = []
    test_data_indices = []
    
    for idx, item in enumerate(dataset):
        if item['language'] in train_languages:
            train_data_indices.append(idx)
        elif item['language'] in test_languages:
            test_data_indices.append(idx)
    
    # í›ˆë ¨ ì–¸ì–´ ë°ì´í„°ë¥¼ train/valë¡œ ë¶„í• 
    train_subset = [dataset[i] for i in train_data_indices]
    train_indices, val_indices, _ = create_stratified_split_multilingual(
        train_subset, 
        train_split=1-val_split, 
        val_split=val_split, 
        test_split=0,
        random_seed=random_seed
    )
    
    # ì¸ë±ìŠ¤ ë§¤í•‘ (subset â†’ original)
    train_indices = [train_data_indices[i] for i in train_indices]
    val_indices = [train_data_indices[i] for i in val_indices]
    
    # í…ŒìŠ¤íŠ¸ ì–¸ì–´ ë°ì´í„°ë¥¼ val/testë¡œ ë¶„í• 
    test_subset = [dataset[i] for i in test_data_indices]
    if len(test_subset) > 0:
        _, test_val_indices, test_test_indices = create_stratified_split_multilingual(
            test_subset,
            train_split=0,
            val_split=0.5,
            test_split=0.5,
            random_seed=random_seed
        )
        
        # í…ŒìŠ¤íŠ¸ ì–¸ì–´ì˜ valì„ ì „ì²´ valì— ì¶”ê°€
        val_indices.extend([test_data_indices[i] for i in test_val_indices])
        test_indices = [test_data_indices[i] for i in test_test_indices]
    else:
        test_indices = []
    
    print(f"\nâœ… Cross-lingual Split ì™„ë£Œ:")
    print(f"  í›ˆë ¨: {len(train_indices)}ê°œ (ì–¸ì–´: {train_languages})")
    print(f"  ê²€ì¦: {len(val_indices)}ê°œ (í˜¼í•©)")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ (ì–¸ì–´: {test_languages})")
    
    return train_indices, val_indices, test_indices

def prepare_multilingual_dataset(data_dir, max_seq_len=512, languages=None, 
                                tokenizer_name='bert-base-uncased'):
    """
    ë‹¤êµ­ì–´ ë°ì´í„°ì…‹ ì¤€ë¹„
    """
    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
    
    # ë‹¤êµ­ì–´ ë°ì´í„° ë¡œë“œ
    raw_data = read_multilingual_data(data_dir, languages)
    
    # í…ìŠ¤íŠ¸ í† í°í™”
    print(f"\nğŸ”¤ í…ìŠ¤íŠ¸ í† í°í™” ì¤‘...")
    processed_data = []
    
    for item in raw_data:
        try:
            # BERT í† í°í™”
            encoding = tokenizer.encode_plus(
                item['text'],
                add_special_tokens=True,
                max_length=max_seq_len,
                padding='max_length',
                return_attention_mask=True,
                return_tensors='pt',
                truncation=True
            )
            
            processed_item = {
                'input_ids': encoding['input_ids'],
                'attention_mask': encoding['attention_mask'],
                'audio_path': item['audio_path'],
                'label': item['label'],
                'language': item['language'],
                'patient_id': item['patient_id']
            }
            processed_data.append(processed_item)
            
        except Exception as e:
            print(f"âš ï¸ í† í°í™” ì‹¤íŒ¨: {item['patient_id']} - {e}")
    
    print(f"âœ… {len(processed_data)}ê°œ ìƒ˜í”Œ í† í°í™” ì™„ë£Œ")
    
    return MultilingualDementiaDataset(processed_data)

def collate_fn_multilingual(batch):
    """
    ë‹¤êµ­ì–´ ë°ì´í„°ì…‹ìš© collate function
    """
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    audio = torch.stack([item['audio'] for item in batch])
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.float)
    languages = [item['language'] for item in batch]
    patient_ids = [item['patient_id'] for item in batch]
    
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'audio': audio,
        'labels': labels,
        'languages': languages,
        'patient_ids': patient_ids
    }
