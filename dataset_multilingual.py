import torch
from torch.utils.data import Dataset
import numpy as np
from pathlib import Path
from transformers import BertTokenizer
from collections import Counter, defaultdict
from sklearn.model_selection import train_test_split
import random

class MultilingualDementiaDataset(Dataset):
    def __init__(self, data):
        """
        ë‹¤êµ­ì–´ ì¹˜ë§¤ ë°ì´í„°ì…‹
        data: list of dicts with keys: 'text', 'audio_path', 'label', 'language', 'patient_id'
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë¡œë“œ
        try:
            audio_specs = np.load(item['audio_path'])
            # í¬ê¸° ì¡°ì •: (3, 128, 250)ìœ¼ë¡œ ë§ì¶”ê¸°
            if audio_specs.shape != (3, 128, 250):
                # íŒ¨ë”© ë˜ëŠ” í¬ë¡­ìœ¼ë¡œ í¬ê¸° ì¡°ì •
                audio_specs = self._resize_audio(audio_specs)
        except:
            # ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
            audio_specs = np.zeros((3, 128, 250), dtype=np.float32)
        
        return {
            'input_ids': item['input_ids'].squeeze(0),  # (max_seq_len,)
            'attention_mask': item['attention_mask'].squeeze(0),  # (max_seq_len,)
            'audio': torch.tensor(audio_specs, dtype=torch.float32),  # (3, 128, 250)
            'label': item['label'],
            'language': item['language'],
            'patient_id': item['patient_id']
        }
    
    def _resize_audio(self, audio_specs):
        """ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í¬ê¸° ì¡°ì •"""
        target_shape = (3, 128, 250)
        
        if len(audio_specs.shape) == 2:
            # (H, W) -> (3, H, W) ë³€í™˜
            audio_specs = np.stack([audio_specs] * 3, axis=0)
        
        current_shape = audio_specs.shape
        resized = np.zeros(target_shape, dtype=np.float32)
        
        # ê° ì°¨ì›ë³„ë¡œ í¬ê¸° ì¡°ì •
        h_min = min(current_shape[1], target_shape[1])
        w_min = min(current_shape[2], target_shape[2])
        
        resized[:, :h_min, :w_min] = audio_specs[:, :h_min, :w_min]
        
        return resized

def read_multilingual_data(data_dir, languages=None):
    """
    ë‹¤êµ­ì–´ ë°ì´í„° ë¡œë“œ
    data_dir: training_dset ê²½ë¡œ
    languages: ë¡œë“œí•  ì–¸ì–´ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì–¸ì–´)
    """
    if languages is None:
        languages = ['English', 'Greek', 'Spanish', 'Mandarin']
    
    all_data = []
    data_dir = Path(data_dir)
    
    print(f"ğŸ“‚ ë‹¤êµ­ì–´ ë°ì´í„° ë¡œë“œ ì¤‘: {languages}")
    
    for language in languages:
        lang_dir = data_dir / language
        if not lang_dir.exists():
            print(f"âš ï¸ ì–¸ì–´ ë””ë ‰í† ë¦¬ ì—†ìŒ: {lang_dir}")
            continue
            
        print(f"  ğŸ“ {language} ë°ì´í„° ë¡œë“œ ì¤‘...")
        lang_data = load_language_data(lang_dir, language)
        all_data.extend(lang_data)
        print(f"    âœ… {len(lang_data)}ê°œ ìƒ˜í”Œ ë¡œë“œë¨")
    
    print(f"ğŸ“Š ì „ì²´ ë¡œë“œëœ ë°ì´í„°: {len(all_data)}ê°œ ìƒ˜í”Œ")
    
    # ì–¸ì–´ë³„ í†µê³„
    lang_stats = Counter([item['language'] for item in all_data])
    label_stats = Counter([item['label'] for item in all_data])
    
    print(f"ğŸ“ˆ ì–¸ì–´ë³„ ë¶„í¬:")
    for lang, count in lang_stats.items():
        print(f"  {lang}: {count}ê°œ")
    
    print(f"ğŸ“ˆ ë¼ë²¨ë³„ ë¶„í¬:")
    label_names = {0: 'ì •ìƒ', 1: 'ì¹˜ë§¤'}
    for label, count in label_stats.items():
        print(f"  {label_names[label]}: {count}ê°œ")
    
    return all_data

def load_language_data(lang_dir, language):
    """íŠ¹ì • ì–¸ì–´ì˜ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # ì–¸ì–´ë³„ ë°ì´í„° êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
    if language == 'English':
        data = load_english_data(lang_dir, language)
    elif language == 'Greek':
        data = load_greek_data(lang_dir, language)
    elif language == 'Spanish':
        data = load_spanish_data(lang_dir, language)
    elif language == 'Mandarin':
        data = load_mandarin_data(lang_dir, language)
    
    return data

def load_english_data(lang_dir, language):
    """ì˜ì–´ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # textdataì™€ voicedata ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ (ë‹¤ë¥¸ ì–¸ì–´ì™€ ë™ì¼í•œ êµ¬ì¡°)
    text_dir = lang_dir / 'textdata'
    voice_dir = lang_dir / 'voicedata'
    
    if text_dir.exists() and voice_dir.exists():
        # AD, HC ì¹´í…Œê³ ë¦¬
        categories = [
            ('HC', 0),    # Healthy Control
            ('AD', 1)     # Alzheimer's Disease
        ]
        
        for cat_name, label in categories:
            cat_text_dir = text_dir / cat_name
            cat_voice_dir = voice_dir / cat_name
            
            if cat_text_dir.exists() and cat_voice_dir.exists():
                # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ
                for text_file in cat_text_dir.glob('*.txt'):
                    audio_file = cat_voice_dir / f"{text_file.stem}.npy"
                    
                    if audio_file.exists():
                        try:
                            with open(text_file, 'r', encoding='utf-8') as f:
                                text = f.read().strip()
                            
                            if text and len(text) >= 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                                patient_id = f"{language}_{cat_name}_{text_file.stem}"
                                data.append({
                                    'text': text,
                                    'audio_path': str(audio_file),
                                    'label': label,
                                    'language': language,
                                    'patient_id': patient_id
                                })
                        except Exception as e:
                            print(f"âš ï¸ ì˜ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    # Pitt ë””ë ‰í† ë¦¬ë„ ìˆë‹¤ë©´ ì¶”ê°€ë¡œ ë¡œë“œ
    pitt_dir = lang_dir / 'Pitt'
    if pitt_dir.exists():
        pitt_data = load_pitt_data(pitt_dir, language)
        data.extend(pitt_data)
    
    return data

def load_pitt_data(pitt_dir, language):
    """Pitt ì½”í¼ìŠ¤ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    text_dir = pitt_dir / 'textdata'
    voice_dir = pitt_dir / 'voicedata'
    
    if not (text_dir.exists() and voice_dir.exists()):
        return data
    
    # AD, HC ì¹´í…Œê³ ë¦¬ë¡œ ë¡œë“œ (ì‹¤ì œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì— ë§ê²Œ)
    categories = [
        ('HC', 0),    # Healthy Control
        ('AD', 1)     # Alzheimer's Disease  
    ]
    
    for cat_name, label in categories:
        cat_text_dir = text_dir / cat_name
        cat_voice_dir = voice_dir / cat_name
        
        if not (cat_text_dir.exists() and cat_voice_dir.exists()):
            continue
        
        # í•˜ìœ„ íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ë“¤ (cookie, fluency, recall, sentence)
        for task_dir in cat_text_dir.iterdir():
            if task_dir.is_dir():
                voice_task_dir = cat_voice_dir / task_dir.name
                
                if not voice_task_dir.exists():
                    continue
                
                # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ
                for text_file in task_dir.glob('*.txt'):
                    # í•´ë‹¹í•˜ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
                    audio_file = voice_task_dir / f"{text_file.stem}.npy"
                    
                    if not audio_file.exists():
                        # ë‹¤ë¥¸ í™•ì¥ìë¡œë„ ì‹œë„
                        audio_file = voice_task_dir / f"{text_file.stem}.wav"
                        if not audio_file.exists():
                            continue
                    
                    # í…ìŠ¤íŠ¸ ì½ê¸°
                    try:
                        with open(text_file, 'r', encoding='utf-8') as f:
                            text = f.read().strip()
                        
                        if not text or len(text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ìŠ¤í‚µ
                            continue
                        
                        # í™˜ì ID ì¶”ì¶œ (ì¹´í…Œê³ ë¦¬_íƒœìŠ¤í¬_íŒŒì¼ëª…)
                        patient_id = f"{language}_{cat_name}_{task_dir.name}_{text_file.stem}"
                        
                        data.append({
                            'text': text,
                            'audio_path': str(audio_file),
                            'label': label,
                            'language': language,
                            'patient_id': patient_id
                        })
                    except Exception as e:
                        print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    return data

def load_greek_data(lang_dir, language):
    """ê·¸ë¦¬ìŠ¤ì–´ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # textdataì™€ voicedata ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
    text_dir = lang_dir / 'textdata'
    voice_dir = lang_dir / 'voicedata'
    
    if text_dir.exists() and voice_dir.exists():
        # AD, HC, MCI ì¹´í…Œê³ ë¦¬
        categories = [
            ('HC', 0),    # Healthy Control
            ('AD', 1),    # Alzheimer's Disease
            ('MCI', 1)    # Mild Cognitive Impairment (ì¹˜ë§¤ë¡œ ë¶„ë¥˜)
        ]
        
        for cat_name, label in categories:
            cat_text_dir = text_dir / cat_name
            cat_voice_dir = voice_dir / cat_name
            
            if cat_text_dir.exists() and cat_voice_dir.exists():
                # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ
                for text_file in cat_text_dir.glob('*.txt'):
                    audio_file = cat_voice_dir / f"{text_file.stem}.npy"
                    
                    if audio_file.exists():
                        try:
                            with open(text_file, 'r', encoding='utf-8') as f:
                                text = f.read().strip()
                            
                            if text:
                                patient_id = f"{language}_{cat_name}_{text_file.stem}"
                                data.append({
                                    'text': text,
                                    'audio_path': str(audio_file),
                                    'label': label,
                                    'language': language,
                                    'patient_id': patient_id
                                })
                        except Exception as e:
                            print(f"âš ï¸ ê·¸ë¦¬ìŠ¤ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    # long, short, pilot ë””ë ‰í† ë¦¬ì—ì„œë„ ë¡œë“œ
    for subdir_name in ['long', 'short', 'pilot']:
        subdir = lang_dir / subdir_name
        if subdir.exists():
            subdir_data = load_greek_subdir(subdir, language, subdir_name)
            data.extend(subdir_data)
    
    return data

def load_greek_subdir(subdir, language, subdir_name):
    """ê·¸ë¦¬ìŠ¤ì–´ ì„œë¸Œë””ë ‰í† ë¦¬ ë¡œë“œ"""
    data = []
    
    if subdir_name in ['long', 'short']:
        # AD, HC, MCI ì¹´í…Œê³ ë¦¬
        categories = [
            ('HC', 0),
            ('AD', 1),
            ('MCI', 1)
        ]
        
        for cat_name, label in categories:
            cat_dir = subdir / cat_name
            if cat_dir.exists():
                # .npyì™€ .mp3 íŒŒì¼ ìŒ ì°¾ê¸°
                npy_files = list(cat_dir.glob('*.npy'))
                for npy_file in npy_files:
                    # í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ì—ì„œ)
                    # ê°„ë‹¨íˆ íŒŒì¼ëª…ì„ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                    text = f"Greek audio sample {npy_file.stem}"
                    patient_id = f"{language}_{subdir_name}_{cat_name}_{npy_file.stem}"
                    
                    data.append({
                        'text': text,
                        'audio_path': str(npy_file),
                        'label': label,
                        'language': language,
                        'patient_id': patient_id
                    })
    
    return data

def load_spanish_data(lang_dir, language):
    """ìŠ¤í˜ì¸ì–´ ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # textdataì™€ voicedata ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
    text_dir = lang_dir / 'textdata'
    voice_dir = lang_dir / 'voicedata'
    
    if text_dir.exists() and voice_dir.exists():
        # AD, HC, MCI ì¹´í…Œê³ ë¦¬
        categories = [
            ('HC', 0),    # Healthy Control
            ('AD', 1),    # Alzheimer's Disease
            ('MCI', 1)    # Mild Cognitive Impairment (ì¹˜ë§¤ë¡œ ë¶„ë¥˜)
        ]
        
        for cat_name, label in categories:
            cat_text_dir = text_dir / cat_name
            cat_voice_dir = voice_dir / cat_name
            
            if cat_text_dir.exists() and cat_voice_dir.exists():
                # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ
                for text_file in cat_text_dir.glob('*.txt'):
                    audio_file = cat_voice_dir / f"{text_file.stem}.npy"
                    
                    if audio_file.exists():
                        try:
                            with open(text_file, 'r', encoding='utf-8') as f:
                                text = f.read().strip()
                            
                            if text:
                                patient_id = f"{language}_{cat_name}_{text_file.stem}"
                                data.append({
                                    'text': text,
                                    'audio_path': str(audio_file),
                                    'label': label,
                                    'language': language,
                                    'patient_id': patient_id
                                })
                        except Exception as e:
                            print(f"âš ï¸ ìŠ¤í˜ì¸ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    return data

def load_mandarin_data(lang_dir, language):
    """ë§Œë‹¤ë¦° ë°ì´í„° ë¡œë“œ"""
    data = []
    
    # textdataì™€ voicedata ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
    text_dir = lang_dir / 'textdata'
    voice_dir = lang_dir / 'voicedata'
    
    if text_dir.exists() and voice_dir.exists():
        # AD, HC, MCI ì¹´í…Œê³ ë¦¬
        categories = [
            ('HC', 0),    # Healthy Control
            ('AD', 1),    # Alzheimer's Disease
            ('MCI', 1)    # Mild Cognitive Impairment (ì¹˜ë§¤ë¡œ ë¶„ë¥˜)
        ]
        
        for cat_name, label in categories:
            cat_text_dir = text_dir / cat_name
            cat_voice_dir = voice_dir / cat_name
            
            if cat_text_dir.exists() and cat_voice_dir.exists():
                # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ
                for text_file in cat_text_dir.glob('*.txt'):
                    audio_file = cat_voice_dir / f"{text_file.stem}.npy"
                    
                    if audio_file.exists():
                        try:
                            with open(text_file, 'r', encoding='utf-8') as f:
                                text = f.read().strip()
                            
                            if text:
                                patient_id = f"{language}_{cat_name}_{text_file.stem}"
                                data.append({
                                    'text': text,
                                    'audio_path': str(audio_file),
                                    'label': label,
                                    'language': language,
                                    'patient_id': patient_id
                                })
                        except Exception as e:
                            print(f"âš ï¸ ë§Œë‹¤ë¦° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    # Lu, Ye ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ
    for subdir_name in ['Lu', 'Ye']:
        subdir = lang_dir / subdir_name
        if subdir.exists():
            # .npy íŒŒì¼ë“¤ ì§ì ‘ ë¡œë“œ
            npy_files = list(subdir.glob('*.npy'))
            for npy_file in npy_files:
                # íŒŒì¼ëª…ì—ì„œ ë¼ë²¨ ì¶”ì • (ê°„ë‹¨í•œ ê·œì¹™)
                label = 1 if any(x in npy_file.stem.lower() for x in ['ad', 'pd', 'df']) else 0
                text = f"Mandarin audio sample {npy_file.stem}"
                patient_id = f"{language}_{subdir_name}_{npy_file.stem}"
                
                data.append({
                    'text': text,
                    'audio_path': str(npy_file),
                    'label': label,
                    'language': language,
                    'patient_id': patient_id
                })
    
    return data

def load_generic_data(data_dir, language, default_label=None):
    """ì¼ë°˜ì ì¸ ë°ì´í„° êµ¬ì¡° ë¡œë“œ"""
    data = []
    
    # textdataì™€ voicedata ë””ë ‰í† ë¦¬ ì°¾ê¸°
    text_dir = None
    voice_dir = None
    
    if (data_dir / 'textdata').exists():
        text_dir = data_dir / 'textdata'
    if (data_dir / 'voicedata').exists():
        voice_dir = data_dir / 'voicedata'
    
    # textdataì™€ voicedataê°€ ì—†ìœ¼ë©´ ì§ì ‘ íƒìƒ‰
    if text_dir is None or voice_dir is None:
        # ì§ì ‘ txtì™€ npy íŒŒì¼ ë§¤ì¹­
        txt_files = list(data_dir.glob('**/*.txt'))
        npy_files = list(data_dir.glob('**/*.npy'))
        
        # íŒŒì¼ëª… ê¸°ë°˜ ë§¤ì¹­
        txt_dict = {f.stem: f for f in txt_files}
        npy_dict = {f.stem: f for f in npy_files}
        
        for stem in txt_dict:
            if stem in npy_dict:
                try:
                    with open(txt_dict[stem], 'r', encoding='utf-8') as f:
                        text = f.read().strip()
                    
                    # ë¼ë²¨ ì¶”ì • (íŒŒì¼ ê²½ë¡œë‚˜ ì´ë¦„ì—ì„œ)
                    label = estimate_label(txt_dict[stem], default_label)
                    
                    # í™˜ì ID ìƒì„±
                    patient_id = f"{language}_{stem}"
                    
                    data.append({
                        'text': text,
                        'audio_path': str(npy_dict[stem]),
                        'label': label,
                        'language': language,
                        'patient_id': patient_id
                    })
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {txt_dict[stem]} - {e}")
    
    else:
        # textdata/voicedata êµ¬ì¡°ë¡œ ë¡œë“œ
        for text_file in text_dir.glob('**/*.txt'):
            rel_path = text_file.relative_to(text_dir)
            audio_file = voice_dir / rel_path.with_suffix('.npy')
            
            if audio_file.exists():
                try:
                    with open(text_file, 'r', encoding='utf-8') as f:
                        text = f.read().strip()
                    
                    label = estimate_label(text_file, default_label)
                    patient_id = f"{language}_{text_file.stem}"
                    
                    data.append({
                        'text': text,
                        'audio_path': str(audio_file),
                        'label': label,
                        'language': language,
                        'patient_id': patient_id
                    })
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {text_file} - {e}")
    
    return data

def estimate_label(file_path, default_label=None):
    """íŒŒì¼ ê²½ë¡œì—ì„œ ë¼ë²¨ ì¶”ì •"""
    if default_label is not None:
        return default_label
    
    path_str = str(file_path).lower()
    
    # ì¹˜ë§¤ ê´€ë ¨ í‚¤ì›Œë“œ
    dementia_keywords = ['dementia', 'alzheimer', 'ad', 'mci', 'impaired']
    control_keywords = ['control', 'healthy', 'hc', 'normal']
    
    for keyword in dementia_keywords:
        if keyword in path_str:
            return 1
    
    for keyword in control_keywords:
        if keyword in path_str:
            return 0
    
    # ê¸°ë³¸ê°’: 0 (ì •ìƒ)
    return 0

def create_stratified_split_multilingual(dataset, train_split=0.7, val_split=0.1, test_split=0.2, random_seed=42):
    """
    í™˜ì ë‹¨ìœ„ stratified split (ì–¸ì–´ì™€ ë¼ë²¨ì„ ëª¨ë‘ ê³ ë ¤)
    """
    # í™˜ìë³„ ë°ì´í„° ê·¸ë£¹í™”
    patient_groups = defaultdict(list)
    for idx, item in enumerate(dataset):
        patient_id = item['patient_id']
        patient_groups[patient_id].append(idx)
    
    # í™˜ìë³„ ë©”íƒ€ë°ì´í„° ìƒì„±
    patient_metadata = []
    for patient_id, indices in patient_groups.items():
        # ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        first_item = dataset[indices[0]]
        patient_metadata.append({
            'patient_id': patient_id,
            'language': first_item['language'],
            'label': first_item['label'],
            'indices': indices,
            'sample_count': len(indices)
        })
    
    # Stratify í‚¤ ìƒì„± (ì–¸ì–´-ë¼ë²¨ ì¡°í•©)
    patient_stratify_keys = [f"{p['language']}_{p['label']}" for p in patient_metadata]
    patient_indices_list = list(range(len(patient_metadata)))
    
    print(f"\nğŸ“Š í™˜ì ë‹¨ìœ„ Stratified Split:")
    print(f"  ì „ì²´ í™˜ì ìˆ˜: {len(patient_metadata)}")
    print(f"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
    
    # í™˜ìë³„ stratify í‚¤ ë¶„í¬ í™•ì¸
    stratify_dist = Counter(patient_stratify_keys)
    print(f"  Stratify í‚¤ ë¶„í¬:")
    for key, count in stratify_dist.items():
        print(f"    {key}: {count}ëª…")
    
    # ì²« ë²ˆì§¸ ë¶„í• : train vs (val + test)
    train_patient_indices, temp_patient_indices = train_test_split(
        patient_indices_list,
        test_size=val_split + test_split,
        stratify=patient_stratify_keys,
        random_state=random_seed
    )
    
    # ë‘ ë²ˆì§¸ ë¶„í• : val vs test
    temp_patient_stratify_keys = [patient_stratify_keys[i] for i in temp_patient_indices]
    val_patient_indices, test_patient_indices = train_test_split(
        temp_patient_indices,
        test_size=test_split / (val_split + test_split),
        stratify=temp_patient_stratify_keys,
        random_state=random_seed
    )
    
    # í™˜ì ì¸ë±ìŠ¤ë¥¼ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    train_indices = []
    val_indices = []
    test_indices = []
    
    for patient_idx in train_patient_indices:
        train_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in val_patient_indices:
        val_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in test_patient_indices:
        test_indices.extend(patient_metadata[patient_idx]['indices'])
    
    # ê²°ê³¼ í†µê³„
    print(f"\nğŸ“ˆ ë¶„í•  ê²°ê³¼:")
    print(f"  í›ˆë ¨: {len(train_indices)}ê°œ ìƒ˜í”Œ, {len(train_patient_indices)}ëª… í™˜ì")
    print(f"  ê²€ì¦: {len(val_indices)}ê°œ ìƒ˜í”Œ, {len(val_patient_indices)}ëª… í™˜ì")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ ìƒ˜í”Œ, {len(test_patient_indices)}ëª… í™˜ì")
    
    # ì–¸ì–´ë³„ ë¶„í¬ í™•ì¸
    train_lang_dist = Counter([dataset[i]['language'] for i in train_indices])
    val_lang_dist = Counter([dataset[i]['language'] for i in val_indices])
    test_lang_dist = Counter([dataset[i]['language'] for i in test_indices])
    
    print(f"\nğŸ“Š ì–¸ì–´ë³„ ë¶„í¬:")
    all_languages = set(train_lang_dist.keys()) | set(val_lang_dist.keys()) | set(test_lang_dist.keys())
    for lang in all_languages:
        train_count = train_lang_dist.get(lang, 0)
        val_count = val_lang_dist.get(lang, 0)
        test_count = test_lang_dist.get(lang, 0)
        total = train_count + val_count + test_count
        print(f"  {lang}: í›ˆë ¨ {train_count}({train_count/total*100:.1f}%), "
              f"ê²€ì¦ {val_count}({val_count/total*100:.1f}%), "
              f"í…ŒìŠ¤íŠ¸ {test_count}({test_count/total*100:.1f}%)")
    
    return train_indices, val_indices, test_indices

def create_cross_lingual_split(dataset, train_languages, test_languages, random_seed=42):
    """
    Cross-lingual split: í›ˆë ¨ì€ ì†ŒìŠ¤ ì–¸ì–´ë§Œ, ê²€ì¦/í…ŒìŠ¤íŠ¸ëŠ” íƒ€ê²Ÿ ì–¸ì–´ í¬í•¨
    - í›ˆë ¨: ì†ŒìŠ¤ ì–¸ì–´ë§Œ (7:1:2 ì¤‘ 7 ë¶€ë¶„)
    - ê²€ì¦: ì†ŒìŠ¤ ì–¸ì–´ ì¼ë¶€ + íƒ€ê²Ÿ ì–¸ì–´ ì¼ë¶€ (1 ë¶€ë¶„)
    - í…ŒìŠ¤íŠ¸: ì†ŒìŠ¤ ì–¸ì–´ ì¼ë¶€ + íƒ€ê²Ÿ ì–¸ì–´ ì¼ë¶€ (2 ë¶€ë¶„)
    """
    print(f"\nğŸŒ Cross-lingual Split (7:1:2):")
    print(f"  í›ˆë ¨ ì–¸ì–´ (ì†ŒìŠ¤): {train_languages}")
    print(f"  íƒ€ê²Ÿ ì–¸ì–´: {test_languages}")
    
    # ì–¸ì–´ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
    source_data_indices = []
    target_data_indices = []
    
    for idx, item in enumerate(dataset):
        if item['language'] in train_languages:
            source_data_indices.append(idx)
        elif item['language'] in test_languages:
            target_data_indices.append(idx)
    
    print(f"  ì†ŒìŠ¤ ì–¸ì–´ ë°ì´í„°: {len(source_data_indices)}ê°œ")
    print(f"  íƒ€ê²Ÿ ì–¸ì–´ ë°ì´í„°: {len(target_data_indices)}ê°œ")
    
    # ì†ŒìŠ¤ ì–¸ì–´ ë°ì´í„°ë¥¼ 7:1:2ë¡œ ë¶„í• 
    source_subset = [dataset[i] for i in source_data_indices]
    source_train_indices, source_val_indices, source_test_indices = create_stratified_split_multilingual(
        source_subset, 
        train_split=0.7,   # 70%
        val_split=0.1,     # 10% 
        test_split=0.2,    # 20%
        random_seed=random_seed
    )
    
    # ì¸ë±ìŠ¤ ë§¤í•‘ (subset â†’ original)
    train_indices = [source_data_indices[i] for i in source_train_indices]
    source_val_mapped = [source_data_indices[i] for i in source_val_indices]
    source_test_mapped = [source_data_indices[i] for i in source_test_indices]
    
    # íƒ€ê²Ÿ ì–¸ì–´ ë°ì´í„°ë¥¼ 1:2ë¡œ ë¶„í•  (val:test)
    val_indices = source_val_mapped.copy()  # ì†ŒìŠ¤ ì–¸ì–´ valë¡œ ì‹œì‘
    test_indices = source_test_mapped.copy()  # ì†ŒìŠ¤ ì–¸ì–´ testë¡œ ì‹œì‘
    
    if len(target_data_indices) > 0:
        target_subset = [dataset[i] for i in target_data_indices]
        
        # íƒ€ê²Ÿ ì–¸ì–´ë¥¼ 1:2 ë¹„ìœ¨ë¡œ val:test ë¶„í• 
        target_val_ratio = 1 / (1 + 2)  # 1/3
        target_test_ratio = 2 / (1 + 2)  # 2/3
        
        _, target_val_indices, target_test_indices = create_stratified_split_multilingual(
            target_subset,
            train_split=0,
            val_split=target_val_ratio,
            test_split=target_test_ratio,
            random_seed=random_seed
        )
        
        # íƒ€ê²Ÿ ì–¸ì–´ì˜ val/testë¥¼ ì „ì²´ì— ì¶”ê°€
        val_indices.extend([target_data_indices[i] for i in target_val_indices])
        test_indices.extend([target_data_indices[i] for i in target_test_indices])
    
    # ì–¸ì–´ë³„ ë¶„í¬ í™•ì¸
    train_langs = [dataset[i]['language'] for i in train_indices]
    val_langs = [dataset[i]['language'] for i in val_indices]
    test_langs = [dataset[i]['language'] for i in test_indices]
    
    from collections import Counter
    train_lang_dist = Counter(train_langs)
    val_lang_dist = Counter(val_langs)
    test_lang_dist = Counter(test_langs)
    
    print(f"\nâœ… Cross-lingual Split ì™„ë£Œ (7:1:2):")
    print(f"  í›ˆë ¨: {len(train_indices)}ê°œ")
    for lang, count in train_lang_dist.items():
        print(f"    {lang}: {count}ê°œ")
    
    print(f"  ê²€ì¦: {len(val_indices)}ê°œ")
    for lang, count in val_lang_dist.items():
        print(f"    {lang}: {count}ê°œ")
    
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ")
    for lang, count in test_lang_dist.items():
        print(f"    {lang}: {count}ê°œ")
    
    return train_indices, val_indices, test_indices

def prepare_multilingual_dataset(data_dir, max_seq_len=512, languages=None, 
                                tokenizer_name='bert-base-uncased'):
    """
    ë‹¤êµ­ì–´ ë°ì´í„°ì…‹ ì¤€ë¹„
    """
    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
    
    # ë‹¤êµ­ì–´ ë°ì´í„° ë¡œë“œ
    raw_data = read_multilingual_data(data_dir, languages)
    
    # í…ìŠ¤íŠ¸ í† í°í™”
    print(f"\nğŸ”¤ í…ìŠ¤íŠ¸ í† í°í™” ì¤‘...")
    processed_data = []
    
    for item in raw_data:
        try:
            # BERT í† í°í™”
            encoding = tokenizer.encode_plus(
                item['text'],
                add_special_tokens=True,
                max_length=max_seq_len,
                padding='max_length',
                return_attention_mask=True,
                return_tensors='pt',
                truncation=True
            )
            
            processed_item = {
                'input_ids': encoding['input_ids'],
                'attention_mask': encoding['attention_mask'],
                'audio_path': item['audio_path'],
                'label': item['label'],
                'language': item['language'],
                'patient_id': item['patient_id']
            }
            processed_data.append(processed_item)
            
        except Exception as e:
            print(f"âš ï¸ í† í°í™” ì‹¤íŒ¨: {item['patient_id']} - {e}")
    
    print(f"âœ… {len(processed_data)}ê°œ ìƒ˜í”Œ í† í°í™” ì™„ë£Œ")
    
    return MultilingualDementiaDataset(processed_data)

def collate_fn_multilingual(batch):
    """
    ë‹¤êµ­ì–´ ë°ì´í„°ì…‹ìš© collate function
    """
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    audio = torch.stack([item['audio'] for item in batch])
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.float)
    languages = [item['language'] for item in batch]
    patient_ids = [item['patient_id'] for item in batch]
    
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'audio': audio,
        'labels': labels,
        'languages': languages,
        'patient_ids': patient_ids
    }
