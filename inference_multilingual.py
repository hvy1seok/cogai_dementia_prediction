#!/usr/bin/env python3
"""
ë‹¤êµ­ì–´ ë©€í‹°ëª¨ë‹¬ ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
"""
import torch
import torch.nn.functional as F
import numpy as np
import argparse
import os
from pathlib import Path
from transformers import BertTokenizer
import librosa
import cv2

from models_multilingual import MultilingualMultimodalModel
from dataset_multilingual import load_audio_features

def load_model_and_tokenizer(model_path, tokenizer_path, device):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    """
    print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
    
    # ëª¨ë¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location=device)
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°)
    model_config = checkpoint.get('model_config', {})
    text_model_type = model_config.get('text_model_type', 1)
    dropout = model_config.get('dropout', 0.3)
    
    model = MultilingualMultimodalModel(
        text_model_type=text_model_type,
        dropout=dropout
    )
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {tokenizer_path}")
    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)
    print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    return model, tokenizer

def preprocess_text(text, tokenizer, max_seq_len=512):
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    """
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_seq_len,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt',
        truncation=True
    )
    
    return encoding['input_ids'], encoding['attention_mask']

def preprocess_audio(audio_path, target_size=(224, 224)):
    """
    ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ë©œìŠ¤í™í† ê·¸ë¨)
    """
    try:
        # .npy íŒŒì¼ì¸ ê²½ìš° ì§ì ‘ ë¡œë“œ
        if audio_path.endswith('.npy'):
            audio_features = np.load(audio_path)
            if len(audio_features.shape) == 2:
                # 2D -> 3D (ì±„ë„ ì°¨ì› ì¶”ê°€)
                audio_features = np.expand_dims(audio_features, axis=0)
            # 3ì±„ë„ë¡œ ë³µì‚¬ (RGB)
            if audio_features.shape[0] == 1:
                audio_features = np.repeat(audio_features, 3, axis=0)
        else:
            # ì¼ë°˜ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
            audio_features = load_audio_features(audio_path)
        
        # í¬ê¸° ì¡°ì •
        if audio_features.shape[1:] != target_size:
            audio_resized = []
            for channel in range(audio_features.shape[0]):
                resized = cv2.resize(audio_features[channel], target_size, 
                                   interpolation=cv2.INTER_LINEAR)
                audio_resized.append(resized)
            audio_features = np.stack(audio_resized, axis=0)
        
        # ì •ê·œí™”
        audio_features = (audio_features - audio_features.mean()) / (audio_features.std() + 1e-8)
        
        return torch.FloatTensor(audio_features).unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨ {audio_path}: {e}")
        # ë¹ˆ íŠ¹ì§• ë°˜í™˜
        return torch.zeros(1, 3, target_size[0], target_size[1])

def predict_single(model, tokenizer, text, audio_path, device, max_seq_len=512):
    """
    ë‹¨ì¼ ìƒ˜í”Œ ì˜ˆì¸¡
    """
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    input_ids, attention_mask = preprocess_text(text, tokenizer, max_seq_len)
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)
    
    # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
    audio_features = preprocess_audio(audio_path)
    audio_features = audio_features.to(device)
    
    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(audio_features, input_ids, attention_mask)
        probabilities = F.softmax(outputs, dim=1)
        prediction = torch.argmax(outputs, dim=1).item()
        confidence = probabilities[0][prediction].item()
    
    return {
        'prediction': prediction,
        'prediction_label': 'ì¹˜ë§¤' if prediction == 1 else 'ì •ìƒ',
        'confidence': confidence,
        'probabilities': {
            'ì •ìƒ': probabilities[0][0].item(),
            'ì¹˜ë§¤': probabilities[0][1].item()
        }
    }

def predict_batch(model, tokenizer, data_list, device, max_seq_len=512):
    """
    ë°°ì¹˜ ì˜ˆì¸¡
    """
    results = []
    
    for i, data in enumerate(data_list):
        text = data['text']
        audio_path = data['audio_path']
        language = data.get('language', 'Unknown')
        
        print(f"ì˜ˆì¸¡ ì¤‘ ({i+1}/{len(data_list)}): {language}")
        
        try:
            result = predict_single(model, tokenizer, text, audio_path, device, max_seq_len)
            result.update({
                'text': text,
                'audio_path': audio_path,
                'language': language
            })
            results.append(result)
            
        except Exception as e:
            print(f"âš ï¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            results.append({
                'error': str(e),
                'text': text,
                'audio_path': audio_path,
                'language': language
            })
    
    return results

def main():
    parser = argparse.ArgumentParser(description="ë‹¤êµ­ì–´ ë©€í‹°ëª¨ë‹¬ ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ ì¶”ë¡ ")
    
    # ëª¨ë¸ ê´€ë ¨
    parser.add_argument('--model_path', type=str, required=True,
                       help='í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ (.pth)')
    parser.add_argument('--tokenizer_path', type=str, required=True,
                       help='í† í¬ë‚˜ì´ì € ê²½ë¡œ')
    
    # ì…ë ¥ ë°ì´í„°
    parser.add_argument('--text', type=str, 
                       help='ë¶„ì„í•  í…ìŠ¤íŠ¸ (ë‹¨ì¼ ì˜ˆì¸¡ìš©)')
    parser.add_argument('--audio_path', type=str,
                       help='ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ë‹¨ì¼ ì˜ˆì¸¡ìš©)')
    parser.add_argument('--language', type=str, default='Unknown',
                       help='ì–¸ì–´')
    
    # ë°°ì¹˜ ì˜ˆì¸¡ìš©
    parser.add_argument('--batch_file', type=str,
                       help='ë°°ì¹˜ ì˜ˆì¸¡ìš© í…ìŠ¤íŠ¸ íŒŒì¼ (ê° ì¤„: text\taudio_path\tlanguage)')
    
    # ê¸°íƒ€ ì„¤ì •
    parser.add_argument('--max_seq_len', type=int, default=512,
                       help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´')
    parser.add_argument('--device', type=str, default='auto',
                       help='ë””ë°”ì´ìŠ¤ (auto, cpu, cuda)')
    parser.add_argument('--output_file', type=str,
                       help='ê²°ê³¼ ì €ì¥ íŒŒì¼')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(
        args.model_path, 
        args.tokenizer_path, 
        device
    )
    
    if args.batch_file:
        # ë°°ì¹˜ ì˜ˆì¸¡
        print(f"\nğŸ“‚ ë°°ì¹˜ íŒŒì¼ ë¡œë“œ: {args.batch_file}")
        
        data_list = []
        with open(args.batch_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                    
                parts = line.split('\t')
                if len(parts) >= 2:
                    text = parts[0]
                    audio_path = parts[1]
                    language = parts[2] if len(parts) > 2 else 'Unknown'
                    
                    data_list.append({
                        'text': text,
                        'audio_path': audio_path,
                        'language': language
                    })
                else:
                    print(f"âš ï¸ ë¼ì¸ {line_num} í˜•ì‹ ì˜¤ë¥˜: {line}")
        
        print(f"âœ… {len(data_list)}ê°œ ìƒ˜í”Œ ë¡œë“œë¨")
        
        # ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤í–‰
        results = predict_batch(model, tokenizer, data_list, device, args.max_seq_len)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼:")
        print("=" * 80)
        
        correct_predictions = 0
        total_predictions = len(results)
        
        for i, result in enumerate(results, 1):
            if 'error' in result:
                print(f"{i:3d}. ì˜¤ë¥˜: {result['error']}")
            else:
                print(f"{i:3d}. [{result['language']:8s}] {result['prediction_label']} "
                      f"(ì‹ ë¢°ë„: {result['confidence']:.3f}) - {result['text'][:50]}...")
        
        # ê²°ê³¼ ì €ì¥
        if args.output_file:
            import json
            with open(args.output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {args.output_file}")
    
    else:
        # ë‹¨ì¼ ì˜ˆì¸¡
        if not args.text or not args.audio_path:
            print("âŒ ë‹¨ì¼ ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” --textì™€ --audio_pathê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ¯ ë‹¨ì¼ ì˜ˆì¸¡:")
        print(f"  ì–¸ì–´: {args.language}")
        print(f"  í…ìŠ¤íŠ¸: {args.text}")
        print(f"  ì˜¤ë””ì˜¤: {args.audio_path}")
        
        result = predict_single(
            model, tokenizer, args.text, args.audio_path, device, args.max_seq_len
        )
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:")
        print("=" * 50)
        print(f"ì˜ˆì¸¡: {result['prediction_label']}")
        print(f"ì‹ ë¢°ë„: {result['confidence']:.3f}")
        print(f"ì •ìƒ í™•ë¥ : {result['probabilities']['ì •ìƒ']:.3f}")
        print(f"ì¹˜ë§¤ í™•ë¥ : {result['probabilities']['ì¹˜ë§¤']:.3f}")
        
        # ê²°ê³¼ ì €ì¥
        if args.output_file:
            import json
            with open(args.output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {args.output_file}")

if __name__ == '__main__':
    main()
