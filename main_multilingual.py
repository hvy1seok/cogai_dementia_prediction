import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from transformers import get_linear_schedule_with_warmup
import hydra
from omegaconf import DictConfig, OmegaConf
import random
import numpy as np
import os
import argparse
from pathlib import Path

from dataset_multilingual import (
    prepare_multilingual_dataset, 
    collate_fn_multilingual,
    create_stratified_split_multilingual,
    create_cross_lingual_split
)
from models_multilingual import (
    MultilingualMultimodalModel, 
    train_multilingual_model,
    train_cross_lingual_model
)

def set_seed(seed):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def create_dataloaders(dataset, train_indices, val_indices, test_indices, batch_size, num_workers=0):
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    
    train_subset = Subset(dataset, train_indices)
    val_subset = Subset(dataset, val_indices)
    test_subset = Subset(dataset, test_indices)
    
    train_loader = DataLoader(
        train_subset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn_multilingual,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    val_loader = DataLoader(
        val_subset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn_multilingual,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    test_loader = DataLoader(
        test_subset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn_multilingual,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    return train_loader, val_loader, test_loader

def main():
    parser = argparse.ArgumentParser(description="ë‹¤êµ­ì–´ ë©€í‹°ëª¨ë‹¬ ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸")
    
    # ë°ì´í„° ê´€ë ¨ ì„¤ì •
    parser.add_argument('--data_dir', type=str, default='../training_dset', 
                       help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--max_seq_len', type=int, default=512, 
                       help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´')
    parser.add_argument('--batch_size', type=int, default=16, 
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_workers', type=int, default=4, 
                       help='ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜')
    
    # ëª¨ë¸ ê´€ë ¨ ì„¤ì •
    parser.add_argument('--text_model_type', type=int, default=1, choices=[1, 2],
                       help='í…ìŠ¤íŠ¸ ëª¨ë¸ íƒ€ì… (1: BERT only, 2: BERT + LSTM)')
    parser.add_argument('--dropout', type=float, default=0.3, 
                       help='ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨')
    
    # í›ˆë ¨ ê´€ë ¨ ì„¤ì •
    parser.add_argument('--num_epochs', type=int, default=100, 
                       help='í›ˆë ¨ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=2e-5, 
                       help='í•™ìŠµë¥ ')
    parser.add_argument('--weight_decay', type=float, default=0.01, 
                       help='ê°€ì¤‘ì¹˜ ê°ì‡ ')
    parser.add_argument('--warmup_steps', type=int, default=100, 
                       help='ì›Œë°ì—… ìŠ¤í… ìˆ˜')
    parser.add_argument('--seed', type=int, default=42, 
                       help='ëœë¤ ì‹œë“œ')
    
    # ì‹¤í—˜ ëª¨ë“œ ì„¤ì •
    parser.add_argument('--mode', type=str, default='all_languages', 
                       choices=['all_languages', 'cross_lingual'],
                       help='ì‹¤í—˜ ëª¨ë“œ')
    parser.add_argument('--languages', nargs='+', default=['English', 'Greek', 'Spanish', 'Mandarin'],
                       help='ì‚¬ìš©í•  ì–¸ì–´ ëª©ë¡')
    parser.add_argument('--train_languages', nargs='+', 
                       help='Cross-lingual ëª¨ë“œì—ì„œ í›ˆë ¨ì— ì‚¬ìš©í•  ì–¸ì–´')
    parser.add_argument('--test_languages', nargs='+',
                       help='Cross-lingual ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì–¸ì–´')
    
    # ê¸°íƒ€ ì„¤ì •
    parser.add_argument('--device', type=str, default='auto',
                       help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (auto, cpu, cuda)')
    parser.add_argument('--save_dir', type=str, default='checkpoints',
                       help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ì‹œë“œ ê³ ì •
    set_seed(args.seed)
    print(f"ğŸ² ëœë¤ ì‹œë“œ: {args.seed}")
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.save_dir, exist_ok=True)
    
    # =================== ë°ì´í„° ì¤€ë¹„ ===================
    print(f"\nğŸ“‚ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    print(f"  ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
    print(f"  ì–¸ì–´: {args.languages}")
    print(f"  ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {args.max_seq_len}")
    
    # ë‹¤êµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = prepare_multilingual_dataset(
        data_dir=args.data_dir,
        max_seq_len=args.max_seq_len,
        languages=args.languages
    )
    
    print(f"âœ… ì´ {len(dataset)}ê°œ ìƒ˜í”Œ ë¡œë“œë¨")
    
    # =================== ë°ì´í„° ë¶„í•  ===================
    if args.mode == 'all_languages':
        print(f"\nğŸŒ ì „ì²´ ì–¸ì–´ í™˜ì ë‹¨ìœ„ Stratified Split ëª¨ë“œ")
        
        # í™˜ì ë‹¨ìœ„ stratified split (7:1:2)
        train_indices, val_indices, test_indices = create_stratified_split_multilingual(
            dataset.data,
            train_split=0.7,
            val_split=0.1,
            test_split=0.2,
            random_seed=args.seed
        )
        
        experiment_name = f"AllLanguages_{'_'.join(args.languages)}"
        train_languages = args.languages
        test_languages = args.languages
        
    elif args.mode == 'cross_lingual':
        print(f"\nğŸŒ Cross-lingual ëª¨ë“œ")
        
        if not args.train_languages or not args.test_languages:
            raise ValueError("Cross-lingual ëª¨ë“œì—ì„œëŠ” --train_languagesì™€ --test_languagesë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # Cross-lingual split (7:1:2)
        train_indices, val_indices, test_indices = create_cross_lingual_split(
            dataset.data,
            train_languages=args.train_languages,
            test_languages=args.test_languages,
            random_seed=args.seed
        )
        
        experiment_name = f"CrossLingual_Train_{'_'.join(args.train_languages)}_Test_{'_'.join(args.test_languages)}"
        train_languages = args.train_languages
        test_languages = args.test_languages
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader, val_loader, test_loader = create_dataloaders(
        dataset, train_indices, val_indices, test_indices,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    
    print(f"\nğŸ“Š ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"  í›ˆë ¨: {len(train_loader)} ë°°ì¹˜ ({len(train_indices)} ìƒ˜í”Œ)")
    print(f"  ê²€ì¦: {len(val_loader)} ë°°ì¹˜ ({len(val_indices)} ìƒ˜í”Œ)")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_loader)} ë°°ì¹˜ ({len(test_indices)} ìƒ˜í”Œ)")
    
    # =================== ëª¨ë¸ ì´ˆê¸°í™” ===================
    print(f"\nğŸ§  ëª¨ë¸ ì´ˆê¸°í™”...")
    
    model = MultilingualMultimodalModel(
        text_model_type=args.text_model_type,
        dropout=args.dropout
    ).to(device)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"  í…ìŠ¤íŠ¸ ëª¨ë¸ íƒ€ì…: {args.text_model_type}")
    print(f"  ë“œë¡­ì•„ì›ƒ: {args.dropout}")
    print(f"  ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"  í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    
    # =================== ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ===================
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay,
        eps=1e-8
    )
    
    # ì´ ìŠ¤í… ìˆ˜ ê³„ì‚°
    total_steps = len(train_loader) * args.num_epochs
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=total_steps
    )
    
    # ì†ì‹¤ í•¨ìˆ˜
    criterion = nn.BCEWithLogitsLoss()
    
    print(f"\nâš™ï¸ í›ˆë ¨ ì„¤ì •:")
    print(f"  ì˜µí‹°ë§ˆì´ì €: AdamW")
    print(f"  í•™ìŠµë¥ : {args.learning_rate}")
    print(f"  ê°€ì¤‘ì¹˜ ê°ì‡ : {args.weight_decay}")
    print(f"  ì›Œë°ì—… ìŠ¤í…: {args.warmup_steps}")
    print(f"  ì´ ìŠ¤í…: {total_steps}")
    print(f"  ì†ì‹¤ í•¨ìˆ˜: BCEWithLogitsLoss")
    
    # =================== ëª¨ë¸ í›ˆë ¨ ===================
    print(f"\nğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    print(f"  ì‹¤í—˜ëª…: {experiment_name}")
    print(f"  ì—í¬í¬: {args.num_epochs}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    
    if args.mode == 'cross_lingual':
        # Cross-lingual í›ˆë ¨
        model, best_val_auc, final_metrics = train_cross_lingual_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=test_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            loss_fn=criterion,
            num_epochs=args.num_epochs,
            device=device,
            train_languages=train_languages,
            test_languages=test_languages
        )
    else:
        # ì „ì²´ ì–¸ì–´ í›ˆë ¨
        model, best_val_auc, final_metrics = train_multilingual_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=test_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            loss_fn=criterion,
            num_epochs=args.num_epochs,
            device=device,
            experiment_name=experiment_name
        )
    
    # =================== ê²°ê³¼ ì €ì¥ ===================
    results = {
        'experiment_name': experiment_name,
        'mode': args.mode,
        'languages': args.languages,
        'train_languages': train_languages,
        'test_languages': test_languages,
        'best_val_auc': best_val_auc,
        'final_metrics': final_metrics,
        'args': vars(args)
    }
    
    results_path = os.path.join(args.save_dir, f'results_{experiment_name}.json')
    import json
    with open(results_path, 'w', encoding='utf-8') as f:
        # numpy ê°’ë“¤ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
        json_results = {}
        for key, value in results.items():
            if isinstance(value, np.ndarray):
                json_results[key] = value.tolist()
            elif isinstance(value, np.floating):
                json_results[key] = float(value)
            elif isinstance(value, np.integer):
                json_results[key] = int(value)
            else:
                json_results[key] = value
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}")
    
    # =================== ìµœì¢… ìš”ì•½ ===================
    print(f"\nğŸ‰ ì‹¤í—˜ ì™„ë£Œ!")
    print(f"  ì‹¤í—˜ëª…: {experiment_name}")
    print(f"  ìµœê³  ê²€ì¦ AUC: {best_val_auc:.4f}")
    print(f"  ëª¨ë¸ ì €ì¥: best_model_{experiment_name}.pth")
    print(f"  ê²°ê³¼ ì €ì¥: {results_path}")
    
    if args.mode == 'cross_lingual':
        print(f"\nğŸŒ Cross-lingual ì„±ëŠ¥:")
        print(f"  í›ˆë ¨ ì–¸ì–´: {train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {test_languages}")
        
        # í…ŒìŠ¤íŠ¸ ì–¸ì–´ë³„ ì„±ëŠ¥ ì¶œë ¥
        for lang in test_languages:
            if f'{lang}_accuracy' in final_metrics:
                acc = final_metrics[f'{lang}_accuracy']
                auc = final_metrics.get(f'{lang}_auc', 0.0)
                f1 = final_metrics.get(f'{lang}_f1', 0.0)
                samples = final_metrics.get(f'{lang}_samples', 0)
                print(f"    {lang}: Acc={acc:.3f}, AUC={auc:.3f}, F1={f1:.3f} ({samples} samples)")
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
