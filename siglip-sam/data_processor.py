"""
SigLIP-SAMìš© ë°ì´í„° ì²˜ë¦¬ê¸°
ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ê³  í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì²˜ë¦¬
training_dset í´ë” êµ¬ì¡°ì— ë§ì¶° ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©
"""
import os
import torch
import librosa
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
from transformers import AutoProcessor  # SigLIP2 ì§€ì›
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from collections import Counter
import soundfile as sf
from language_parsers import parse_all_languages, get_language_parser

class AudioToMelSpectrogram:
    """ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 n_mels: int = 128,
                 n_fft: int = 2048,
                 hop_length: int = 512,
                 fmin: float = 0.0,
                 fmax: float = 8000.0,
                 image_size: int = 224):
        
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.fmin = fmin
        self.fmax = fmax
        self.image_size = image_size
    
    def audio_to_melspectrogram(self, audio_path: str) -> np.ndarray:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜"""
        try:
            # .npy íŒŒì¼ì¸ ê²½ìš° ì§ì ‘ ë¡œë“œ
            if audio_path.endswith('.npy'):
                audio = np.load(audio_path)
                if len(audio.shape) > 1:
                    audio = audio.flatten()
            else:
                # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
                audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # ë©œìŠ¤í™í† ê·¸ë¨ ê³„ì‚°
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_mels=self.n_mels,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                fmin=self.fmin,
                fmax=self.fmax
            )
            
            # dB ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            # ì •ê·œí™” (0-1 ë²”ìœ„)
            mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())
            
            return mel_spec_norm
            
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜ {audio_path}: {e}")
            # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ ë°˜í™˜
            return np.zeros((self.n_mels, 100))
    
    def melspectrogram_to_image(self, mel_spec: np.ndarray) -> Image.Image:
        """ë©œìŠ¤í™í† ê·¸ë¨ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # ë©œìŠ¤í™í† ê·¸ë¨ì„ 3ì±„ë„ RGB ì´ë¯¸ì§€ë¡œ ë³€í™˜
            mel_rgb = np.stack([mel_spec, mel_spec, mel_spec], axis=-1)
            
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            mel_image = Image.fromarray((mel_rgb * 255).astype(np.uint8))
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            mel_image = mel_image.resize((self.image_size, self.image_size))
            
            return mel_image
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ë¹ˆ ì´ë¯¸ì§€ ë°˜í™˜
            empty_image = Image.new('RGB', (self.image_size, self.image_size), color='black')
            return empty_image
    
    def process_audio(self, audio_path: str) -> Image.Image:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•´ì„œ PIL ì´ë¯¸ì§€ ë°˜í™˜"""
        mel_spec = self.audio_to_melspectrogram(audio_path)
        image = self.melspectrogram_to_image(mel_spec)
        return image

class DementiaDataset(Dataset):
    """ì¹˜ë§¤ ì§„ë‹¨ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 data_dir: str,
                 processor: AutoProcessor,  # SigLIP2 ì§€ì›
                 audio_processor: AudioToMelSpectrogram,
                 split: str = "train",
                 max_length: int = 512,
                 languages: Optional[List[str]] = None):
        
        self.data_dir = data_dir
        self.processor = processor
        self.audio_processor = audio_processor
        self.split = split
        self.max_length = max_length
        self.languages = languages or ["English", "Greek", "Spanish", "Mandarin"]
        
        # ë°ì´í„° ë¡œë“œ
        self.data = self._load_data()
        
    def _load_data(self) -> List[Dict]:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ - ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©"""
        print("ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ training_dset ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íŒŒì‹±
        data = parse_all_languages(self.data_dir, self.languages)
        
        print(f"ì´ {len(data)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
        
        # ì–¸ì–´ë³„ í†µê³„ ì¶œë ¥
        language_stats = {}
        for item in data:
            lang = item['language']
            language_stats[lang] = language_stats.get(lang, 0) + 1
        
        print("ì–¸ì–´ë³„ ìƒ˜í”Œ ìˆ˜:")
        for lang, count in language_stats.items():
            print(f"  {lang}: {count}ê°œ")
        
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict:
        item = self.data[idx]
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ì²˜ë¦¬
        audio_path = item['audio_path']
        
        # .npy íŒŒì¼ì¸ ê²½ìš° (ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ íŠ¹ì§•) ì§ì ‘ ë¡œë“œ
        if audio_path.endswith('.npy'):
            try:
                audio_spec = np.load(audio_path)
                # ì´ë¯¸ ë©œìŠ¤í™í† ê·¸ë¨ í˜•íƒœë¼ë©´ ë°”ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                if len(audio_spec.shape) == 2:
                    image = self.audio_processor.melspectrogram_to_image(audio_spec)
                else:
                    # 3ì°¨ì›ì¸ ê²½ìš° (3, H, W) -> (H, W) ë³€í™˜
                    if audio_spec.shape[0] == 3:
                        # RGB ì±„ë„ í‰ê·  ë˜ëŠ” ì²« ë²ˆì§¸ ì±„ë„ ì‚¬ìš©
                        audio_spec_2d = np.mean(audio_spec, axis=0)
                    else:
                        audio_spec_2d = audio_spec[0] if len(audio_spec.shape) == 3 else audio_spec
                    image = self.audio_processor.melspectrogram_to_image(audio_spec_2d)
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ë¡œë“œ ì˜¤ë¥˜ {audio_path}: {e}")
                # ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ëŒ€ì²´
                empty_spec = np.zeros((self.audio_processor.n_mels, 100))
                image = self.audio_processor.melspectrogram_to_image(empty_spec)
        else:
            # ì¼ë°˜ ì˜¤ë””ì˜¤ íŒŒì¼ì¸ ê²½ìš° ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜
            mel_spec = self.audio_processor.audio_to_melspectrogram(audio_path)
            image = self.audio_processor.melspectrogram_to_image(mel_spec)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì–¸ì–´ ë¬´ê´€ì„ ìœ„í•´ ì†Œë¬¸ì ë³€í™˜)
        text = item['text'].lower().strip()
        
        # SigLIP2 í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬
        inputs = self.processor(
            text=text,
            images=image,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )
        
        # ë°°ì¹˜ ì°¨ì› ì œê±°
        for key in inputs:
            if isinstance(inputs[key], torch.Tensor):
                inputs[key] = inputs[key].squeeze(0)
        
        # SigLIP2ëŠ” attention_maskê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒì„±
        if 'attention_mask' not in inputs and 'input_ids' in inputs:
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
        
        # ë¼ë²¨ ì¶”ê°€
        inputs['labels'] = torch.tensor(item['label'], dtype=torch.long)
        inputs['language'] = item['language']
        
        return inputs

def create_stratified_split(dataset, train_split: float = 0.7, val_split: float = 0.1, test_split: float = 0.2, random_seed: int = 42):
    """
    í™˜ì ë‹¨ìœ„ + ì–¸ì–´ë³„ + ë¼ë²¨ë³„ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ stratified split ìˆ˜í–‰ (train:val:test = 7:1:2)
    Speaker-independent evaluationì„ ìœ„í•´ ë™ì¼ í™˜ìì˜ ëª¨ë“  ìƒ˜í”Œì´ í•œ ì„¸íŠ¸ì—ë§Œ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥
    """
    # ë°ì´í„°ì—ì„œ í™˜ì, ì–¸ì–´, ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
    patients = []
    languages = []
    labels = []
    
    for i in range(len(dataset)):
        item = dataset.data[i]
        patients.append(item.get('patient_id', item['file_id']))  # í™˜ì ID, ì—†ìœ¼ë©´ file_id ì‚¬ìš©
        languages.append(item['language'])
        labels.append(item['label'])
    
    # í™˜ìë³„ë¡œ ê·¸ë£¹í™”
    from collections import defaultdict
    patient_groups = defaultdict(list)
    for i, patient_id in enumerate(patients):
        patient_groups[patient_id].append(i)
    
    print(f"\nğŸ‘¥ í™˜ì ë‹¨ìœ„ ë¶„í• :")
    print(f"  ì „ì²´ í™˜ì ìˆ˜: {len(patient_groups)}")
    print(f"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
    
    # í™˜ìë³„ ë©”íƒ€ë°ì´í„° ìƒì„±
    patient_metadata = []
    for patient_id, indices in patient_groups.items():
        first_idx = indices[0]
        patient_lang = languages[first_idx]
        patient_label = labels[first_idx]
        sample_count = len(indices)
        
        patient_metadata.append({
            'patient_id': patient_id,
            'language': patient_lang,
            'label': patient_label,
            'indices': indices,
            'sample_count': sample_count
        })
    
    # í™˜ì ë‹¨ìœ„ë¡œ stratify í‚¤ ìƒì„± (ì–¸ì–´-ë¼ë²¨ ì¡°í•©)
    patient_stratify_keys = [f"{p['language']}_{p['label']}" for p in patient_metadata]
    patient_indices_list = list(range(len(patient_metadata)))
    
    # í™˜ì ë‹¨ìœ„ë¡œ ë¶„í•  ìˆ˜í–‰
    from sklearn.model_selection import train_test_split
    
    # ì²« ë²ˆì§¸ ë¶„í• : train vs (val + test) - í™˜ì ë‹¨ìœ„
    train_patient_indices, temp_patient_indices = train_test_split(
        patient_indices_list,
        test_size=val_split + test_split,
        stratify=patient_stratify_keys,
        random_state=random_seed
    )
    
    # temp í™˜ìë“¤ì˜ stratify í‚¤ ìƒì„±
    temp_patient_stratify_keys = [patient_stratify_keys[i] for i in temp_patient_indices]
    
    # ë‘ ë²ˆì§¸ ë¶„í• : val vs test - í™˜ì ë‹¨ìœ„
    val_patient_indices, test_patient_indices = train_test_split(
        temp_patient_indices,
        test_size=test_split / (val_split + test_split),  # test ë¹„ìœ¨ ì¡°ì •
        stratify=temp_patient_stratify_keys,
        random_state=random_seed
    )
    
    # í™˜ì ì¸ë±ìŠ¤ë¥¼ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    train_indices = []
    val_indices = []
    test_indices = []
    
    for patient_idx in train_patient_indices:
        train_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in val_patient_indices:
        val_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in test_patient_indices:
        test_indices.extend(patient_metadata[patient_idx]['indices'])
    
    # ë¶„í•  ê²°ê³¼ í†µê³„ ì¶œë ¥ (í™˜ì ë‹¨ìœ„ í¬í•¨)
    print(f"\nğŸ“Š í™˜ì ë‹¨ìœ„ Stratified Split ê²°ê³¼ (7:1:2):")
    print(f"  ì „ì²´ ë°ì´í„°: {len(dataset)} ìƒ˜í”Œ, {len(patient_groups)} í™˜ì")
    print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_indices)} ìƒ˜í”Œ ({len(train_indices)/len(dataset)*100:.1f}%), {len(train_patient_indices)} í™˜ì")
    print(f"  ê²€ì¦ ë°ì´í„°: {len(val_indices)} ìƒ˜í”Œ ({len(val_indices)/len(dataset)*100:.1f}%), {len(val_patient_indices)} í™˜ì")
    print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_indices)} ìƒ˜í”Œ ({len(test_indices)/len(dataset)*100:.1f}%), {len(test_patient_indices)} í™˜ì")
    
    # í™˜ì ë¶„í•  ê²€ì¦: ì¤‘ë³µ í™•ì¸
    train_patients = set([patient_metadata[i]['patient_id'] for i in train_patient_indices])
    val_patients = set([patient_metadata[i]['patient_id'] for i in val_patient_indices])
    test_patients = set([patient_metadata[i]['patient_id'] for i in test_patient_indices])
    
    overlap_train_val = train_patients & val_patients
    overlap_train_test = train_patients & test_patients
    overlap_val_test = val_patients & test_patients
    
    if overlap_train_val or overlap_train_test or overlap_val_test:
        print(f"âš ï¸ í™˜ì ì¤‘ë³µ ë°œê²¬!")
        if overlap_train_val:
            print(f"   Train-Val ì¤‘ë³µ: {overlap_train_val}")
        if overlap_train_test:
            print(f"   Train-Test ì¤‘ë³µ: {overlap_train_test}")
        if overlap_val_test:
            print(f"   Val-Test ì¤‘ë³µ: {overlap_val_test}")
    else:
        print(f"âœ… í™˜ì ë‹¨ìœ„ ë¶„í•  ì„±ê³µ: ì¤‘ë³µ ì—†ìŒ")
    
    # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì–¸ì–´ë³„ ë¶„í¬ í™•ì¸
    train_lang_dist = Counter([languages[i] for i in train_indices])
    val_lang_dist = Counter([languages[i] for i in val_indices])
    test_lang_dist = Counter([languages[i] for i in test_indices])
    train_label_dist = Counter([labels[i] for i in train_indices])
    val_label_dist = Counter([labels[i] for i in val_indices])
    test_label_dist = Counter([labels[i] for i in test_indices])
    
    print(f"\nğŸ“Š ì–¸ì–´ë³„ ë¶„í¬:")
    for lang in set(languages):
        train_count = train_lang_dist[lang]
        val_count = val_lang_dist[lang]
        test_count = test_lang_dist[lang]
        total_count = train_count + val_count + test_count
        if total_count > 0:
            print(f"  {lang}: í›ˆë ¨ {train_count}ê°œ ({train_count/total_count*100:.1f}%), "
                  f"ê²€ì¦ {val_count}ê°œ ({val_count/total_count*100:.1f}%), "
                  f"í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_count/total_count*100:.1f}%)")
    
    print(f"\nğŸ“Š ë¼ë²¨ë³„ ë¶„í¬:")
    label_names = {0: 'ì •ìƒ', 1: 'ì¹˜ë§¤'}
    for label in [0, 1]:
        train_count = train_label_dist[label]
        val_count = val_label_dist[label]
        test_count = test_label_dist[label]
        total_count = train_count + val_count + test_count
        if total_count > 0:
            print(f"  {label_names[label]}: í›ˆë ¨ {train_count}ê°œ ({train_count/total_count*100:.1f}%), "
                  f"ê²€ì¦ {val_count}ê°œ ({val_count/total_count*100:.1f}%), "
                  f"í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_count/total_count*100:.1f}%)")
    
    return train_indices, val_indices, test_indices

def create_dataloaders(data_dir: str,
                      processor: AutoProcessor,
                      config,
                      cross_lingual_mode: bool = False,
                      train_languages: List[str] = None,
                      test_languages: List[str] = None) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """ë°ì´í„°ë¡œë” ìƒì„± (ì¼ë°˜ ëª¨ë“œ ë˜ëŠ” Cross-Lingual ëª¨ë“œ)"""
    
    audio_processor = AudioToMelSpectrogram(
        sample_rate=config.sample_rate,
        n_mels=config.n_mels,
        n_fft=config.n_fft,
        hop_length=config.hop_length,
        fmin=config.fmin,
        fmax=config.fmax,
        image_size=config.image_size
    )
    
    if cross_lingual_mode:
        print("ğŸŒ Cross-Lingual ëª¨ë“œ: ì–¸ì–´ë³„ë¡œ ë¶„ë¦¬ëœ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±")
        print(f"  í›ˆë ¨ ì–¸ì–´: {train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {test_languages}")
        
        # í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± (train + val)
        train_full_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=train_languages
        )
        
        # í›ˆë ¨ ë°ì´í„°ë¥¼ train:val = 7:1ë¡œ ë¶„í• 
        train_indices, val_indices, _ = create_stratified_split(
            train_full_dataset,
            train_split=0.875,  # 7/(7+1) = 0.875
            val_split=0.125,    # 1/(7+1) = 0.125
            test_split=0.0,     # Cross-lingualì—ì„œëŠ” testëŠ” ë‹¤ë¥¸ ì–¸ì–´
            random_seed=config.random_seed
        )
        
        train_dataset = Subset(train_full_dataset, train_indices)
        val_dataset = Subset(train_full_dataset, val_indices)
        
        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±
        test_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=test_languages
        )
        
        print(f"ğŸ“Š Cross-Lingual ë°ì´í„° ë¶„í• :")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {train_languages})")
        print(f"  ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {train_languages})")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {test_languages})")
        
    else:
        print("ğŸ¯ ì¼ë°˜ ëª¨ë“œ: Stratified Split ìˆ˜í–‰ ì¤‘...")
        
        # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
        full_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=config.languages
        )
        
        # Stratified ë°ì´í„° ë¶„í•  (ì–¸ì–´ë³„ + ë¼ë²¨ë³„ ë¹„ìœ¨ ìœ ì§€)
        train_indices, val_indices, test_indices = create_stratified_split(
            full_dataset, 
            train_split=config.train_split,
            val_split=config.val_split,
            test_split=config.test_split,
            random_seed=config.random_seed
        )
        
        # Subsetìœ¼ë¡œ ë°ì´í„°ì…‹ ë¶„í• 
        train_dataset = Subset(full_dataset, train_indices)
        val_dataset = Subset(full_dataset, val_indices)
        test_dataset = Subset(full_dataset, test_indices)
        
        print(f"ğŸ“Š ì¼ë°˜ ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ ({config.train_split*100:.0f}%)")
        print(f"  ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ìƒ˜í”Œ ({config.val_split*100:.0f}%)")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)} ìƒ˜í”Œ ({config.test_split*100:.0f}%)")
        print(f"  ì „ì²´ ë°ì´í„°: {len(full_dataset)} ìƒ˜í”Œ")
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory
    )
    
    return train_loader, val_loader, test_loader
