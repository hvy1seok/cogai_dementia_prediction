"""
SigLIP-SAMìš© ë°ì´í„° ì²˜ë¦¬ê¸°
ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ê³  í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì²˜ë¦¬
training_dset í´ë” êµ¬ì¡°ì— ë§ì¶° ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©
"""
import os
import torch
import librosa
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
from transformers import AutoProcessor  # SigLIP2 ì§€ì›
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from collections import Counter
import soundfile as sf
from language_parsers import parse_all_languages, get_language_parser

class AudioToMelSpectrogram:
    """ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 n_mels: int = 128,
                 n_fft: int = 2048,
                 hop_length: int = 512,
                 fmin: float = 0.0,
                 fmax: float = 8000.0,
                 image_size: int = 224):
        
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.fmin = fmin
        self.fmax = fmax
        self.image_size = image_size
    
    def audio_to_melspectrogram(self, audio_path: str) -> np.ndarray:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜"""
        try:
            # .npy íŒŒì¼ì¸ ê²½ìš° ì§ì ‘ ë¡œë“œ
            if audio_path.endswith('.npy'):
                audio = np.load(audio_path)
                if len(audio.shape) > 1:
                    audio = audio.flatten()
            else:
                # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
                audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # ë©œìŠ¤í™í† ê·¸ë¨ ê³„ì‚°
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_mels=self.n_mels,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                fmin=self.fmin,
                fmax=self.fmax
            )
            
            # dB ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            # ì •ê·œí™” (0-1 ë²”ìœ„)
            mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())
            
            return mel_spec_norm
            
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜ {audio_path}: {e}")
            # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ ë°˜í™˜
            return np.zeros((self.n_mels, 100))
    
    def melspectrogram_to_image(self, mel_spec: np.ndarray) -> Image.Image:
        """ë©œìŠ¤í™í† ê·¸ë¨ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            # ë©œìŠ¤í™í† ê·¸ë¨ì„ 3ì±„ë„ RGB ì´ë¯¸ì§€ë¡œ ë³€í™˜
            mel_rgb = np.stack([mel_spec, mel_spec, mel_spec], axis=-1)
            
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            mel_image = Image.fromarray((mel_rgb * 255).astype(np.uint8))
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            mel_image = mel_image.resize((self.image_size, self.image_size))
            
            return mel_image
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ë¹ˆ ì´ë¯¸ì§€ ë°˜í™˜
            empty_image = Image.new('RGB', (self.image_size, self.image_size), color='black')
            return empty_image
    
    def process_audio(self, audio_path: str) -> Image.Image:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•´ì„œ PIL ì´ë¯¸ì§€ ë°˜í™˜"""
        mel_spec = self.audio_to_melspectrogram(audio_path)
        image = self.melspectrogram_to_image(mel_spec)
        return image

class DementiaDataset(Dataset):
    """ì¹˜ë§¤ ì§„ë‹¨ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 data_dir: str,
                 processor: AutoProcessor,  # SigLIP2 ì§€ì›
                 audio_processor: AudioToMelSpectrogram,
                 split: str = "train",
                 max_length: int = 512,
                 languages: Optional[List[str]] = None):
        
        self.data_dir = data_dir
        self.processor = processor
        self.audio_processor = audio_processor
        self.split = split
        self.max_length = max_length
        self.languages = languages or ["English", "Greek", "Spanish", "Mandarin"]
        
        # ë°ì´í„° ë¡œë“œ
        self.data = self._load_data()
        
    def _load_data(self) -> List[Dict]:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ - ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©"""
        print("ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ training_dset ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íŒŒì‹±
        data = parse_all_languages(self.data_dir, self.languages)
        
        print(f"ì´ {len(data)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
        
        # ì–¸ì–´ë³„ í†µê³„ ì¶œë ¥
        language_stats = {}
        for item in data:
            lang = item['language']
            language_stats[lang] = language_stats.get(lang, 0) + 1
        
        print("ì–¸ì–´ë³„ ìƒ˜í”Œ ìˆ˜:")
        for lang, count in language_stats.items():
            print(f"  {lang}: {count}ê°œ")
        
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict:
        item = self.data[idx]
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ì²˜ë¦¬
        audio_path = item['audio_path']
        
        # .npy íŒŒì¼ì¸ ê²½ìš° (ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ íŠ¹ì§•) ì§ì ‘ ë¡œë“œ
        if audio_path.endswith('.npy'):
            try:
                audio_spec = np.load(audio_path)
                # ì´ë¯¸ ë©œìŠ¤í™í† ê·¸ë¨ í˜•íƒœë¼ë©´ ë°”ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                if len(audio_spec.shape) == 2:
                    image = self.audio_processor.melspectrogram_to_image(audio_spec)
                else:
                    # 3ì°¨ì›ì¸ ê²½ìš° (3, H, W) -> (H, W) ë³€í™˜
                    if audio_spec.shape[0] == 3:
                        # RGB ì±„ë„ í‰ê·  ë˜ëŠ” ì²« ë²ˆì§¸ ì±„ë„ ì‚¬ìš©
                        audio_spec_2d = np.mean(audio_spec, axis=0)
                    else:
                        audio_spec_2d = audio_spec[0] if len(audio_spec.shape) == 3 else audio_spec
                    image = self.audio_processor.melspectrogram_to_image(audio_spec_2d)
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ë¡œë“œ ì˜¤ë¥˜ {audio_path}: {e}")
                # ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ëŒ€ì²´
                empty_spec = np.zeros((self.audio_processor.n_mels, 100))
                image = self.audio_processor.melspectrogram_to_image(empty_spec)
        else:
            # ì¼ë°˜ ì˜¤ë””ì˜¤ íŒŒì¼ì¸ ê²½ìš° ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜
            mel_spec = self.audio_processor.audio_to_melspectrogram(audio_path)
            image = self.audio_processor.melspectrogram_to_image(mel_spec)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì–¸ì–´ ë¬´ê´€ì„ ìœ„í•´ ì†Œë¬¸ì ë³€í™˜)
        text = item['text'].lower().strip()
        
        # SigLIP2 í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬
        inputs = self.processor(
            text=text,
            images=image,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )
        
        # ë°°ì¹˜ ì°¨ì› ì œê±°
        for key in inputs:
            if isinstance(inputs[key], torch.Tensor):
                inputs[key] = inputs[key].squeeze(0)
        
        # SigLIP2ëŠ” attention_maskê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒì„±
        if 'attention_mask' not in inputs and 'input_ids' in inputs:
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
        
        # ë¼ë²¨ ì¶”ê°€
        inputs['labels'] = torch.tensor(item['label'], dtype=torch.long)
        inputs['language'] = item['language']
        
        return inputs

def create_stratified_split(dataset, train_split: float = 0.8, random_seed: int = 42):
    """
    ì–¸ì–´ë³„ + ë¼ë²¨ë³„ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ stratified split ìˆ˜í–‰
    """
    # ë°ì´í„°ì—ì„œ ì–¸ì–´ì™€ ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
    languages = []
    labels = []
    
    for i in range(len(dataset)):
        item = dataset.data[i]
        languages.append(item['language'])
        labels.append(item['label'])
    
    # ì–¸ì–´-ë¼ë²¨ ì¡°í•©ìœ¼ë¡œ stratify í‚¤ ìƒì„±
    stratify_keys = [f"{lang}_{label}" for lang, label in zip(languages, labels)]
    
    # ì „ì²´ ì¸ë±ìŠ¤ ìƒì„±
    indices = list(range(len(dataset)))
    
    # Stratified split ìˆ˜í–‰
    train_indices, test_indices = train_test_split(
        indices,
        test_size=1-train_split,
        stratify=stratify_keys,
        random_state=random_seed
    )
    
    # ë¶„í•  ê²°ê³¼ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š Stratified Split ê²°ê³¼:")
    print(f"  ì „ì²´ ë°ì´í„°: {len(dataset)} ìƒ˜í”Œ")
    print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_indices)} ìƒ˜í”Œ ({len(train_indices)/len(dataset)*100:.1f}%)")
    print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_indices)} ìƒ˜í”Œ ({len(test_indices)/len(dataset)*100:.1f}%)")
    
    return train_indices, test_indices

def create_dataloaders(data_dir: str,
                      processor: AutoProcessor,
                      config,
                      cross_lingual_mode: bool = False,
                      train_languages: List[str] = None,
                      test_languages: List[str] = None) -> Tuple[DataLoader, DataLoader]:
    """ë°ì´í„°ë¡œë” ìƒì„± (ì¼ë°˜ ëª¨ë“œ ë˜ëŠ” Cross-Lingual ëª¨ë“œ)"""
    
    audio_processor = AudioToMelSpectrogram(
        sample_rate=config.sample_rate,
        n_mels=config.n_mels,
        n_fft=config.n_fft,
        hop_length=config.hop_length,
        fmin=config.fmin,
        fmax=config.fmax,
        image_size=config.image_size
    )
    
    if cross_lingual_mode:
        print("ğŸŒ Cross-Lingual ëª¨ë“œ: ì–¸ì–´ë³„ë¡œ ë¶„ë¦¬ëœ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±")
        print(f"  í›ˆë ¨ ì–¸ì–´: {train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {test_languages}")
        
        # í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=train_languages
        )
        
        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±
        test_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=test_languages
        )
        
        print(f"ğŸ“Š Cross-Lingual ë°ì´í„° ë¶„í• :")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {train_languages})")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {test_languages})")
        
    else:
        print("ğŸ¯ ì¼ë°˜ ëª¨ë“œ: Stratified Split ìˆ˜í–‰ ì¤‘...")
        
        # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
        full_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=config.languages
        )
        
        # Stratified ë°ì´í„° ë¶„í•  (ì–¸ì–´ë³„ + ë¼ë²¨ë³„ ë¹„ìœ¨ ìœ ì§€)
        train_indices, test_indices = create_stratified_split(
            full_dataset, 
            train_split=config.train_split,
            random_seed=config.random_seed
        )
        
        # Subsetìœ¼ë¡œ ë°ì´í„°ì…‹ ë¶„í• 
        train_dataset = Subset(full_dataset, train_indices)
        test_dataset = Subset(full_dataset, test_indices)
        
        print(f"ğŸ“Š ì¼ë°˜ ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ ({config.train_split*100:.0f}%)")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)} ìƒ˜í”Œ ({config.test_split*100:.0f}%)")
        print(f"  ì „ì²´ ë°ì´í„°: {len(full_dataset)} ìƒ˜í”Œ")
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory
    )
    
    return train_loader, test_loader
