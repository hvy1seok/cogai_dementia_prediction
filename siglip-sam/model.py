"""
SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸
ìˆœìˆ˜ PyTorch êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoProcessor
import numpy as np
from typing import Dict, Optional, Tuple, List
from sam_optimizer import SAM

# Lion Optimizer ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from lion_pytorch import Lion
    LION_AVAILABLE = True
    print("ğŸ¦ lion-pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    LION_AVAILABLE = False
    print("âš ï¸ lion-pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install lion-pytorchë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

class FocalLoss(nn.Module):
    """
    Focal Loss êµ¬í˜„ - ë¶ˆê· í˜• ë°ì´í„°ì…‹ì— íš¨ê³¼ì 
    alphaê°€ ë¦¬ìŠ¤íŠ¸/í…ì„œì¸ ê²½ìš° í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
    """
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        if isinstance(alpha, (list, tuple)):
            # í…ì„œë¥¼ ëª¨ë“ˆ íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡í•˜ì—¬ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
            self.register_buffer('alpha', torch.tensor(alpha, dtype=torch.float32))
        else:
            self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        
        # í´ë˜ìŠ¤ë³„ alpha ê°€ì¤‘ì¹˜ ì ìš©
        if hasattr(self, 'alpha') and isinstance(self.alpha, torch.Tensor):
            # alphaê°€ í…ì„œì¸ ê²½ìš° í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            # ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ë¥¼ ë§ì¶°ì¤Œ (ì•ˆì „ì„±ì„ ìœ„í•´)
            alpha_tensor = self.alpha.to(targets.device)
            alpha_t = alpha_tensor.gather(0, targets.long())
            focal_loss = alpha_t * (1-pt)**self.gamma * ce_loss
        else:
            # alphaê°€ ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹
            focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class SigLIP2ContrastiveLoss(nn.Module):
    """
    SigLIP2 ìŠ¤íƒ€ì¼ Contrastive Loss êµ¬í˜„
    - Sigmoid matching (CLIPì˜ softmax ëŒ€ì‹ )
    - Same-patient audio-text pairsë¥¼ positiveë¡œ ì²˜ë¦¬
    - Cross-modal representation alignment
    """
    def __init__(self, temperature: float = 0.07, use_sigmoid: bool = True):
        super(SigLIP2ContrastiveLoss, self).__init__()
        self.temperature = temperature
        self.use_sigmoid = use_sigmoid
        self.cross_entropy = nn.CrossEntropyLoss()
        self.bce_with_logits = nn.BCEWithLogitsLoss()
    
    def forward(self, audio_embeds: torch.Tensor, text_embeds: torch.Tensor, 
                patient_ids: List[str]) -> Dict[str, torch.Tensor]:
        """
        Args:
            audio_embeds: [batch_size, embed_dim] ì˜¤ë””ì˜¤ ì„ë² ë”©
            text_embeds: [batch_size, embed_dim] í…ìŠ¤íŠ¸ ì„ë² ë”©  
            patient_ids: [batch_size] í™˜ì ID ë¦¬ìŠ¤íŠ¸
        
        Returns:
            Dict containing contrastive loss and alignment metrics
        """
        batch_size = audio_embeds.shape[0]
        device = audio_embeds.device
        
        # L2 ì •ê·œí™”
        audio_embeds = F.normalize(audio_embeds, p=2, dim=1)
        text_embeds = F.normalize(text_embeds, p=2, dim=1)
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°: [batch_size, batch_size]
        similarity_matrix = torch.matmul(audio_embeds, text_embeds.T) / self.temperature
        
        # Positive pairs ë§ˆìŠ¤í¬ ìƒì„± (ê°™ì€ í™˜ì)
        positive_mask = torch.zeros(batch_size, batch_size, device=device)
        for i in range(batch_size):
            for j in range(batch_size):
                if patient_ids[i] == patient_ids[j]:
                    positive_mask[i, j] = 1.0
        
        # Negative pairs ë§ˆìŠ¤í¬ (ë‹¤ë¥¸ í™˜ì)
        negative_mask = 1.0 - positive_mask
        
        if self.use_sigmoid:
            # SigLIP ìŠ¤íƒ€ì¼: Sigmoid matching
            # Positive pairsëŠ” 1ì— ê°€ê¹ê²Œ, negative pairsëŠ” 0ì— ê°€ê¹ê²Œ
            positive_loss = self.bce_with_logits(
                similarity_matrix * positive_mask, 
                positive_mask
            )
            
            negative_loss = self.bce_with_logits(
                similarity_matrix * negative_mask,
                torch.zeros_like(negative_mask)
            )
            
            contrastive_loss = (positive_loss + negative_loss) / 2
            
        else:
            # CLIP ìŠ¤íƒ€ì¼: Softmax contrastive
            # Audio-to-text direction
            a2t_loss = self.cross_entropy(similarity_matrix, torch.arange(batch_size, device=device))
            
            # Text-to-audio direction  
            t2a_loss = self.cross_entropy(similarity_matrix.T, torch.arange(batch_size, device=device))
            
            contrastive_loss = (a2t_loss + t2a_loss) / 2
        
        # ì •ë ¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        with torch.no_grad():
            # Positive pairs í‰ê·  ìœ ì‚¬ë„
            positive_similarities = similarity_matrix * positive_mask
            positive_count = positive_mask.sum()
            avg_positive_sim = positive_similarities.sum() / (positive_count + 1e-8)
            
            # Negative pairs í‰ê·  ìœ ì‚¬ë„
            negative_similarities = similarity_matrix * negative_mask
            negative_count = negative_mask.sum()
            avg_negative_sim = negative_similarities.sum() / (negative_count + 1e-8)
            
            # ì •ë ¬ ì •ë„ (positive - negative)
            alignment_score = avg_positive_sim - avg_negative_sim
        
        return {
            'contrastive_loss': contrastive_loss,
            'avg_positive_similarity': avg_positive_sim,
            'avg_negative_similarity': avg_negative_sim,
            'alignment_score': alignment_score,
            'positive_pairs_count': positive_count,
            'negative_pairs_count': negative_count
        }

class SigLIPSAMDementiaClassifier(nn.Module):
    """
    SigLIP2 ê¸°ë°˜ ì¹˜ë§¤ ì§„ë‹¨ ë¶„ë¥˜ê¸° (SAM ì˜µí‹°ë§ˆì´ì € ì§€ì›)
    """
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # SigLIP2 ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ SigLIP2 ëª¨ë¸ ë¡œë“œ ì¤‘: {config.model_name}")
        self.siglip = AutoModel.from_pretrained(config.model_name)
        
        # ëª¨ë¸ ì„¤ì • í™•ì¸
        model_config = self.siglip.config
        if hasattr(model_config, 'projection_dim'):
            self.hidden_size = model_config.projection_dim
        elif hasattr(model_config, 'hidden_size'):
            self.hidden_size = model_config.hidden_size
        elif hasattr(model_config, 'vision_config') and hasattr(model_config.vision_config, 'hidden_size'):
            self.hidden_size = model_config.vision_config.hidden_size
        else:
            self.hidden_size = 768  # ê¸°ë³¸ê°’
        
        print(f"ğŸ“ Hidden size: {self.hidden_size}")
        
        # ì–¸ì–´ ì„ë² ë”© (ì„ íƒì )
        self.language_embedding = nn.Embedding(10, 512)  # ìµœëŒ€ 10ê°œ ì–¸ì–´ ì§€ì›
        self.language_projection = nn.Linear(512, self.hidden_size)
        
        # ë¶„ë¥˜ê¸°ëŠ” ë™ì ìœ¼ë¡œ ìƒì„±
        self.classifier = None
        
        # ì–¸ì–´ ID ë§¤í•‘
        self.language_to_id = {
            'English': 0, 'Greek': 1, 'Korean': 2, 'Spanish': 3, 'French': 4,
            'German': 5, 'Italian': 6, 'Portuguese': 7, 'Japanese': 8, 'Chinese': 9,
            'Mandarin': 9  # Chineseì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        }
        
        # ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‚˜ì¤‘ì— í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì´ˆê¸°í™”
        self.config = config
        self.criterion = None  # ë‚˜ì¤‘ì— ì„¤ì •
        
        # SigLIP2 Contrastive Learning ì„¤ì •
        self.use_contrastive = getattr(config, 'use_contrastive', True)
        self.contrastive_weight = getattr(config, 'contrastive_weight', 0.5)
        self.contrastive_temperature = getattr(config, 'contrastive_temperature', 0.07)
        
        if self.use_contrastive:
            self.contrastive_loss = SigLIP2ContrastiveLoss(
                temperature=self.contrastive_temperature,
                use_sigmoid=True  # SigLIP2 ìŠ¤íƒ€ì¼
            )
            print(f"ğŸ”— SigLIP2 Contrastive Learning í™œì„±í™”:")
            print(f"   ê°€ì¤‘ì¹˜: {self.contrastive_weight}")
            print(f"   ì˜¨ë„: {self.contrastive_temperature}")
        else:
            self.contrastive_loss = None
            print("âšª Contrastive Learning ë¹„í™œì„±í™”")
    
    def setup_loss_function(self, class_weights=None):
        """ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™” - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©"""
        if self.config.loss_type == "focal":
            if class_weights is not None and self.config.auto_class_weights:
                # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìë™ ì ìš©
                alpha = class_weights
                print(f"ğŸ¯ Focal Loss ì‚¬ìš©: alpha={alpha} (ìë™ ê³„ì‚°), gamma={self.config.focal_gamma}")
                print(f"   ì •ìƒ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {alpha[0]:.3f}, ì¹˜ë§¤ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {alpha[1]:.3f}")
            else:
                # ìˆ˜ë™ ì„¤ì • ë˜ëŠ” ê· ë“± ê°€ì¤‘ì¹˜
                alpha = self.config.focal_alpha
                print(f"ğŸ¯ Focal Loss ì‚¬ìš©: alpha={alpha} (ìˆ˜ë™ ì„¤ì •), gamma={self.config.focal_gamma}")
            
            self.criterion = FocalLoss(alpha=alpha, gamma=self.config.focal_gamma)
            
        elif self.config.loss_type == "bce":
            self.criterion = nn.BCEWithLogitsLoss()
            print("âš–ï¸ BCE Loss ì‚¬ìš©")
        else:
            if class_weights is not None and self.config.auto_class_weights:
                # CrossEntropyì—ë„ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
                weight_tensor = torch.tensor(class_weights, dtype=torch.float32)
                self.criterion = nn.CrossEntropyLoss(weight=weight_tensor)
                print(f"ğŸ“Š Cross Entropy Loss ì‚¬ìš©: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ {class_weights}")
            else:
                self.criterion = nn.CrossEntropyLoss()
                print("ğŸ“Š Cross Entropy Loss ì‚¬ìš©")
    
    def forward(self, batch, return_embeddings=False):
        """
        ìˆœì „íŒŒ - SigLIP2 Contrastive Learning í¬í•¨
        """
        # SigLIP2 ëª¨ë¸ í†µê³¼
        outputs = self.siglip(
            input_ids=batch.get('input_ids'),
            pixel_values=batch.get('pixel_values'),
            attention_mask=batch.get('attention_mask'),
            pixel_attention_mask=batch.get('pixel_attention_mask'),
            spatial_shapes=batch.get('spatial_shapes'),
            return_dict=True
        )
        
        # ê°œë³„ ì„ë² ë”© ì¶”ì¶œ (contrastive learningìš©)
        audio_embeds = outputs.image_embeds  # ë©œìŠ¤í™í† ê·¸ë¨ì„ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬
        text_embeds = outputs.text_embeds
        
        # ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„± (ë¶„ë¥˜ìš©)
        multimodal_embeddings = (audio_embeds + text_embeds) / 2
        
        # ì–¸ì–´ ì„ë² ë”© ì¶”ê°€ (ì„ íƒì )
        if 'languages' in batch:
            language_ids = torch.tensor([
                self.language_to_id.get(lang, 0) for lang in batch['languages']
            ], device=multimodal_embeddings.device)
            
            lang_embeds = self.language_embedding(language_ids)
            lang_embeds = self.language_projection(lang_embeds)
            
            # ì–¸ì–´ ì„ë² ë”©ê³¼ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ê²°í•©
            multimodal_embeddings = multimodal_embeddings + lang_embeds
        
        # ë¶„ë¥˜ê¸° ë™ì  ìƒì„± (ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œì—ë§Œ)
        if self.classifier is None:
            input_dim = multimodal_embeddings.size(-1)
            self.classifier = nn.Sequential(
                nn.Dropout(0.1),
                nn.Linear(input_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(512, 2)  # ì´ì§„ ë¶„ë¥˜
            ).to(multimodal_embeddings.device)
            print(f"ğŸ—ï¸ ë¶„ë¥˜ê¸° ìƒì„±: {input_dim} -> 512 -> 2")
        
        # ë¶„ë¥˜
        logits = self.classifier(multimodal_embeddings)
        
        if return_embeddings:
            return {
                'logits': logits,
                'audio_embeds': audio_embeds,
                'text_embeds': text_embeds,
                'multimodal_embeds': multimodal_embeddings
            }
        
        return logits
    
    def compute_loss(self, model_outputs, labels, patient_ids=None):
        """
        í†µí•© ì†ì‹¤ ê³„ì‚° - Classification + Contrastive Learning
        """
        # ëª¨ë¸ ì¶œë ¥ ì²˜ë¦¬
        if isinstance(model_outputs, dict):
            logits = model_outputs['logits']
            audio_embeds = model_outputs.get('audio_embeds')
            text_embeds = model_outputs.get('text_embeds')
        else:
            logits = model_outputs
            audio_embeds = None
            text_embeds = None
        
        # ë¶„ë¥˜ ì†ì‹¤ ê³„ì‚°
        if self.config.loss_type == "bce":
            # BCEëŠ” ì´ì§„ ë¶„ë¥˜ìš©ì´ë¯€ë¡œ ë¼ë²¨ì„ floatìœ¼ë¡œ ë³€í™˜í•˜ê³  ë¡œì§“ì˜ ë‘ ë²ˆì§¸ í´ë˜ìŠ¤ë§Œ ì‚¬ìš©
            labels_bce = labels.float()
            logits_bce = logits[:, 1]  # ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥ ë§Œ ì‚¬ìš©
            classification_loss = self.criterion(logits_bce, labels_bce)
        else:
            classification_loss = self.criterion(logits, labels)
        
        # Contrastive ì†ì‹¤ ê³„ì‚° (í™œì„±í™”ëœ ê²½ìš°)
        total_loss = classification_loss
        contrastive_metrics = {}
        
        if (self.use_contrastive and 
            audio_embeds is not None and 
            text_embeds is not None and 
            patient_ids is not None):
            
            contrastive_outputs = self.contrastive_loss(
                audio_embeds, text_embeds, patient_ids
            )
            
            contrastive_loss = contrastive_outputs['contrastive_loss']
            total_loss = (1 - self.contrastive_weight) * classification_loss + \
                        self.contrastive_weight * contrastive_loss
            
            # Contrastive ë©”íŠ¸ë¦­ ì €ì¥
            contrastive_metrics = {
                'contrastive_loss': contrastive_loss.item(),
                'avg_positive_similarity': contrastive_outputs['avg_positive_similarity'].item(),
                'avg_negative_similarity': contrastive_outputs['avg_negative_similarity'].item(),
                'alignment_score': contrastive_outputs['alignment_score'].item(),
                'positive_pairs_count': contrastive_outputs['positive_pairs_count'].item(),
                'negative_pairs_count': contrastive_outputs['negative_pairs_count'].item()
            }
        
        return {
            'total_loss': total_loss,
            'classification_loss': classification_loss,
            'contrastive_metrics': contrastive_metrics
        }
    
    def create_optimizer(self, config):
        """
        ì˜µí‹°ë§ˆì´ì € ìƒì„±
        """
        # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•˜ì§€ ì•Šì„ íŒŒë¼ë¯¸í„°ë“¤
        no_decay = ['bias', 'LayerNorm.weight']
        
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ìƒì„±
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': config.weight_decay,
            },
            {
                'params': [p for n, p in self.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0,
            },
        ]
        
        # ì˜µí‹°ë§ˆì´ì € ì„ íƒ
        if config.optimizer_type == "lion":
            if not LION_AVAILABLE:
                print("âš ï¸ lion-pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. AdamWë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                optimizer = torch.optim.AdamW(
                    optimizer_grouped_parameters,
                    lr=config.learning_rate,
                    weight_decay=config.weight_decay
                )
                print(f"âš¡ AdamW Optimizer ì‚¬ìš© (Lion ëŒ€ì²´): lr={config.learning_rate}")
            else:
                optimizer = Lion(
                    optimizer_grouped_parameters,
                    lr=config.learning_rate,
                    weight_decay=config.weight_decay
                )
                print(f"ğŸ¦ Lion Optimizer ì‚¬ìš© (lion-pytorch): lr={config.learning_rate}")
        elif config.optimizer_type == "sam":
            optimizer = SAM(
                self.parameters(),
                torch.optim.AdamW,
                lr=config.learning_rate,
                weight_decay=config.weight_decay,
                rho=config.sam_rho,
                adaptive=config.sam_adaptive
            )
            print(f"ğŸ¯ SAM Optimizer ì‚¬ìš©: lr={config.learning_rate}, rho={config.sam_rho}, adaptive={config.sam_adaptive}")
        else:
            optimizer = torch.optim.AdamW(
                optimizer_grouped_parameters,
                lr=config.learning_rate,
                weight_decay=config.weight_decay
            )
            print(f"âš¡ AdamW Optimizer ì‚¬ìš©: lr={config.learning_rate}")
        
        return optimizer
    
    def create_scheduler(self, optimizer, config, total_steps):
        """
        ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
        """
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config.num_epochs
        )
        
        return scheduler
