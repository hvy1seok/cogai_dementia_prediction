"""
SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ
ìˆœìˆ˜ PyTorch êµ¬í˜„ (SAM ì˜µí‹°ë§ˆì´ì € ì§€ì›)
"""

import os
import sys
import time
import argparse
import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
import wandb
import numpy as np
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoProcessor
from typing import Dict, List

from config import SigLIPSAMConfig
from model import SigLIPSAMDementiaClassifier
from data_processor import create_dataloaders
from sam_optimizer import SAM

def setup_wandb(config: SigLIPSAMConfig):
    """wandb ì„¤ì • - ì‹¤í—˜ ì„¤ì •ì´ í¬í•¨ëœ ìƒì„¸í•œ ì´ë¦„ ìƒì„±"""
    # ì‹¤í–‰ ì´ë¦„ ìƒì„± - ì„¤ì • ì •ë³´ í¬í•¨
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ì–¸ì–´ ì •ë³´
    if config.cross_lingual_mode:
        train_langs = "_".join(config.train_languages) if config.train_languages else "Unknown"
        test_langs = "_".join(config.test_languages) if config.test_languages else "Unknown"
        lang_info = f"CrossLingual_Train{train_langs}_Test{test_langs}"
    else:
        lang_info = "_".join(config.languages) if len(config.languages) <= 2 else f"{len(config.languages)}langs"
    
    # ëª¨ë¸ ë° ì„¤ì • ì •ë³´
    model_info = config.model_name.split("/")[-1] if "/" in config.model_name else config.model_name
    loss_info = config.loss_type
    opt_info = config.optimizer_type
    
    run_name = f"siglip-sam_{lang_info}_{model_info}_{loss_info}_{opt_info}_bs{config.batch_size}_lr{config.learning_rate}_{timestamp}"
    
    wandb.init(
        project="dementia-prediction-siglip-sam",
        name=run_name,
        tags=[
            f"loss_{config.loss_type}",
            f"optimizer_{config.optimizer_type}",
            f"batch_size_{config.batch_size}",
            f"languages_{len(config.languages)}",
            "cross_lingual" if config.cross_lingual_mode else "standard",
            "sam_optimizer" if config.optimizer_type == "sam" else "standard_optimizer"
        ],
        config={
            "model_name": config.model_name,
            "learning_rate": config.learning_rate,
            "batch_size": config.batch_size,
            "num_epochs": config.num_epochs,
            "languages": config.languages,
            "loss_type": config.loss_type,
            "optimizer_type": config.optimizer_type,
            "focal_alpha": config.focal_alpha,
            "focal_gamma": config.focal_gamma,
            "sam_rho": config.sam_rho,
            "sam_adaptive": config.sam_adaptive,
            "sample_rate": config.sample_rate,
            "n_mels": config.n_mels,
            "image_size": config.image_size,
            "max_length": config.max_length,
            "weight_decay": config.weight_decay,
            "warmup_steps": config.warmup_steps,
            "mixed_precision": config.mixed_precision,
            "gradient_clip_norm": config.gradient_clip_norm,
            # Cross-lingual ì„¤ì •
            "cross_lingual_mode": config.cross_lingual_mode,
            "train_languages": config.train_languages,
            "test_languages": config.test_languages,
        }
    )

def compute_metrics(predictions, labels, languages=None):
    """ë©”íŠ¸ë¦­ ê³„ì‚° - ìµœì  threshold ê¸°ë°˜ + ì–¸ì–´ë³„ ë¶„ì„"""
    predictions = np.array(predictions)
    labels = np.array(labels)
    
    if predictions.shape[1] == 2:
        # ì´ì§„ ë¶„ë¥˜: ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥  ì‚¬ìš©
        probs = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()
        
        # ROC AUC ê³„ì‚°
        try:
            auc = roc_auc_score(labels, probs)
        except:
            auc = 0.0
        
        # ìµœì  threshold ì°¾ê¸° (Youden's J statistic)
        try:
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(labels, probs)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
        except:
            optimal_threshold = 0.5
        
        # ìµœì  thresholdë¡œ ì˜ˆì¸¡
        optimal_preds = (probs >= optimal_threshold).astype(int)
        
        # ê¸°ë³¸ threshold (0.5)ë¡œë„ ì˜ˆì¸¡
        default_preds = (probs >= 0.5).astype(int)
        
        # argmax ì˜ˆì¸¡ (ê¸°ì¡´ ë°©ì‹)
        argmax_preds = np.argmax(predictions, axis=1)
        
        # ìµœì  threshold ê¸°ë°˜ ë©”íŠ¸ë¦­ (ë©”ì¸)
        optimal_accuracy = accuracy_score(labels, optimal_preds)
        optimal_precision, optimal_recall, optimal_f1, _ = precision_recall_fscore_support(
            labels, optimal_preds, average='weighted', zero_division=0
        )
        
        # ë¹„êµìš© ë©”íŠ¸ë¦­ë“¤
        default_accuracy = accuracy_score(labels, default_preds)
        default_precision, default_recall, default_f1, _ = precision_recall_fscore_support(
            labels, default_preds, average='weighted', zero_division=0
        )
        
        argmax_accuracy = accuracy_score(labels, argmax_preds)
        argmax_precision, argmax_recall, argmax_f1, _ = precision_recall_fscore_support(
            labels, argmax_preds, average='weighted', zero_division=0
        )
        
        metrics = {
            # ë©”ì¸ ì§€í‘œ (ìµœì  threshold ê¸°ë°˜)
            'accuracy': optimal_accuracy,
            'precision': optimal_precision,
            'recall': optimal_recall,
            'f1': optimal_f1,
            'auc': auc,
            'optimal_threshold': optimal_threshold,
            
            # ë¹„êµ ì§€í‘œë“¤
            'accuracy_default': default_accuracy,
            'precision_default': default_precision,
            'recall_default': default_recall,
            'f1_default': default_f1,
            
            'accuracy_argmax': argmax_accuracy,
            'precision_argmax': argmax_precision,
            'recall_argmax': argmax_recall,
            'f1_argmax': argmax_f1,
        }
        
        # ì–¸ì–´ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        if languages is not None:
            language_metrics = compute_language_specific_metrics(probs, labels, languages, optimal_threshold)
            metrics.update(language_metrics)
        
        return metrics
    else:
        # ë‹¤ì¤‘ ë¶„ë¥˜
        preds = np.argmax(predictions, axis=1)
        accuracy = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': 0.0,
            'optimal_threshold': 0.5
        }

def compute_language_specific_metrics(y_scores, y_true, all_languages, optimal_threshold):
    """ì–¸ì–´ë³„ í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì¶œë ¥"""
    from collections import defaultdict, Counter
    import numpy as np
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
    
    # ì–¸ì–´ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
    language_data = defaultdict(lambda: {'scores': [], 'labels': [], 'indices': []})
    
    for i, (score, label, lang) in enumerate(zip(y_scores, y_true, all_languages)):
        language_data[lang]['scores'].append(score)
        language_data[lang]['labels'].append(label)
        language_data[lang]['indices'].append(i)
    
    print(f"\nğŸŒ ì–¸ì–´ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"{'='*80}")
    
    # wandb ë¡œê¹…ìš© ì–¸ì–´ë³„ ë©”íŠ¸ë¦­
    language_metrics = {}
    
    for lang in sorted(language_data.keys()):
        lang_scores = np.array(language_data[lang]['scores'])
        lang_labels = np.array(language_data[lang]['labels'])
        
        if len(lang_scores) == 0:
            continue
            
        # ì–¸ì–´ë³„ AUC ê³„ì‚°
        try:
            lang_auc = roc_auc_score(lang_labels, lang_scores)
        except ValueError:
            lang_auc = 0.0
        
        # ìµœì  thresholdë¡œ ì˜ˆì¸¡
        lang_optimal_preds = (lang_scores >= optimal_threshold).astype(int)
        
        # ê¸°ë³¸ threshold (0.5)ë¡œ ì˜ˆì¸¡
        lang_default_preds = (lang_scores >= 0.5).astype(int)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        lang_optimal_acc = accuracy_score(lang_labels, lang_optimal_preds)
        lang_default_acc = accuracy_score(lang_labels, lang_default_preds)
        
        lang_precision, lang_recall, lang_f1, _ = precision_recall_fscore_support(
            lang_labels, lang_optimal_preds, average='weighted', zero_division=0
        )
        
        # í´ë˜ìŠ¤ë³„ ë¶„í¬
        label_dist = Counter(lang_labels)
        normal_count = label_dist[0]
        dementia_count = label_dist[1]
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š {lang} ({len(lang_scores)}ê°œ ìƒ˜í”Œ)")
        print(f"   ì •ìƒ: {normal_count}ê°œ, ì¹˜ë§¤: {dementia_count}ê°œ")
        print(f"   AUC: {lang_auc:.4f}")
        print(f"   Accuracy (ìµœì ): {lang_optimal_acc:.4f}")
        print(f"   Accuracy (0.5): {lang_default_acc:.4f}")
        print(f"   Precision: {lang_precision:.4f}")
        print(f"   Recall: {lang_recall:.4f}")
        print(f"   F1: {lang_f1:.4f}")
        
        # wandb ë¡œê¹…ìš© ë©”íŠ¸ë¦­ ì €ì¥
        language_metrics[f'{lang}_auc'] = lang_auc
        language_metrics[f'{lang}_accuracy_optimal'] = lang_optimal_acc
        language_metrics[f'{lang}_accuracy_default'] = lang_default_acc
        language_metrics[f'{lang}_precision'] = lang_precision
        language_metrics[f'{lang}_recall'] = lang_recall
        language_metrics[f'{lang}_f1'] = lang_f1
        language_metrics[f'{lang}_sample_count'] = len(lang_scores)
        language_metrics[f'{lang}_normal_count'] = normal_count
        language_metrics[f'{lang}_dementia_count'] = dementia_count
    
    print(f"{'='*80}")
    
    return language_metrics

def plot_roc_curve(predictions, labels, title="ROC Curve", save_path=None):
    """ROC ê³¡ì„ ì„ ê·¸ë¦¬ê³  wandbì— ë¡œê¹…"""
    plt.style.use('default')
    fig, ax = plt.subplots(figsize=(8, 6))
    
    try:
        # í™•ë¥ ê°’ ì¶”ì¶œ (ì´ì§„ ë¶„ë¥˜ì˜ positive class í™•ë¥ )
        if len(predictions.shape) > 1:
            probs = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()
        else:
            probs = predictions
        
        # ROC ê³¡ì„  ê³„ì‚°
        if len(np.unique(labels)) > 1:
            fpr, tpr, thresholds = roc_curve(labels, probs)
            auc_score = roc_auc_score(labels, probs)
            
            # ROC ê³¡ì„  ê·¸ë¦¬ê¸°
            ax.plot(fpr, tpr, color='darkorange', lw=2, 
                    label=f'ROC curve (AUC = {auc_score:.3f})')
            ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                    label='Random classifier')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate', fontsize=12)
            ax.set_ylabel('True Positive Rate', fontsize=12)
            ax.set_title(f'{title} (AUC = {auc_score:.3f})', fontsize=14, fontweight='bold')
            ax.legend(loc="lower right", fontsize=10)
            ax.grid(True, alpha=0.3)
            
            # ìµœì  ì„ê³„ê°’ í‘œì‹œ
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
            ax.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=8, 
                    label=f'Optimal threshold = {optimal_threshold:.3f}')
            ax.legend(loc="lower right", fontsize=10)
            
            print(f"ğŸ“Š ROC ê³¡ì„  ìƒì„± ì™„ë£Œ: AUC = {auc_score:.3f}")
            
        else:
            ax.text(0.5, 0.5, 'Cannot plot ROC curve\n(only one class present)', 
                    ha='center', va='center', fontsize=14)
            ax.set_xlim([0, 1])
            ax.set_ylim([0, 1])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(title)
            print("âš ï¸ ROC ê³¡ì„  ìƒì„± ë¶ˆê°€: ë‹¨ì¼ í´ë˜ìŠ¤ë§Œ ì¡´ì¬")
        
        plt.tight_layout()
        
        # ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ROC ê³¡ì„  ì €ì¥: {save_path}")
        
        # wandbì— ë¡œê¹…
        if wandb.run is not None:
            wandb.log({f"{title.lower().replace(' ', '_')}_plot": wandb.Image(fig)})
            print(f"ğŸ“Š ROC ê³¡ì„  wandb ì—…ë¡œë“œ: {title}")
        
        plt.close(fig)
        return fig
        
    except Exception as e:
        print(f"âŒ ROC ê³¡ì„  ìƒì„± ì˜¤ë¥˜: {e}")
        plt.close(fig)
        return None

def plot_confusion_matrix(predictions, labels, title="Confusion Matrix", save_path=None):
    """Confusion Matrixë¥¼ ê·¸ë¦¬ê³  wandbì— ë¡œê¹…"""
    try:
        # ì˜ˆì¸¡ê°’ ë³€í™˜
        if len(predictions.shape) > 1:
            preds = np.argmax(predictions, axis=1)
        else:
            preds = (predictions > 0.5).astype(int)
        
        # Confusion Matrix ê³„ì‚°
        cm = confusion_matrix(labels, preds)
        
        # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Normal', 'Dementia'], 
                   yticklabels=['Normal', 'Dementia'])
        plt.title(f'{title}', fontsize=14, fontweight='bold')
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.tight_layout()
        
        # ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Confusion Matrix ì €ì¥: {save_path}")
        
        # wandbì— ë¡œê¹…
        if wandb.run is not None:
            wandb.log({f"{title.lower().replace(' ', '_')}_plot": wandb.Image(plt.gcf())})
            print(f"ğŸ“Š Confusion Matrix wandb ì—…ë¡œë“œ: {title}")
        
        plt.close()
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥
        accuracy = np.trace(cm) / np.sum(cm)
        print(f"ğŸ“Š {title} ìš”ì•½:")
        print(f"   ì •í™•ë„: {accuracy:.4f}")
        print(f"   ì •ìƒ â†’ ì •ìƒ: {cm[0,0]}, ì •ìƒ â†’ ì¹˜ë§¤: {cm[0,1]}")
        print(f"   ì¹˜ë§¤ â†’ ì •ìƒ: {cm[1,0]}, ì¹˜ë§¤ â†’ ì¹˜ë§¤: {cm[1,1]}")
        
    except Exception as e:
        print(f"âŒ Confusion Matrix ìƒì„± ì˜¤ë¥˜: {e}")
        plt.close()

def train_epoch(model, train_loader, optimizer, config, scaler=None, use_mixed_precision=False):
    """í•œ ì—í¬í¬ í›ˆë ¨ - SigLIP2 Contrastive Learning í¬í•¨"""
    model.train()
    total_loss = 0.0
    total_classification_loss = 0.0
    total_contrastive_loss = 0.0
    all_predictions = []
    all_labels = []
    contrastive_metrics_sum = {}
    
    for batch_idx, batch in enumerate(train_loader):
        # GPUë¡œ ì´ë™
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(config.device)
        
        # í™˜ì ID ì¶”ì¶œ (contrastive learningìš©)
        if 'patient_id' in batch:
            patient_ids = batch['patient_id'] if isinstance(batch['patient_id'], list) else [batch['patient_id']]
        else:
            # Fallback: ì„ì‹œ patient_id ìƒì„±
            if 'language' in batch:
                languages = batch['language'] if isinstance(batch['language'], list) else ['Unknown'] * len(batch['labels'])
                patient_ids = [f"{lang}_{i // 2}" for i, lang in enumerate(languages)]
            else:
                patient_ids = [f"patient_{i // 2}" for i in range(len(batch['labels']))]
        
        # SAM ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ (mixed precision ë¹„í™œì„±í™”ë¡œ ì•ˆì •ì„± í™•ë³´)
        if config.optimizer_type == "sam":
            # SAMì€ mixed precision ì—†ì´ ì‚¬ìš© (ì•ˆì •ì„±ì„ ìœ„í•´)
            # ì²« ë²ˆì§¸ forward pass
            model_outputs = model(batch, return_embeddings=model.use_contrastive)
            loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids)
            loss = loss_dict['total_loss']
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
            optimizer.first_step(zero_grad=True)
            
            # ë‘ ë²ˆì§¸ forward pass
            model_outputs = model(batch, return_embeddings=model.use_contrastive)
            loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids)
            loss = loss_dict['total_loss']
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
            optimizer.second_step(zero_grad=True)
        
        else:
            # ì¼ë°˜ ì˜µí‹°ë§ˆì´ì €
            optimizer.zero_grad()
            
            if scaler and use_mixed_precision:
                with autocast('cuda'):
                    model_outputs = model(batch, return_embeddings=model.use_contrastive)
                    loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids)
                    loss = loss_dict['total_loss']
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                model_outputs = model(batch, return_embeddings=model.use_contrastive)
                loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids)
                loss = loss_dict['total_loss']
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                optimizer.step()
        
        # ë¡œì§“ ì¶”ì¶œ (ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)
        if isinstance(model_outputs, dict):
            logits = model_outputs['logits']
        else:
            logits = model_outputs
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        total_loss += loss.item()
        total_classification_loss += loss_dict['classification_loss'].item()
        
        # Contrastive ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        if loss_dict['contrastive_metrics']:
            if 'contrastive_loss' in loss_dict['contrastive_metrics']:
                total_contrastive_loss += loss_dict['contrastive_metrics']['contrastive_loss']
            
            # í‰ê· ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ëˆ„ì 
            for key, value in loss_dict['contrastive_metrics'].items():
                if key not in contrastive_metrics_sum:
                    contrastive_metrics_sum[key] = 0.0
                contrastive_metrics_sum[key] += value
        
        all_predictions.extend(logits.detach().cpu().numpy())
        all_labels.extend(batch['labels'].cpu().numpy())
        
        # ë¡œê¹…
        if batch_idx % config.log_interval == 0:
            log_msg = f'Train Batch {batch_idx}/{len(train_loader)}: Total Loss = {loss.item():.4f}'
            if loss_dict['contrastive_metrics']:
                log_msg += f', Cls Loss = {loss_dict["classification_loss"].item():.4f}'
                if 'contrastive_loss' in loss_dict['contrastive_metrics']:
                    log_msg += f', Cont Loss = {loss_dict["contrastive_metrics"]["contrastive_loss"]:.4f}'
                if 'alignment_score' in loss_dict['contrastive_metrics']:
                    log_msg += f', Align = {loss_dict["contrastive_metrics"]["alignment_score"]:.3f}'
            print(log_msg)
            
            # wandb ë¡œê¹…
            wandb_log = {
                'train_batch_total_loss': loss.item(),
                'train_batch_classification_loss': loss_dict['classification_loss'].item(),
                'train_step': batch_idx
            }
            
            # Contrastive ë©”íŠ¸ë¦­ ì¶”ê°€
            for key, value in loss_dict['contrastive_metrics'].items():
                wandb_log[f'train_batch_{key}'] = value
                
            wandb.log(wandb_log)
    
    # ì—í¬í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
    num_batches = len(train_loader)
    avg_total_loss = total_loss / num_batches
    avg_classification_loss = total_classification_loss / num_batches
    avg_contrastive_loss = total_contrastive_loss / num_batches if total_contrastive_loss > 0 else 0.0
    
    # Contrastive ë©”íŠ¸ë¦­ í‰ê·  ê³„ì‚°
    avg_contrastive_metrics = {}
    for key, value in contrastive_metrics_sum.items():
        avg_contrastive_metrics[key] = value / num_batches
    
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))
    
    # ì¶”ê°€ ë©”íŠ¸ë¦­ ì •ë³´
    metrics['classification_loss'] = avg_classification_loss
    metrics['contrastive_loss'] = avg_contrastive_loss
    metrics.update(avg_contrastive_metrics)
    
    return avg_total_loss, metrics

def evaluate(model, test_loader, config, use_mixed_precision=False, title_prefix="Test"):
    """ëª¨ë¸ í‰ê°€ - ì–¸ì–´ë³„ ë¶„ì„ ë° Contrastive Learning í¬í•¨"""
    model.eval()
    total_loss = 0.0
    total_classification_loss = 0.0
    total_contrastive_loss = 0.0
    all_predictions = []
    all_labels = []
    all_languages = []
    contrastive_metrics_sum = {}
    
    with torch.no_grad():
        for batch in test_loader:
            # GPUë¡œ ì´ë™
            for key in batch:
                if isinstance(batch[key], torch.Tensor):
                    batch[key] = batch[key].to(config.device)
            
            # í™˜ì ID ì¶”ì¶œ (contrastive learningìš©)
            if 'patient_id' in batch:
                patient_ids = batch['patient_id'] if isinstance(batch['patient_id'], list) else [batch['patient_id']]
            else:
                # Fallback: ì„ì‹œ patient_id ìƒì„±
                if 'language' in batch:
                    languages = batch['language'] if isinstance(batch['language'], list) else ['Unknown'] * len(batch['labels'])
                    patient_ids = [f"{lang}_{i // 2}" for i, lang in enumerate(languages)]
                else:
                    patient_ids = [f"patient_{i // 2}" for i in range(len(batch['labels']))]
            
            if use_mixed_precision:
                with autocast('cuda'):
                    model_outputs = model(batch, return_embeddings=model.use_contrastive)
                    loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids)
                    loss = loss_dict['total_loss']
            else:
                model_outputs = model(batch, return_embeddings=model.use_contrastive)
                loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids)
                loss = loss_dict['total_loss']
            
            # ë¡œì§“ ì¶”ì¶œ
            if isinstance(model_outputs, dict):
                logits = model_outputs['logits']
            else:
                logits = model_outputs
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            total_classification_loss += loss_dict['classification_loss'].item()
            
            # Contrastive ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            if loss_dict['contrastive_metrics']:
                if 'contrastive_loss' in loss_dict['contrastive_metrics']:
                    total_contrastive_loss += loss_dict['contrastive_metrics']['contrastive_loss']
                
                # í‰ê· ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ëˆ„ì 
                for key, value in loss_dict['contrastive_metrics'].items():
                    if key not in contrastive_metrics_sum:
                        contrastive_metrics_sum[key] = 0.0
                    contrastive_metrics_sum[key] += value
            
            all_predictions.extend(logits.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())
            
            # ì–¸ì–´ ì •ë³´ ìˆ˜ì§‘
            if 'language' in batch:
                if isinstance(batch['language'], list):
                    all_languages.extend(batch['language'])
                else:
                    # í…ì„œì¸ ê²½ìš° ì²˜ë¦¬
                    all_languages.extend(['Unknown'] * len(batch['labels']))
            else:
                all_languages.extend(['Unknown'] * len(batch['labels']))
    
    # ë©”íŠ¸ë¦­ ê³„ì‚° (ì–¸ì–´ë³„ ë¶„ì„ í¬í•¨)
    num_batches = len(test_loader)
    avg_total_loss = total_loss / num_batches
    avg_classification_loss = total_classification_loss / num_batches
    avg_contrastive_loss = total_contrastive_loss / num_batches if total_contrastive_loss > 0 else 0.0
    
    # Contrastive ë©”íŠ¸ë¦­ í‰ê·  ê³„ì‚°
    avg_contrastive_metrics = {}
    for key, value in contrastive_metrics_sum.items():
        avg_contrastive_metrics[key] = value / num_batches
    
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels), all_languages)
    
    # ì¶”ê°€ ë©”íŠ¸ë¦­ ì •ë³´
    metrics['classification_loss'] = avg_classification_loss
    metrics['contrastive_loss'] = avg_contrastive_loss
    metrics.update(avg_contrastive_metrics)
    
    # ROC ê³¡ì„  ê·¸ë¦¬ê¸° ë° wandb ì—…ë¡œë“œ
    try:
        plot_roc_curve(
            predictions=np.array(all_predictions), 
            labels=np.array(all_labels), 
            title=f"{title_prefix} ROC Curve",
            save_path=os.path.join(config.output_dir, f"{title_prefix.lower()}_roc_curve.png")
        )
    except Exception as e:
        print(f"âš ï¸ ROC ê³¡ì„  ìƒì„± ì‹¤íŒ¨: {e}")
    
    # Confusion Matrix ê·¸ë¦¬ê¸° ë° wandb ì—…ë¡œë“œ
    try:
        plot_confusion_matrix(
            predictions=np.array(all_predictions), 
            labels=np.array(all_labels), 
            title=f"{title_prefix} Confusion Matrix",
            save_path=os.path.join(config.output_dir, f"{title_prefix.lower()}_confusion_matrix.png")
        )
    except Exception as e:
        print(f"âš ï¸ Confusion Matrix ìƒì„± ì‹¤íŒ¨: {e}")
    
    return avg_total_loss, metrics

def save_checkpoint(model, optimizer, epoch, metrics, config, is_best=False):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config
    }
    
    filename = f"checkpoint_epoch_{epoch:03d}_auc_{metrics['auc']:.3f}.pt"
    if is_best:
        filename = f"best_model_auc_{metrics['auc']:.3f}_epoch_{epoch:03d}.pt"
    
    filepath = os.path.join(config.checkpoint_dir, filename)
    torch.save(checkpoint, filepath)
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
    
    return filepath

def train_model(config: SigLIPSAMConfig):
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("=== SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.cuda.is_available():
        config.device = "cuda"
        print(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        config.device = "cpu"
        print("CPU ì‚¬ìš©")
    
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(config.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.random_seed)
    
    # SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(config.model_name)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader, test_loader = create_dataloaders(
        data_dir=config.data_dir,
        processor=processor,
        config=config,
        cross_lingual_mode=config.cross_lingual_mode,
        train_languages=config.train_languages,
        test_languages=config.test_languages
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_loader.dataset)} ìƒ˜í”Œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_loader.dataset)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_loader.dataset)} ìƒ˜í”Œ")
    
    # ëª¨ë¸ ìƒì„±
    print("ëª¨ë¸ ìƒì„± ì¤‘...")
    model = SigLIPSAMDementiaClassifier(config)
    model.to(config.device)
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    from data_processor import compute_class_weights
    
    # í›ˆë ¨ ë°ì´í„°ì…‹ì—ì„œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    if hasattr(train_loader.dataset, 'dataset'):
        # Subsetì¸ ê²½ìš° ì›ë³¸ ë°ì´í„°ì…‹ ì ‘ê·¼
        original_dataset = train_loader.dataset.dataset
    else:
        original_dataset = train_loader.dataset
    
    class_weights = compute_class_weights(original_dataset, config)
    model.setup_loss_function(class_weights)
    
    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer = model.create_optimizer(config)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    total_steps = len(train_loader) * config.num_epochs
    scheduler = model.create_scheduler(optimizer, config, total_steps)
    
    # Mixed precision ìŠ¤ì¼€ì¼ëŸ¬ (SAM ì‚¬ìš© ì‹œ ë¹„í™œì„±í™”)
    use_mixed_precision = config.mixed_precision and config.optimizer_type != "sam"
    scaler = GradScaler('cuda') if use_mixed_precision else None
    
    if config.optimizer_type == "sam" and config.mixed_precision:
        print("âš ï¸ SAM ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ Mixed Precisionì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤ (ì•ˆì •ì„±ì„ ìœ„í•´)")
        use_mixed_precision = False
    
    # wandb ì„¤ì •
    setup_wandb(config)
    
    # í›ˆë ¨ ë£¨í”„
    best_val_auc = 0.0
    best_model_path = None
    early_stopping_patience = getattr(config, 'early_stopping_patience', 15)
    epochs_without_improvement = 0
    
    for epoch in range(config.num_epochs):
        print(f"\n=== Epoch {epoch+1}/{config.num_epochs} ===")
        
        # í›ˆë ¨
        train_loss, train_metrics = train_epoch(model, train_loader, optimizer, config, scaler, use_mixed_precision)
        
        # ê²€ì¦
        val_loss, val_metrics = evaluate(model, val_loader, config, use_mixed_precision, title_prefix="Val")
        
        # í…ŒìŠ¤íŠ¸ (ë§¤ ì—í¬í¬ë§ˆë‹¤ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ)
        test_loss, test_metrics = evaluate(model, test_loader, config, use_mixed_precision, title_prefix="Test")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ë¡œê¹… (ìµœì  threshold ê¸°ë°˜ ë©”íŠ¸ë¦­ í¬í•¨)
        wandb_log = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_accuracy': train_metrics['accuracy'],
            'train_precision': train_metrics['precision'],
            'train_recall': train_metrics['recall'],
            'train_f1': train_metrics['f1'],
            'train_auc': train_metrics['auc'],
            'val_loss': val_loss,
            'val_accuracy': val_metrics['accuracy'],
            'val_precision': val_metrics['precision'],
            'val_recall': val_metrics['recall'],
            'val_f1': val_metrics['f1'],
            'val_auc': val_metrics['auc'],
            'test_loss': test_loss,
            'test_accuracy': test_metrics['accuracy'],
            'test_precision': test_metrics['precision'],
            'test_recall': test_metrics['recall'],
            'test_f1': test_metrics['f1'],
            'test_auc': test_metrics['auc'],
            'learning_rate': optimizer.param_groups[0]['lr']
        }
        
        # Contrastive Learning ë©”íŠ¸ë¦­ ì¶”ê°€
        for prefix, metrics_dict in [('train', train_metrics), ('val', val_metrics), ('test', test_metrics)]:
            # Classification vs Contrastive loss ë¶„ë¦¬
            if 'classification_loss' in metrics_dict:
                wandb_log[f'{prefix}_classification_loss'] = metrics_dict['classification_loss']
            if 'contrastive_loss' in metrics_dict:
                wandb_log[f'{prefix}_contrastive_loss'] = metrics_dict['contrastive_loss']
            
            # Cross-modal alignment ë©”íŠ¸ë¦­
            if 'alignment_score' in metrics_dict:
                wandb_log[f'{prefix}_alignment_score'] = metrics_dict['alignment_score']
            if 'avg_positive_similarity' in metrics_dict:
                wandb_log[f'{prefix}_positive_similarity'] = metrics_dict['avg_positive_similarity']
            if 'avg_negative_similarity' in metrics_dict:
                wandb_log[f'{prefix}_negative_similarity'] = metrics_dict['avg_negative_similarity']
        
        # ìµœì  threshold ì •ë³´ ì¶”ê°€
        if 'optimal_threshold' in val_metrics:
            wandb_log['val_optimal_threshold'] = val_metrics['optimal_threshold']
        if 'optimal_threshold' in test_metrics:
            wandb_log['test_optimal_threshold'] = test_metrics['optimal_threshold']
        
        # ë¹„êµ ë©”íŠ¸ë¦­ë„ ì¶”ê°€
        if 'accuracy_default' in val_metrics:
            wandb_log['val_accuracy_default_0.5'] = val_metrics['accuracy_default']
            wandb_log['val_accuracy_argmax'] = val_metrics['accuracy_argmax']
        if 'accuracy_default' in test_metrics:
            wandb_log['test_accuracy_default_0.5'] = test_metrics['accuracy_default']
            wandb_log['test_accuracy_argmax'] = test_metrics['accuracy_argmax']
        
        # ì–¸ì–´ë³„ ë©”íŠ¸ë¦­ ì¶”ê°€ (í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ì—ì„œë§Œ)
        for key, value in test_metrics.items():
            if any(lang in key for lang in ['English', 'Greek', 'Spanish', 'Mandarin']):
                wandb_log[f'test_{key}'] = value
        
        wandb.log(wandb_log)
        
        # ê²°ê³¼ ì¶œë ¥ (ìµœì  threshold ê¸°ë°˜)
        print(f"í›ˆë ¨ - Loss: {train_loss:.4f}, Acc: {train_metrics['accuracy']:.4f}, Prec: {train_metrics['precision']:.4f}, Rec: {train_metrics['recall']:.4f}, F1: {train_metrics['f1']:.4f}, AUC: {train_metrics['auc']:.4f}")
        print(f"ê²€ì¦ - Loss: {val_loss:.4f}, Acc: {val_metrics['accuracy']:.4f}, Prec: {val_metrics['precision']:.4f}, Rec: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ - Loss: {test_loss:.4f}, Acc: {test_metrics['accuracy']:.4f}, Prec: {test_metrics['precision']:.4f}, Rec: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}, AUC: {test_metrics['auc']:.4f}")
        
        # Threshold ì •ë³´ ì¶œë ¥
        if 'optimal_threshold' in val_metrics:
            print(f"ğŸ¯ ê²€ì¦ ìµœì  threshold: {val_metrics['optimal_threshold']:.3f}")
        if 'optimal_threshold' in test_metrics:
            print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ìµœì  threshold: {test_metrics['optimal_threshold']:.3f}")
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ ë° Early Stopping (validation AUC ê¸°ì¤€)
        if val_metrics['auc'] > best_val_auc:
            best_val_auc = val_metrics['auc']
            best_model_path = save_checkpoint(model, optimizer, epoch + 1, val_metrics, config, is_best=True)
            epochs_without_improvement = 0
            print(f"ğŸ† ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ ëª¨ë¸! Validation AUC: {best_val_auc:.4f}")
        else:
            epochs_without_improvement += 1
            print(f"â³ ê°œì„  ì—†ìŒ: {epochs_without_improvement}/{early_stopping_patience} epochs")
        
        # Early Stopping ì²´í¬
        if epochs_without_improvement >= early_stopping_patience:
            print(f"\nğŸ›‘ Early Stopping! {early_stopping_patience} epochs ë™ì•ˆ validation AUC ê°œì„  ì—†ìŒ")
            print(f"ğŸ† ìµœì¢… ë² ìŠ¤íŠ¸ Validation AUC: {best_val_auc:.4f}")
            break
        
        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % config.save_interval == 0:
            save_checkpoint(model, optimizer, epoch + 1, val_metrics, config, is_best=False)
    
    print(f"\n=== í›ˆë ¨ ì™„ë£Œ ===")
    print(f"ğŸ† ë² ìŠ¤íŠ¸ Validation AUC: {best_val_auc:.4f}")
    print(f"ğŸ’¾ ë² ìŠ¤íŠ¸ ëª¨ë¸: {best_model_path}")
    
    # ìµœì¢… í…ŒìŠ¤íŠ¸ (ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œí•´ì„œ ìµœì¢… í‰ê°€)
    if best_model_path and os.path.exists(best_model_path):
        print("\nğŸ” ë² ìŠ¤íŠ¸ ëª¨ë¸ë¡œ ìµœì¢… í‰ê°€ ìˆ˜í–‰...")
        try:
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(best_model_path, map_location=config.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # ìµœì¢… í…ŒìŠ¤íŠ¸
            final_test_loss, final_test_metrics = evaluate(model, test_loader, config, use_mixed_precision)
            
            # ìµœì¢… ê²°ê³¼ wandb ë¡œê¹…
            final_wandb_log = {
                'final_test_loss': final_test_loss,
                'final_test_accuracy': final_test_metrics['accuracy'],
                'final_test_f1': final_test_metrics['f1'],
                'final_test_auc': final_test_metrics['auc'],
            }
            
            # ì–¸ì–´ë³„ ìµœì¢… ë©”íŠ¸ë¦­ ì¶”ê°€
            for key, value in final_test_metrics.items():
                if any(lang in key for lang in ['English', 'Greek', 'Spanish', 'Mandarin']):
                    final_wandb_log[f'final_{key}'] = value
            
            wandb.log(final_wandb_log)
            
            print(f"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ë² ìŠ¤íŠ¸ ëª¨ë¸):")
            print(f"   Loss: {final_test_loss:.4f}")
            print(f"   AUC: {final_test_metrics['auc']:.4f}")
            print(f"   Accuracy: {final_test_metrics['accuracy']:.4f}")
            print(f"   Precision: {final_test_metrics['precision']:.4f}")
            print(f"   Recall: {final_test_metrics['recall']:.4f}")
            print(f"   F1: {final_test_metrics['f1']:.4f}")
            
            if 'optimal_threshold' in final_test_metrics:
                print(f"   ìµœì  Threshold: {final_test_metrics['optimal_threshold']:.3f}")
                
                # Threshold ë¹„êµ ì¶œë ¥
                if 'accuracy_default' in final_test_metrics:
                    print(f"\nğŸ“Š Threshold ë¹„êµ:")
                    print(f"   ìµœì  threshold ({final_test_metrics['optimal_threshold']:.3f}): Acc={final_test_metrics['accuracy']:.4f}")
                    print(f"   ê¸°ë³¸ threshold (0.500): Acc={final_test_metrics['accuracy_default']:.4f}")
                    print(f"   Argmax ë°©ì‹: Acc={final_test_metrics['accuracy_argmax']:.4f}")
            
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… í‰ê°€ ì‹¤íŒ¨: {e}")
    
    # wandb ì¢…ë£Œ
    wandb.finish()
    
    return model, best_model_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨")
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--data_dir", type=str, default="../../training_dset", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str, default="../modules/outputs/siglip-sam", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="google/siglip2-base-patch16-naflex", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=32, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=100, help="ì—í¬í¬ ìˆ˜")
    
    # ì–¸ì–´ë³„ íŒŒì„œ ì„ íƒ ì˜µì…˜
    parser.add_argument("--parser", type=str, default="all", 
                       choices=["all", "English", "Greek", "Spanish", "Mandarin", "cross_lingual"],
                       help="ì‚¬ìš©í•  ì–¸ì–´ íŒŒì„œ")
    parser.add_argument("--languages", nargs="+", default=None, help="íŠ¹ì • ì–¸ì–´ ëª©ë¡")
    
    # Cross-lingual ëª¨ë“œ ì˜µì…˜
    parser.add_argument("--train_languages", nargs="+", default=["English", "Spanish", "Mandarin"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í›ˆë ¨ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    parser.add_argument("--test_languages", nargs="+", default=["Greek"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ì˜µì…˜
    parser.add_argument("--loss_type", type=str, default="cross_entropy",
                       choices=["cross_entropy", "focal", "bce"],
                       help="ì†ì‹¤ í•¨ìˆ˜ íƒ€ì…")
    parser.add_argument("--focal_alpha", type=float, default=1.0, help="Focal Loss alpha íŒŒë¼ë¯¸í„°")
    parser.add_argument("--focal_gamma", type=float, default=2.0, help="Focal Loss gamma íŒŒë¼ë¯¸í„°")
    parser.add_argument("--auto_class_weights", action="store_true", help="í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ë³´ì •")
    
    # ì˜µí‹°ë§ˆì´ì € ì„ íƒ ì˜µì…˜
    parser.add_argument("--optimizer_type", type=str, default="sam",
                       choices=["adamw", "lion", "sam"],
                       help="ì˜µí‹°ë§ˆì´ì € íƒ€ì…")
    parser.add_argument("--sam_rho", type=float, default=0.05, help="SAM rho íŒŒë¼ë¯¸í„°")
    parser.add_argument("--sam_adaptive", action="store_true", help="Adaptive SAM ì‚¬ìš©")
    
    # SigLIP2 Contrastive Learning ì˜µì…˜
    parser.add_argument("--use_contrastive", action="store_true", default=True, help="Contrastive Learning ì‚¬ìš©")
    parser.add_argument("--no_contrastive", action="store_true", help="Contrastive Learning ë¹„í™œì„±í™”")
    parser.add_argument("--contrastive_weight", type=float, default=0.5, help="Contrastive vs Classification ì†ì‹¤ ê°€ì¤‘ì¹˜")
    parser.add_argument("--contrastive_temperature", type=float, default=0.07, help="Contrastive Learning ì˜¨ë„ íŒŒë¼ë¯¸í„°")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SigLIPSAMConfig()
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config.data_dir = args.data_dir
    if args.output_dir:
        config.output_dir = args.output_dir
        config.checkpoint_dir = f"{args.output_dir}/checkpoints"
    if args.model_name:
        config.model_name = args.model_name
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.num_epochs:
        config.num_epochs = args.num_epochs
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    if args.loss_type:
        config.loss_type = args.loss_type
    if args.focal_alpha:
        config.focal_alpha = args.focal_alpha
    if args.focal_gamma:
        config.focal_gamma = args.focal_gamma
    config.auto_class_weights = args.auto_class_weights
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    if args.optimizer_type:
        config.optimizer_type = args.optimizer_type
    if args.sam_rho:
        config.sam_rho = args.sam_rho
    config.sam_adaptive = args.sam_adaptive
    
    # SigLIP2 Contrastive Learning ì„¤ì •
    if args.no_contrastive:
        config.use_contrastive = False
    else:
        config.use_contrastive = args.use_contrastive
    config.contrastive_weight = args.contrastive_weight
    config.contrastive_temperature = args.contrastive_temperature
    
    # ì–¸ì–´ íŒŒì„œ ì„¤ì •
    if args.parser == "cross_lingual":
        # Cross-lingual ëª¨ë“œ
        config.cross_lingual_mode = True
        config.train_languages = args.train_languages
        config.test_languages = args.test_languages
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ë¦„ ì—…ë°ì´íŠ¸
        train_langs_str = "_".join(args.train_languages)
        test_langs_str = "_".join(args.test_languages)
        config.output_dir = f"{config.output_dir}/CrossLingual_Train_{train_langs_str}_Test_{test_langs_str}"
        config.checkpoint_dir = f"{config.output_dir}/checkpoints"
        
        print("ğŸŒ Cross-Lingual ëª¨ë“œ í™œì„±í™”")
        print(f"  í›ˆë ¨ ì–¸ì–´: {args.train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {args.test_languages}")
        print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_dir}")
        
        # config.languagesëŠ” ëª¨ë“  ì–¸ì–´ í¬í•¨ (ë°ì´í„° í™•ì¸ìš©)
        config.languages = args.train_languages + args.test_languages
        
    elif args.parser == "all":
        if args.languages:
            config.languages = args.languages
        else:
            config.languages = ["English", "Greek", "Spanish", "Mandarin"]
    else:
        # ë‹¨ì¼ ì–¸ì–´ ì„ íƒ
        config.languages = [args.parser]
    
    print(f"ì„ íƒëœ ì–¸ì–´: {config.languages}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
    print(f"ì˜µí‹°ë§ˆì´ì €: {config.optimizer_type}")
    print(f"ì†ì‹¤ í•¨ìˆ˜: {config.loss_type}")
    
    # ê²½ë¡œ ë””ë²„ê¹…
    print(f"\nğŸ” ê²½ë¡œ ë””ë²„ê¹…:")
    print(f"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"  config.data_dir: {config.data_dir}")
    print(f"  ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(config.data_dir)}")
    print(f"  ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(config.data_dir)}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.output_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    
    # ëª¨ë¸ í›ˆë ¨
    model, best_model_path = train_model(config)

if __name__ == "__main__":
    main()
