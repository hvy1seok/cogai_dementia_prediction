"""
SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ
ìˆœìˆ˜ PyTorch êµ¬í˜„ (SAM ì˜µí‹°ë§ˆì´ì € ì§€ì›)
"""

import os
import sys
import time
import argparse
import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
import wandb
import numpy as np
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoProcessor
from typing import Dict, List

from config import SigLIPSAMConfig
from model import SigLIPSAMDementiaClassifier
from data_processor import create_dataloaders
from sam_optimizer import SAM

def setup_wandb(config: SigLIPSAMConfig):
    """wandb ì„¤ì • - ì‹¤í—˜ ì„¤ì •ì´ í¬í•¨ëœ ìƒì„¸í•œ ì´ë¦„ ìƒì„±"""
    # ì‹¤í–‰ ì´ë¦„ ìƒì„± - ì„¤ì • ì •ë³´ í¬í•¨
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ì–¸ì–´ ì •ë³´
    if config.cross_lingual_mode:
        train_langs = "_".join(config.train_languages) if config.train_languages else "Unknown"
        test_langs = "_".join(config.test_languages) if config.test_languages else "Unknown"
        lang_info = f"CrossLingual_Train{train_langs}_Test{test_langs}"
    else:
        lang_info = "_".join(config.languages) if len(config.languages) <= 2 else f"{len(config.languages)}langs"
    
    # ëª¨ë¸ ë° ì„¤ì • ì •ë³´
    model_info = config.model_name.split("/")[-1] if "/" in config.model_name else config.model_name
    loss_info = config.loss_type
    opt_info = config.optimizer_type
    
    run_name = f"siglip-sam_{lang_info}_{model_info}_{loss_info}_{opt_info}_bs{config.batch_size}_lr{config.learning_rate}_{timestamp}"
    
    wandb.init(
        project="dementia-prediction-siglip-sam",
        name=run_name,
        tags=[
            f"loss_{config.loss_type}",
            f"optimizer_{config.optimizer_type}",
            f"batch_size_{config.batch_size}",
            f"languages_{len(config.languages)}",
            "cross_lingual" if config.cross_lingual_mode else "standard",
            "sam_optimizer" if config.optimizer_type == "sam" else "standard_optimizer"
        ],
        config={
            "model_name": config.model_name,
            "learning_rate": config.learning_rate,
            "batch_size": config.batch_size,
            "num_epochs": config.num_epochs,
            "languages": config.languages,
            "loss_type": config.loss_type,
            "optimizer_type": config.optimizer_type,
            "focal_alpha": config.focal_alpha,
            "focal_gamma": config.focal_gamma,
            "sam_rho": config.sam_rho,
            "sam_adaptive": config.sam_adaptive,
            "sample_rate": config.sample_rate,
            "n_mels": config.n_mels,
            "image_size": config.image_size,
            "max_length": config.max_length,
            "weight_decay": config.weight_decay,
            "warmup_steps": config.warmup_steps,
            "mixed_precision": config.mixed_precision,
            "gradient_clip_norm": config.gradient_clip_norm,
            # Cross-lingual ì„¤ì •
            "cross_lingual_mode": config.cross_lingual_mode,
            "train_languages": config.train_languages,
            "test_languages": config.test_languages,
        }
    )

def compute_metrics(predictions, labels):
    """ë©”íŠ¸ë¦­ ê³„ì‚° - ìµœì  threshold ê¸°ë°˜"""
    predictions = np.array(predictions)
    labels = np.array(labels)
    
    if predictions.shape[1] == 2:
        # ì´ì§„ ë¶„ë¥˜: ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥  ì‚¬ìš©
        probs = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()
        
        # ROC AUC ê³„ì‚°
        try:
            auc = roc_auc_score(labels, probs)
        except:
            auc = 0.0
        
        # ìµœì  threshold ì°¾ê¸° (Youden's J statistic)
        try:
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(labels, probs)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
        except:
            optimal_threshold = 0.5
        
        # ìµœì  thresholdë¡œ ì˜ˆì¸¡
        optimal_preds = (probs >= optimal_threshold).astype(int)
        
        # ê¸°ë³¸ threshold (0.5)ë¡œë„ ì˜ˆì¸¡
        default_preds = (probs >= 0.5).astype(int)
        
        # argmax ì˜ˆì¸¡ (ê¸°ì¡´ ë°©ì‹)
        argmax_preds = np.argmax(predictions, axis=1)
        
        # ìµœì  threshold ê¸°ë°˜ ë©”íŠ¸ë¦­ (ë©”ì¸)
        optimal_accuracy = accuracy_score(labels, optimal_preds)
        optimal_precision, optimal_recall, optimal_f1, _ = precision_recall_fscore_support(
            labels, optimal_preds, average='weighted', zero_division=0
        )
        
        # ë¹„êµìš© ë©”íŠ¸ë¦­ë“¤
        default_accuracy = accuracy_score(labels, default_preds)
        default_precision, default_recall, default_f1, _ = precision_recall_fscore_support(
            labels, default_preds, average='weighted', zero_division=0
        )
        
        argmax_accuracy = accuracy_score(labels, argmax_preds)
        argmax_precision, argmax_recall, argmax_f1, _ = precision_recall_fscore_support(
            labels, argmax_preds, average='weighted', zero_division=0
        )
        
        return {
            # ë©”ì¸ ì§€í‘œ (ìµœì  threshold ê¸°ë°˜)
            'accuracy': optimal_accuracy,
            'precision': optimal_precision,
            'recall': optimal_recall,
            'f1': optimal_f1,
            'auc': auc,
            'optimal_threshold': optimal_threshold,
            
            # ë¹„êµ ì§€í‘œë“¤
            'accuracy_default': default_accuracy,
            'precision_default': default_precision,
            'recall_default': default_recall,
            'f1_default': default_f1,
            
            'accuracy_argmax': argmax_accuracy,
            'precision_argmax': argmax_precision,
            'recall_argmax': argmax_recall,
            'f1_argmax': argmax_f1,
        }
    else:
        # ë‹¤ì¤‘ ë¶„ë¥˜
        preds = np.argmax(predictions, axis=1)
        accuracy = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': 0.0,
            'optimal_threshold': 0.5
        }

def plot_roc_curve(predictions, labels, title="ROC Curve", save_path=None):
    """ROC ê³¡ì„ ì„ ê·¸ë¦¬ê³  wandbì— ë¡œê¹…"""
    plt.style.use('default')
    fig, ax = plt.subplots(figsize=(8, 6))
    
    try:
        # í™•ë¥ ê°’ ì¶”ì¶œ (ì´ì§„ ë¶„ë¥˜ì˜ positive class í™•ë¥ )
        if len(predictions.shape) > 1:
            probs = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()
        else:
            probs = predictions
        
        # ROC ê³¡ì„  ê³„ì‚°
        if len(np.unique(labels)) > 1:
            fpr, tpr, thresholds = roc_curve(labels, probs)
            auc_score = roc_auc_score(labels, probs)
            
            # ROC ê³¡ì„  ê·¸ë¦¬ê¸°
            ax.plot(fpr, tpr, color='darkorange', lw=2, 
                    label=f'ROC curve (AUC = {auc_score:.3f})')
            ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                    label='Random classifier')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate', fontsize=12)
            ax.set_ylabel('True Positive Rate', fontsize=12)
            ax.set_title(f'{title} (AUC = {auc_score:.3f})', fontsize=14, fontweight='bold')
            ax.legend(loc="lower right", fontsize=10)
            ax.grid(True, alpha=0.3)
            
            # ìµœì  ì„ê³„ê°’ í‘œì‹œ
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
            ax.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=8, 
                    label=f'Optimal threshold = {optimal_threshold:.3f}')
            ax.legend(loc="lower right", fontsize=10)
            
            print(f"ğŸ“Š ROC ê³¡ì„  ìƒì„± ì™„ë£Œ: AUC = {auc_score:.3f}")
            
        else:
            ax.text(0.5, 0.5, 'Cannot plot ROC curve\n(only one class present)', 
                    ha='center', va='center', fontsize=14)
            ax.set_xlim([0, 1])
            ax.set_ylim([0, 1])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(title)
            print("âš ï¸ ROC ê³¡ì„  ìƒì„± ë¶ˆê°€: ë‹¨ì¼ í´ë˜ìŠ¤ë§Œ ì¡´ì¬")
        
        plt.tight_layout()
        
        # ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ROC ê³¡ì„  ì €ì¥: {save_path}")
        
        # wandbì— ë¡œê¹…
        if wandb.run is not None:
            wandb.log({f"{title.lower().replace(' ', '_')}_plot": wandb.Image(fig)})
            print(f"ğŸ“Š ROC ê³¡ì„  wandb ì—…ë¡œë“œ: {title}")
        
        plt.close(fig)
        return fig
        
    except Exception as e:
        print(f"âŒ ROC ê³¡ì„  ìƒì„± ì˜¤ë¥˜: {e}")
        plt.close(fig)
        return None

def plot_confusion_matrix(predictions, labels, title="Confusion Matrix", save_path=None):
    """Confusion Matrixë¥¼ ê·¸ë¦¬ê³  wandbì— ë¡œê¹…"""
    try:
        # ì˜ˆì¸¡ê°’ ë³€í™˜
        if len(predictions.shape) > 1:
            preds = np.argmax(predictions, axis=1)
        else:
            preds = (predictions > 0.5).astype(int)
        
        # Confusion Matrix ê³„ì‚°
        cm = confusion_matrix(labels, preds)
        
        # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Normal', 'Dementia'], 
                   yticklabels=['Normal', 'Dementia'])
        plt.title(f'{title}', fontsize=14, fontweight='bold')
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.tight_layout()
        
        # ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Confusion Matrix ì €ì¥: {save_path}")
        
        # wandbì— ë¡œê¹…
        if wandb.run is not None:
            wandb.log({f"{title.lower().replace(' ', '_')}_plot": wandb.Image(plt.gcf())})
            print(f"ğŸ“Š Confusion Matrix wandb ì—…ë¡œë“œ: {title}")
        
        plt.close()
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥
        accuracy = np.trace(cm) / np.sum(cm)
        print(f"ğŸ“Š {title} ìš”ì•½:")
        print(f"   ì •í™•ë„: {accuracy:.4f}")
        print(f"   ì •ìƒ â†’ ì •ìƒ: {cm[0,0]}, ì •ìƒ â†’ ì¹˜ë§¤: {cm[0,1]}")
        print(f"   ì¹˜ë§¤ â†’ ì •ìƒ: {cm[1,0]}, ì¹˜ë§¤ â†’ ì¹˜ë§¤: {cm[1,1]}")
        
    except Exception as e:
        print(f"âŒ Confusion Matrix ìƒì„± ì˜¤ë¥˜: {e}")
        plt.close()

def train_epoch(model, train_loader, optimizer, config, scaler=None, use_mixed_precision=False):
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    model.train()
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    
    for batch_idx, batch in enumerate(train_loader):
        # GPUë¡œ ì´ë™
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(config.device)
        
        # SAM ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ (mixed precision ë¹„í™œì„±í™”ë¡œ ì•ˆì •ì„± í™•ë³´)
        if config.optimizer_type == "sam":
            # SAMì€ mixed precision ì—†ì´ ì‚¬ìš© (ì•ˆì •ì„±ì„ ìœ„í•´)
            # ì²« ë²ˆì§¸ forward pass
            logits = model(batch)
            loss = model.compute_loss(logits, batch['labels'])
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
            optimizer.first_step(zero_grad=True)
            
            # ë‘ ë²ˆì§¸ forward pass
            logits = model(batch)
            loss = model.compute_loss(logits, batch['labels'])
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
            optimizer.second_step(zero_grad=True)
        
        else:
            # ì¼ë°˜ ì˜µí‹°ë§ˆì´ì €
            optimizer.zero_grad()
            
            if scaler and use_mixed_precision:
                with autocast('cuda'):
                    logits = model(batch)
                    loss = model.compute_loss(logits, batch['labels'])
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                logits = model(batch)
                loss = model.compute_loss(logits, batch['labels'])
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                optimizer.step()
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        total_loss += loss.item()
        all_predictions.extend(logits.detach().cpu().numpy())
        all_labels.extend(batch['labels'].cpu().numpy())
        
        # ë¡œê¹…
        if batch_idx % config.log_interval == 0:
            print(f'Train Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item():.4f}')
            wandb.log({
                'train_batch_loss': loss.item(),
                'train_step': batch_idx
            })
    
    # ì—í¬í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_loss = total_loss / len(train_loader)
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))
    
    return avg_loss, metrics

def evaluate(model, test_loader, config, use_mixed_precision=False, title_prefix="Test"):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in test_loader:
            # GPUë¡œ ì´ë™
            for key in batch:
                if isinstance(batch[key], torch.Tensor):
                    batch[key] = batch[key].to(config.device)
            
            if use_mixed_precision:
                with autocast('cuda'):
                    logits = model(batch)
                    loss = model.compute_loss(logits, batch['labels'])
            else:
                logits = model(batch)
                loss = model.compute_loss(logits, batch['labels'])
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            all_predictions.extend(logits.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_loss = total_loss / len(test_loader)
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))
    
    # ROC ê³¡ì„  ê·¸ë¦¬ê¸° ë° wandb ì—…ë¡œë“œ
    try:
        plot_roc_curve(
            predictions=np.array(all_predictions), 
            labels=np.array(all_labels), 
            title=f"{title_prefix} ROC Curve",
            save_path=os.path.join(config.output_dir, f"{title_prefix.lower()}_roc_curve.png")
        )
    except Exception as e:
        print(f"âš ï¸ ROC ê³¡ì„  ìƒì„± ì‹¤íŒ¨: {e}")
    
    # Confusion Matrix ê·¸ë¦¬ê¸° ë° wandb ì—…ë¡œë“œ
    try:
        plot_confusion_matrix(
            predictions=np.array(all_predictions), 
            labels=np.array(all_labels), 
            title=f"{title_prefix} Confusion Matrix",
            save_path=os.path.join(config.output_dir, f"{title_prefix.lower()}_confusion_matrix.png")
        )
    except Exception as e:
        print(f"âš ï¸ Confusion Matrix ìƒì„± ì‹¤íŒ¨: {e}")
    
    return avg_loss, metrics

def save_checkpoint(model, optimizer, epoch, metrics, config, is_best=False):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config
    }
    
    filename = f"checkpoint_epoch_{epoch:03d}_auc_{metrics['auc']:.3f}.pt"
    if is_best:
        filename = f"best_model_auc_{metrics['auc']:.3f}_epoch_{epoch:03d}.pt"
    
    filepath = os.path.join(config.checkpoint_dir, filename)
    torch.save(checkpoint, filepath)
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
    
    return filepath

def train_model(config: SigLIPSAMConfig):
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("=== SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.cuda.is_available():
        config.device = "cuda"
        print(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        config.device = "cpu"
        print("CPU ì‚¬ìš©")
    
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(config.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.random_seed)
    
    # SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(config.model_name)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader, test_loader = create_dataloaders(
        data_dir=config.data_dir,
        processor=processor,
        config=config,
        cross_lingual_mode=config.cross_lingual_mode,
        train_languages=config.train_languages,
        test_languages=config.test_languages
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_loader.dataset)} ìƒ˜í”Œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_loader.dataset)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_loader.dataset)} ìƒ˜í”Œ")
    
    # ëª¨ë¸ ìƒì„±
    print("ëª¨ë¸ ìƒì„± ì¤‘...")
    model = SigLIPSAMDementiaClassifier(config)
    model.to(config.device)
    
    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer = model.create_optimizer(config)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    total_steps = len(train_loader) * config.num_epochs
    scheduler = model.create_scheduler(optimizer, config, total_steps)
    
    # Mixed precision ìŠ¤ì¼€ì¼ëŸ¬ (SAM ì‚¬ìš© ì‹œ ë¹„í™œì„±í™”)
    use_mixed_precision = config.mixed_precision and config.optimizer_type != "sam"
    scaler = GradScaler('cuda') if use_mixed_precision else None
    
    if config.optimizer_type == "sam" and config.mixed_precision:
        print("âš ï¸ SAM ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ Mixed Precisionì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤ (ì•ˆì •ì„±ì„ ìœ„í•´)")
        use_mixed_precision = False
    
    # wandb ì„¤ì •
    setup_wandb(config)
    
    # í›ˆë ¨ ë£¨í”„
    best_val_auc = 0.0
    best_model_path = None
    
    for epoch in range(config.num_epochs):
        print(f"\n=== Epoch {epoch+1}/{config.num_epochs} ===")
        
        # í›ˆë ¨
        train_loss, train_metrics = train_epoch(model, train_loader, optimizer, config, scaler, use_mixed_precision)
        
        # ê²€ì¦
        val_loss, val_metrics = evaluate(model, val_loader, config, use_mixed_precision, title_prefix="Val")
        
        # í…ŒìŠ¤íŠ¸ (ë§¤ ì—í¬í¬ë§ˆë‹¤ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ)
        test_loss, test_metrics = evaluate(model, test_loader, config, use_mixed_precision, title_prefix="Test")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ë¡œê¹… (ìµœì  threshold ê¸°ë°˜ ë©”íŠ¸ë¦­ í¬í•¨)
        wandb_log = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_accuracy': train_metrics['accuracy'],
            'train_precision': train_metrics['precision'],
            'train_recall': train_metrics['recall'],
            'train_f1': train_metrics['f1'],
            'train_auc': train_metrics['auc'],
            'val_loss': val_loss,
            'val_accuracy': val_metrics['accuracy'],
            'val_precision': val_metrics['precision'],
            'val_recall': val_metrics['recall'],
            'val_f1': val_metrics['f1'],
            'val_auc': val_metrics['auc'],
            'test_loss': test_loss,
            'test_accuracy': test_metrics['accuracy'],
            'test_precision': test_metrics['precision'],
            'test_recall': test_metrics['recall'],
            'test_f1': test_metrics['f1'],
            'test_auc': test_metrics['auc'],
            'learning_rate': optimizer.param_groups[0]['lr']
        }
        
        # ìµœì  threshold ì •ë³´ ì¶”ê°€
        if 'optimal_threshold' in val_metrics:
            wandb_log['val_optimal_threshold'] = val_metrics['optimal_threshold']
        if 'optimal_threshold' in test_metrics:
            wandb_log['test_optimal_threshold'] = test_metrics['optimal_threshold']
        
        # ë¹„êµ ë©”íŠ¸ë¦­ë„ ì¶”ê°€
        if 'accuracy_default' in val_metrics:
            wandb_log['val_accuracy_default_0.5'] = val_metrics['accuracy_default']
            wandb_log['val_accuracy_argmax'] = val_metrics['accuracy_argmax']
        if 'accuracy_default' in test_metrics:
            wandb_log['test_accuracy_default_0.5'] = test_metrics['accuracy_default']
            wandb_log['test_accuracy_argmax'] = test_metrics['accuracy_argmax']
        
        wandb.log(wandb_log)
        
        # ê²°ê³¼ ì¶œë ¥ (ìµœì  threshold ê¸°ë°˜)
        print(f"í›ˆë ¨ - Loss: {train_loss:.4f}, Acc: {train_metrics['accuracy']:.4f}, Prec: {train_metrics['precision']:.4f}, Rec: {train_metrics['recall']:.4f}, F1: {train_metrics['f1']:.4f}, AUC: {train_metrics['auc']:.4f}")
        print(f"ê²€ì¦ - Loss: {val_loss:.4f}, Acc: {val_metrics['accuracy']:.4f}, Prec: {val_metrics['precision']:.4f}, Rec: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ - Loss: {test_loss:.4f}, Acc: {test_metrics['accuracy']:.4f}, Prec: {test_metrics['precision']:.4f}, Rec: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}, AUC: {test_metrics['auc']:.4f}")
        
        # Threshold ì •ë³´ ì¶œë ¥
        if 'optimal_threshold' in val_metrics:
            print(f"ğŸ¯ ê²€ì¦ ìµœì  threshold: {val_metrics['optimal_threshold']:.3f}")
        if 'optimal_threshold' in test_metrics:
            print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ìµœì  threshold: {test_metrics['optimal_threshold']:.3f}")
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (validation AUC ê¸°ì¤€)
        if val_metrics['auc'] > best_val_auc:
            best_val_auc = val_metrics['auc']
            best_model_path = save_checkpoint(model, optimizer, epoch + 1, val_metrics, config, is_best=True)
            print(f"ğŸ† ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ ëª¨ë¸! Validation AUC: {best_val_auc:.4f}")
        
        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % config.save_interval == 0:
            save_checkpoint(model, optimizer, epoch + 1, val_metrics, config, is_best=False)
    
    print(f"\n=== í›ˆë ¨ ì™„ë£Œ ===")
    print(f"ğŸ† ë² ìŠ¤íŠ¸ Validation AUC: {best_val_auc:.4f}")
    print(f"ğŸ’¾ ë² ìŠ¤íŠ¸ ëª¨ë¸: {best_model_path}")
    
    # ìµœì¢… í…ŒìŠ¤íŠ¸ (ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œí•´ì„œ ìµœì¢… í‰ê°€)
    if best_model_path and os.path.exists(best_model_path):
        print("\nğŸ” ë² ìŠ¤íŠ¸ ëª¨ë¸ë¡œ ìµœì¢… í‰ê°€ ìˆ˜í–‰...")
        try:
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ
            checkpoint = torch.load(best_model_path, map_location=config.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # ìµœì¢… í…ŒìŠ¤íŠ¸
            final_test_loss, final_test_metrics = evaluate(model, test_loader, config, use_mixed_precision)
            
            # ìµœì¢… ê²°ê³¼ wandb ë¡œê¹…
            wandb.log({
                'final_test_loss': final_test_loss,
                'final_test_accuracy': final_test_metrics['accuracy'],
                'final_test_f1': final_test_metrics['f1'],
                'final_test_auc': final_test_metrics['auc'],
            })
            
            print(f"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ë² ìŠ¤íŠ¸ ëª¨ë¸):")
            print(f"   Loss: {final_test_loss:.4f}")
            print(f"   AUC: {final_test_metrics['auc']:.4f}")
            print(f"   Accuracy: {final_test_metrics['accuracy']:.4f}")
            print(f"   Precision: {final_test_metrics['precision']:.4f}")
            print(f"   Recall: {final_test_metrics['recall']:.4f}")
            print(f"   F1: {final_test_metrics['f1']:.4f}")
            
            if 'optimal_threshold' in final_test_metrics:
                print(f"   ìµœì  Threshold: {final_test_metrics['optimal_threshold']:.3f}")
                
                # Threshold ë¹„êµ ì¶œë ¥
                if 'accuracy_default' in final_test_metrics:
                    print(f"\nğŸ“Š Threshold ë¹„êµ:")
                    print(f"   ìµœì  threshold ({final_test_metrics['optimal_threshold']:.3f}): Acc={final_test_metrics['accuracy']:.4f}")
                    print(f"   ê¸°ë³¸ threshold (0.500): Acc={final_test_metrics['accuracy_default']:.4f}")
                    print(f"   Argmax ë°©ì‹: Acc={final_test_metrics['accuracy_argmax']:.4f}")
            
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… í‰ê°€ ì‹¤íŒ¨: {e}")
    
    # wandb ì¢…ë£Œ
    wandb.finish()
    
    return model, best_model_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨")
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--data_dir", type=str, default="../../training_dset", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str, default="../modules/outputs/siglip-sam", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="google/siglip2-base-patch16-naflex", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=32, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=100, help="ì—í¬í¬ ìˆ˜")
    
    # ì–¸ì–´ë³„ íŒŒì„œ ì„ íƒ ì˜µì…˜
    parser.add_argument("--parser", type=str, default="all", 
                       choices=["all", "English", "Greek", "Spanish", "Mandarin", "cross_lingual"],
                       help="ì‚¬ìš©í•  ì–¸ì–´ íŒŒì„œ")
    parser.add_argument("--languages", nargs="+", default=None, help="íŠ¹ì • ì–¸ì–´ ëª©ë¡")
    
    # Cross-lingual ëª¨ë“œ ì˜µì…˜
    parser.add_argument("--train_languages", nargs="+", default=["English", "Spanish", "Mandarin"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í›ˆë ¨ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    parser.add_argument("--test_languages", nargs="+", default=["Greek"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ì˜µì…˜
    parser.add_argument("--loss_type", type=str, default="cross_entropy",
                       choices=["cross_entropy", "focal", "bce"],
                       help="ì†ì‹¤ í•¨ìˆ˜ íƒ€ì…")
    parser.add_argument("--focal_alpha", type=float, default=1.0, help="Focal Loss alpha íŒŒë¼ë¯¸í„°")
    parser.add_argument("--focal_gamma", type=float, default=2.0, help="Focal Loss gamma íŒŒë¼ë¯¸í„°")
    
    # ì˜µí‹°ë§ˆì´ì € ì„ íƒ ì˜µì…˜
    parser.add_argument("--optimizer_type", type=str, default="sam",
                       choices=["adamw", "lion", "sam"],
                       help="ì˜µí‹°ë§ˆì´ì € íƒ€ì…")
    parser.add_argument("--sam_rho", type=float, default=0.05, help="SAM rho íŒŒë¼ë¯¸í„°")
    parser.add_argument("--sam_adaptive", action="store_true", help="Adaptive SAM ì‚¬ìš©")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SigLIPSAMConfig()
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config.data_dir = args.data_dir
    if args.output_dir:
        config.output_dir = args.output_dir
        config.checkpoint_dir = f"{args.output_dir}/checkpoints"
    if args.model_name:
        config.model_name = args.model_name
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.num_epochs:
        config.num_epochs = args.num_epochs
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    if args.loss_type:
        config.loss_type = args.loss_type
    if args.focal_alpha:
        config.focal_alpha = args.focal_alpha
    if args.focal_gamma:
        config.focal_gamma = args.focal_gamma
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    if args.optimizer_type:
        config.optimizer_type = args.optimizer_type
    if args.sam_rho:
        config.sam_rho = args.sam_rho
    config.sam_adaptive = args.sam_adaptive
    
    # ì–¸ì–´ íŒŒì„œ ì„¤ì •
    if args.parser == "cross_lingual":
        # Cross-lingual ëª¨ë“œ
        config.cross_lingual_mode = True
        config.train_languages = args.train_languages
        config.test_languages = args.test_languages
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ë¦„ ì—…ë°ì´íŠ¸
        train_langs_str = "_".join(args.train_languages)
        test_langs_str = "_".join(args.test_languages)
        config.output_dir = f"{config.output_dir}/CrossLingual_Train_{train_langs_str}_Test_{test_langs_str}"
        config.checkpoint_dir = f"{config.output_dir}/checkpoints"
        
        print("ğŸŒ Cross-Lingual ëª¨ë“œ í™œì„±í™”")
        print(f"  í›ˆë ¨ ì–¸ì–´: {args.train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {args.test_languages}")
        print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_dir}")
        
        # config.languagesëŠ” ëª¨ë“  ì–¸ì–´ í¬í•¨ (ë°ì´í„° í™•ì¸ìš©)
        config.languages = args.train_languages + args.test_languages
        
    elif args.parser == "all":
        if args.languages:
            config.languages = args.languages
        else:
            config.languages = ["English", "Greek", "Spanish", "Mandarin"]
    else:
        # ë‹¨ì¼ ì–¸ì–´ ì„ íƒ
        config.languages = [args.parser]
    
    print(f"ì„ íƒëœ ì–¸ì–´: {config.languages}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
    print(f"ì˜µí‹°ë§ˆì´ì €: {config.optimizer_type}")
    print(f"ì†ì‹¤ í•¨ìˆ˜: {config.loss_type}")
    
    # ê²½ë¡œ ë””ë²„ê¹…
    print(f"\nğŸ” ê²½ë¡œ ë””ë²„ê¹…:")
    print(f"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"  config.data_dir: {config.data_dir}")
    print(f"  ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(config.data_dir)}")
    print(f"  ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(config.data_dir)}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.output_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    
    # ëª¨ë¸ í›ˆë ¨
    model, best_model_path = train_model(config)

if __name__ == "__main__":
    main()
