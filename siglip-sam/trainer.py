"""
SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ
ìˆœìˆ˜ PyTorch êµ¬í˜„ (SAM ì˜µí‹°ë§ˆì´ì € ì§€ì›)
"""

import os
import sys
import time
import argparse
import torch
import torch.nn as nn
from torch.cuda.amp import GradScaler, autocast
import wandb
import numpy as np
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from transformers import AutoProcessor
from typing import Dict, List

from config import SigLIPSAMConfig
from model import SigLIPSAMDementiaClassifier
from data_processor import create_dataloaders
from sam_optimizer import SAM

def setup_wandb(config: SigLIPSAMConfig):
    """wandb ì„¤ì • - ì‹¤í—˜ ì„¤ì •ì´ í¬í•¨ëœ ìƒì„¸í•œ ì´ë¦„ ìƒì„±"""
    # ì‹¤í–‰ ì´ë¦„ ìƒì„± - ì„¤ì • ì •ë³´ í¬í•¨
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ì–¸ì–´ ì •ë³´
    if config.cross_lingual_mode:
        train_langs = "_".join(config.train_languages) if config.train_languages else "Unknown"
        test_langs = "_".join(config.test_languages) if config.test_languages else "Unknown"
        lang_info = f"CrossLingual_Train{train_langs}_Test{test_langs}"
    else:
        lang_info = "_".join(config.languages) if len(config.languages) <= 2 else f"{len(config.languages)}langs"
    
    # ëª¨ë¸ ë° ì„¤ì • ì •ë³´
    model_info = config.model_name.split("/")[-1] if "/" in config.model_name else config.model_name
    loss_info = config.loss_type
    opt_info = config.optimizer_type
    
    run_name = f"siglip-sam_{lang_info}_{model_info}_{loss_info}_{opt_info}_bs{config.batch_size}_lr{config.learning_rate}_{timestamp}"
    
    wandb.init(
        project="dementia-prediction-siglip-sam",
        name=run_name,
        tags=[
            f"loss_{config.loss_type}",
            f"optimizer_{config.optimizer_type}",
            f"batch_size_{config.batch_size}",
            f"languages_{len(config.languages)}",
            "cross_lingual" if config.cross_lingual_mode else "standard",
            "sam_optimizer" if config.optimizer_type == "sam" else "standard_optimizer"
        ],
        config={
            "model_name": config.model_name,
            "learning_rate": config.learning_rate,
            "batch_size": config.batch_size,
            "num_epochs": config.num_epochs,
            "languages": config.languages,
            "loss_type": config.loss_type,
            "optimizer_type": config.optimizer_type,
            "focal_alpha": config.focal_alpha,
            "focal_gamma": config.focal_gamma,
            "sam_rho": config.sam_rho,
            "sam_adaptive": config.sam_adaptive,
            "sample_rate": config.sample_rate,
            "n_mels": config.n_mels,
            "image_size": config.image_size,
            "max_length": config.max_length,
            "weight_decay": config.weight_decay,
            "warmup_steps": config.warmup_steps,
            "mixed_precision": config.mixed_precision,
            "gradient_clip_norm": config.gradient_clip_norm,
            # Cross-lingual ì„¤ì •
            "cross_lingual_mode": config.cross_lingual_mode,
            "train_languages": config.train_languages,
            "test_languages": config.test_languages,
        }
    )

def compute_metrics(predictions, labels):
    """ë©”íŠ¸ë¦­ ê³„ì‚°"""
    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’
    preds = np.argmax(predictions, axis=1)
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    accuracy = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    
    # AUC ê³„ì‚° (í™•ë¥ ê°’ ì‚¬ìš©)
    try:
        probs = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()
        auc = roc_auc_score(labels, probs)
    except:
        auc = 0.0
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc
    }

def train_epoch(model, train_loader, optimizer, config, scaler=None):
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    model.train()
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    
    for batch_idx, batch in enumerate(train_loader):
        # GPUë¡œ ì´ë™
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(config.device)
        
        # SAM ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ
        if config.optimizer_type == "sam":
            # ì²« ë²ˆì§¸ forward pass
            def closure():
                if scaler and config.mixed_precision:
                    with autocast():
                        logits = model(batch)
                        loss = model.compute_loss(logits, batch['labels'])
                    scaler.scale(loss).backward()
                else:
                    logits = model(batch)
                    loss = model.compute_loss(logits, batch['labels'])
                    loss.backward()
                return loss
            
            # SAM first step
            if scaler and config.mixed_precision:
                with autocast():
                    logits = model(batch)
                    loss = model.compute_loss(logits, batch['labels'])
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                optimizer.first_step(zero_grad=True)
                
                # ë‘ ë²ˆì§¸ forward pass
                closure()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                optimizer.second_step(zero_grad=True)
                scaler.update()
            else:
                logits = model(batch)
                loss = model.compute_loss(logits, batch['labels'])
                loss.backward()
                optimizer.first_step(zero_grad=True)
                
                # ë‘ ë²ˆì§¸ forward pass
                closure()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                optimizer.second_step(zero_grad=True)
        
        else:
            # ì¼ë°˜ ì˜µí‹°ë§ˆì´ì €
            optimizer.zero_grad()
            
            if scaler and config.mixed_precision:
                with autocast():
                    logits = model(batch)
                    loss = model.compute_loss(logits, batch['labels'])
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                logits = model(batch)
                loss = model.compute_loss(logits, batch['labels'])
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                optimizer.step()
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        total_loss += loss.item()
        all_predictions.extend(logits.detach().cpu().numpy())
        all_labels.extend(batch['labels'].cpu().numpy())
        
        # ë¡œê¹…
        if batch_idx % config.log_interval == 0:
            print(f'Train Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item():.4f}')
            wandb.log({
                'train_batch_loss': loss.item(),
                'train_step': batch_idx
            })
    
    # ì—í¬í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_loss = total_loss / len(train_loader)
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))
    
    return avg_loss, metrics

def evaluate(model, test_loader, config):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in test_loader:
            # GPUë¡œ ì´ë™
            for key in batch:
                if isinstance(batch[key], torch.Tensor):
                    batch[key] = batch[key].to(config.device)
            
            if config.mixed_precision:
                with autocast():
                    logits = model(batch)
                    loss = model.compute_loss(logits, batch['labels'])
            else:
                logits = model(batch)
                loss = model.compute_loss(logits, batch['labels'])
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            all_predictions.extend(logits.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_loss = total_loss / len(test_loader)
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))
    
    return avg_loss, metrics

def save_checkpoint(model, optimizer, epoch, metrics, config, is_best=False):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config
    }
    
    filename = f"checkpoint_epoch_{epoch:03d}_auc_{metrics['auc']:.3f}.pt"
    if is_best:
        filename = f"best_model_auc_{metrics['auc']:.3f}_epoch_{epoch:03d}.pt"
    
    filepath = os.path.join(config.checkpoint_dir, filename)
    torch.save(checkpoint, filepath)
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
    
    return filepath

def train_model(config: SigLIPSAMConfig):
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("=== SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.cuda.is_available():
        config.device = "cuda"
        print(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        config.device = "cpu"
        print("CPU ì‚¬ìš©")
    
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(config.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.random_seed)
    
    # SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(config.model_name)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    train_loader, test_loader = create_dataloaders(
        data_dir=config.data_dir,
        processor=processor,
        config=config,
        cross_lingual_mode=config.cross_lingual_mode,
        train_languages=config.train_languages,
        test_languages=config.test_languages
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_loader.dataset)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_loader.dataset)} ìƒ˜í”Œ")
    
    # ëª¨ë¸ ìƒì„±
    print("ëª¨ë¸ ìƒì„± ì¤‘...")
    model = SigLIPSAMDementiaClassifier(config)
    model.to(config.device)
    
    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer = model.create_optimizer(config)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    total_steps = len(train_loader) * config.num_epochs
    scheduler = model.create_scheduler(optimizer, config, total_steps)
    
    # Mixed precision ìŠ¤ì¼€ì¼ëŸ¬
    scaler = GradScaler() if config.mixed_precision else None
    
    # wandb ì„¤ì •
    setup_wandb(config)
    
    # í›ˆë ¨ ë£¨í”„
    best_auc = 0.0
    best_model_path = None
    
    for epoch in range(config.num_epochs):
        print(f"\n=== Epoch {epoch+1}/{config.num_epochs} ===")
        
        # í›ˆë ¨
        train_loss, train_metrics = train_epoch(model, train_loader, optimizer, config, scaler)
        
        # í‰ê°€
        test_loss, test_metrics = evaluate(model, test_loader, config)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ë¡œê¹…
        wandb.log({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_accuracy': train_metrics['accuracy'],
            'train_f1': train_metrics['f1'],
            'train_auc': train_metrics['auc'],
            'test_loss': test_loss,
            'test_accuracy': test_metrics['accuracy'],
            'test_f1': test_metrics['f1'],
            'test_auc': test_metrics['auc'],
            'learning_rate': optimizer.param_groups[0]['lr']
        })
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"í›ˆë ¨ - Loss: {train_loss:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}, AUC: {train_metrics['auc']:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ - Loss: {test_loss:.4f}, Acc: {test_metrics['accuracy']:.4f}, F1: {test_metrics['f1']:.4f}, AUC: {test_metrics['auc']:.4f}")
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
        if test_metrics['auc'] > best_auc:
            best_auc = test_metrics['auc']
            best_model_path = save_checkpoint(model, optimizer, epoch + 1, test_metrics, config, is_best=True)
            print(f"ğŸ† ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ ëª¨ë¸! AUC: {best_auc:.4f}")
        
        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % config.save_interval == 0:
            save_checkpoint(model, optimizer, epoch + 1, test_metrics, config, is_best=False)
    
    print(f"\n=== í›ˆë ¨ ì™„ë£Œ ===")
    print(f"ğŸ† ë² ìŠ¤íŠ¸ AUC: {best_auc:.4f}")
    print(f"ğŸ’¾ ë² ìŠ¤íŠ¸ ëª¨ë¸: {best_model_path}")
    
    # wandb ì¢…ë£Œ
    wandb.finish()
    
    return model, best_model_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="SigLIP-SAM ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨")
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--data_dir", type=str, default="../../training_dset", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str, default="../modules/outputs/siglip-sam", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="google/siglip2-base-patch16-naflex", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=32, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=100, help="ì—í¬í¬ ìˆ˜")
    
    # ì–¸ì–´ë³„ íŒŒì„œ ì„ íƒ ì˜µì…˜
    parser.add_argument("--parser", type=str, default="all", 
                       choices=["all", "English", "Greek", "Spanish", "Mandarin", "cross_lingual"],
                       help="ì‚¬ìš©í•  ì–¸ì–´ íŒŒì„œ")
    parser.add_argument("--languages", nargs="+", default=None, help="íŠ¹ì • ì–¸ì–´ ëª©ë¡")
    
    # Cross-lingual ëª¨ë“œ ì˜µì…˜
    parser.add_argument("--train_languages", nargs="+", default=["English", "Spanish", "Mandarin"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í›ˆë ¨ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    parser.add_argument("--test_languages", nargs="+", default=["Greek"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ì˜µì…˜
    parser.add_argument("--loss_type", type=str, default="cross_entropy",
                       choices=["cross_entropy", "focal", "bce"],
                       help="ì†ì‹¤ í•¨ìˆ˜ íƒ€ì…")
    parser.add_argument("--focal_alpha", type=float, default=1.0, help="Focal Loss alpha íŒŒë¼ë¯¸í„°")
    parser.add_argument("--focal_gamma", type=float, default=2.0, help="Focal Loss gamma íŒŒë¼ë¯¸í„°")
    
    # ì˜µí‹°ë§ˆì´ì € ì„ íƒ ì˜µì…˜
    parser.add_argument("--optimizer_type", type=str, default="sam",
                       choices=["adamw", "lion", "sam"],
                       help="ì˜µí‹°ë§ˆì´ì € íƒ€ì…")
    parser.add_argument("--sam_rho", type=float, default=0.05, help="SAM rho íŒŒë¼ë¯¸í„°")
    parser.add_argument("--sam_adaptive", action="store_true", help="Adaptive SAM ì‚¬ìš©")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SigLIPSAMConfig()
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config.data_dir = args.data_dir
    if args.output_dir:
        config.output_dir = args.output_dir
        config.checkpoint_dir = f"{args.output_dir}/checkpoints"
    if args.model_name:
        config.model_name = args.model_name
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.num_epochs:
        config.num_epochs = args.num_epochs
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    if args.loss_type:
        config.loss_type = args.loss_type
    if args.focal_alpha:
        config.focal_alpha = args.focal_alpha
    if args.focal_gamma:
        config.focal_gamma = args.focal_gamma
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    if args.optimizer_type:
        config.optimizer_type = args.optimizer_type
    if args.sam_rho:
        config.sam_rho = args.sam_rho
    config.sam_adaptive = args.sam_adaptive
    
    # ì–¸ì–´ íŒŒì„œ ì„¤ì •
    if args.parser == "cross_lingual":
        # Cross-lingual ëª¨ë“œ
        config.cross_lingual_mode = True
        config.train_languages = args.train_languages
        config.test_languages = args.test_languages
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ë¦„ ì—…ë°ì´íŠ¸
        train_langs_str = "_".join(args.train_languages)
        test_langs_str = "_".join(args.test_languages)
        config.output_dir = f"{config.output_dir}/CrossLingual_Train_{train_langs_str}_Test_{test_langs_str}"
        config.checkpoint_dir = f"{config.output_dir}/checkpoints"
        
        print("ğŸŒ Cross-Lingual ëª¨ë“œ í™œì„±í™”")
        print(f"  í›ˆë ¨ ì–¸ì–´: {args.train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {args.test_languages}")
        print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_dir}")
        
        # config.languagesëŠ” ëª¨ë“  ì–¸ì–´ í¬í•¨ (ë°ì´í„° í™•ì¸ìš©)
        config.languages = args.train_languages + args.test_languages
        
    elif args.parser == "all":
        if args.languages:
            config.languages = args.languages
        else:
            config.languages = ["English", "Greek", "Spanish", "Mandarin"]
    else:
        # ë‹¨ì¼ ì–¸ì–´ ì„ íƒ
        config.languages = [args.parser]
    
    print(f"ì„ íƒëœ ì–¸ì–´: {config.languages}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
    print(f"ì˜µí‹°ë§ˆì´ì €: {config.optimizer_type}")
    print(f"ì†ì‹¤ í•¨ìˆ˜: {config.loss_type}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.output_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    
    # ëª¨ë¸ í›ˆë ¨
    model, best_model_path = train_model(config)

if __name__ == "__main__":
    main()
