"""
ì§„ì •í•œ SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ
- EMA Teacher-Student í•™ìŠµ
- Multi-Loss: SILC/TIPS + Sigmoid + LoCa + Classification
- Caption generation ë° dense captioning
- SAM ì˜µí‹°ë§ˆì´ì € ì§€ì›
"""

import os
import sys
import time
import argparse
import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
import wandb
import numpy as np
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoProcessor, AutoTokenizer
from typing import Dict, List

from config import SigLIPSAMConfig
from true_siglip2_model import TrueSigLIP2DementiaClassifier
from data_processor import create_dataloaders
from sam_optimizer import SAM

def setup_wandb(config: SigLIPSAMConfig):
    """wandb ì„¤ì • - True SigLIP2 ì „ìš©"""
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ì–¸ì–´ ì •ë³´
    if config.cross_lingual_mode:
        train_langs = "_".join(config.train_languages) if config.train_languages else "Unknown"
        test_langs = "_".join(config.test_languages) if config.test_languages else "Unknown"
        lang_info = f"CrossLingual_Train{train_langs}_Test{test_langs}"
    else:
        lang_info = "_".join(config.languages) if len(config.languages) <= 2 else f"{len(config.languages)}langs"
    
    model_info = "TrueSigLIP2"
    loss_info = f"{config.loss_type}_MultiLoss"
    opt_info = config.optimizer_type
    
    run_name = f"true-siglip2_{lang_info}_{model_info}_{loss_info}_{opt_info}_bs{config.batch_size}_lr{config.learning_rate}_{timestamp}"
    
    wandb.init(
        project="dementia-prediction-true-siglip2",
        name=run_name,
        tags=[
            f"loss_{config.loss_type}",
            f"optimizer_{config.optimizer_type}",
            f"batch_size_{config.batch_size}",
            f"languages_{len(config.languages)}",
            "cross_lingual" if config.cross_lingual_mode else "standard",
            "true_siglip2",
            "ema_teacher_student",
            "multi_loss",
            "caption_generation"
        ],
        config={
            "model_name": config.model_name,
            "learning_rate": config.learning_rate,
            "batch_size": config.batch_size,
            "num_epochs": config.num_epochs,
            "languages": config.languages,
            "loss_type": config.loss_type,
            "optimizer_type": config.optimizer_type,
            "sam_rho": config.sam_rho,
            "ema_momentum": getattr(config, 'ema_momentum', 0.999),
            "silc_weight": getattr(config, 'silc_weight', 0.2),
            "sigmoid_weight": getattr(config, 'sigmoid_weight', 1.0),
            "loca_weight": getattr(config, 'loca_weight', 1.0),
            "classification_weight": getattr(config, 'classification_weight', 1.0),
            "cross_lingual_mode": config.cross_lingual_mode,
            "train_languages": config.train_languages,
            "test_languages": config.test_languages,
        }
    )

def compute_metrics(predictions, labels, languages=None):
    """ë©”íŠ¸ë¦­ ê³„ì‚° - ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©"""
    predictions = np.array(predictions)
    labels = np.array(labels)
    
    if predictions.shape[1] == 2:
        probs = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()
        
        try:
            auc = roc_auc_score(labels, probs)
        except:
            auc = 0.0
        
        try:
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(labels, probs)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
        except:
            optimal_threshold = 0.5
        
        optimal_preds = (probs >= optimal_threshold).astype(int)
        default_preds = (probs >= 0.5).astype(int)
        argmax_preds = np.argmax(predictions, axis=1)
        
        optimal_accuracy = accuracy_score(labels, optimal_preds)
        optimal_precision, optimal_recall, optimal_f1, _ = precision_recall_fscore_support(
            labels, optimal_preds, average='weighted', zero_division=0
        )
        
        default_accuracy = accuracy_score(labels, default_preds)
        default_precision, default_recall, default_f1, _ = precision_recall_fscore_support(
            labels, default_preds, average='weighted', zero_division=0
        )
        
        metrics = {
            'accuracy': optimal_accuracy,
            'precision': optimal_precision,
            'recall': optimal_recall,
            'f1': optimal_f1,
            'auc': auc,
            'optimal_threshold': optimal_threshold,
            'accuracy_default': default_accuracy,
            'precision_default': default_precision,
            'recall_default': default_recall,
            'f1_default': default_f1,
        }
        
        if languages is not None:
            from trainer import compute_language_specific_metrics
            language_metrics = compute_language_specific_metrics(probs, labels, languages, optimal_threshold)
            metrics.update(language_metrics)
        
        return metrics
    else:
        preds = np.argmax(predictions, axis=1)
        accuracy = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': 0.0,
            'optimal_threshold': 0.5
        }

def train_epoch(model, train_loader, optimizer, config, scaler=None, use_mixed_precision=False):
    """í•œ ì—í¬í¬ í›ˆë ¨ - True SigLIP2 Multi-Loss"""
    model.train()
    
    total_loss = 0.0
    total_classification_loss = 0.0
    total_silc_loss = 0.0
    total_sigmoid_loss = 0.0
    total_loca_loss = 0.0
    
    all_predictions = []
    all_labels = []
    loss_components_sum = {}
    
    for batch_idx, batch in enumerate(train_loader):
        # GPUë¡œ ì´ë™
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(config.device)
        
        # í™˜ì ID ì¶”ì¶œ
        if 'patient_id' in batch:
            patient_ids = batch['patient_id'] if isinstance(batch['patient_id'], list) else [batch['patient_id']]
        else:
            if 'language' in batch:
                languages = batch['language'] if isinstance(batch['language'], list) else ['Unknown'] * len(batch['labels'])
                patient_ids = [f"{lang}_{i // 2}" for i, lang in enumerate(languages)]
            else:
                patient_ids = [f"patient_{i // 2}" for i in range(len(batch['labels']))]
        
        # Caption targets (ì„ì‹œë¡œ None - ì‹¤ì œ êµ¬í˜„ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ)
        caption_targets = None  # TODO: ì‹¤ì œ caption ë°ì´í„° ì¶”ê°€ì‹œ êµ¬í˜„
        
        # SAM ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ
        if config.optimizer_type == "sam":
            # ì²« ë²ˆì§¸ forward pass
            model_outputs = model(batch, return_embeddings=True, training=True)
            loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids, caption_targets)
            loss = loss_dict['total_loss']
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
            optimizer.first_step(zero_grad=True)
            
            # ë‘ ë²ˆì§¸ forward pass
            model_outputs = model(batch, return_embeddings=True, training=True)
            loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids, caption_targets)
            loss = loss_dict['total_loss']
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
            optimizer.second_step(zero_grad=True)
            
            # EMA Teacher ì—…ë°ì´íŠ¸
            model.update_teacher()
        
        else:
            # ì¼ë°˜ ì˜µí‹°ë§ˆì´ì €
            optimizer.zero_grad()
            
            if scaler and use_mixed_precision:
                with autocast('cuda'):
                    model_outputs = model(batch, return_embeddings=True, training=True)
                    loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids, caption_targets)
                    loss = loss_dict['total_loss']
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                model_outputs = model(batch, return_embeddings=True, training=True)
                loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids, caption_targets)
                loss = loss_dict['total_loss']
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                optimizer.step()
            
            # EMA Teacher ì—…ë°ì´íŠ¸
            model.update_teacher()
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        logits = model_outputs['classification_logits']
        total_loss += loss.item()
        
        # Loss components ìˆ˜ì§‘
        for key, value in loss_dict['loss_components'].items():
            if key not in loss_components_sum:
                loss_components_sum[key] = 0.0
            if isinstance(value, torch.Tensor):
                loss_components_sum[key] += value.item()
            else:
                loss_components_sum[key] += value
        
        all_predictions.extend(logits.detach().cpu().numpy())
        all_labels.extend(batch['labels'].cpu().numpy())
        
        # ë¡œê¹…
        if batch_idx % config.log_interval == 0:
            log_msg = f'Train Batch {batch_idx}/{len(train_loader)}: Total Loss = {loss.item():.4f}'
            
            # ì£¼ìš” loss components ì¶œë ¥
            if 'classification_loss' in loss_dict['loss_components']:
                log_msg += f', Cls = {loss_dict["loss_components"]["classification_loss"]:.4f}'
            if 'silc_silc_tips_loss' in loss_dict['loss_components']:
                log_msg += f', SILC = {loss_dict["loss_components"]["silc_silc_tips_loss"]:.4f}'
            if 'sigmoid_contrastive_loss' in loss_dict['loss_components']:
                log_msg += f', Sigmoid = {loss_dict["loss_components"]["sigmoid_contrastive_loss"]:.4f}'
            if 'loca_loca_loss' in loss_dict['loss_components']:
                log_msg += f', LoCa = {loss_dict["loss_components"]["loca_loca_loss"]:.4f}'
            
            print(log_msg)
            
            # wandb ë¡œê¹…
            wandb_log = {
                'train_batch_total_loss': loss.item(),
                'train_step': batch_idx
            }
            
            # Loss components ì¶”ê°€
            for key, value in loss_dict['loss_components'].items():
                if isinstance(value, torch.Tensor):
                    wandb_log[f'train_batch_{key}'] = value.item()
                else:
                    wandb_log[f'train_batch_{key}'] = value
                    
            wandb.log(wandb_log)
    
    # ì—í¬í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
    num_batches = len(train_loader)
    avg_total_loss = total_loss / num_batches
    
    # Loss components í‰ê·  ê³„ì‚°
    avg_loss_components = {}
    for key, value in loss_components_sum.items():
        avg_loss_components[key] = value / num_batches
    
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))
    metrics.update(avg_loss_components)
    
    return avg_total_loss, metrics

def evaluate(model, test_loader, config, use_mixed_precision=False, title_prefix="Test"):
    """ëª¨ë¸ í‰ê°€ - True SigLIP2"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    all_languages = []
    loss_components_sum = {}
    
    with torch.no_grad():
        for batch in test_loader:
            # GPUë¡œ ì´ë™
            for key in batch:
                if isinstance(batch[key], torch.Tensor):
                    batch[key] = batch[key].to(config.device)
            
            # í™˜ì ID ì¶”ì¶œ
            if 'patient_id' in batch:
                patient_ids = batch['patient_id'] if isinstance(batch['patient_id'], list) else [batch['patient_id']]
            else:
                if 'language' in batch:
                    languages = batch['language'] if isinstance(batch['language'], list) else ['Unknown'] * len(batch['labels'])
                    patient_ids = [f"{lang}_{i // 2}" for i, lang in enumerate(languages)]
                else:
                    patient_ids = [f"patient_{i // 2}" for i in range(len(batch['labels']))]
            
            caption_targets = None
            
            if use_mixed_precision:
                with autocast('cuda'):
                    model_outputs = model(batch, return_embeddings=True, training=False)
                    loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids, caption_targets)
                    loss = loss_dict['total_loss']
            else:
                model_outputs = model(batch, return_embeddings=True, training=False)
                loss_dict = model.compute_loss(model_outputs, batch['labels'], patient_ids, caption_targets)
                loss = loss_dict['total_loss']
            
            logits = model_outputs['classification_logits']
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            
            # Loss components ìˆ˜ì§‘
            for key, value in loss_dict['loss_components'].items():
                if key not in loss_components_sum:
                    loss_components_sum[key] = 0.0
                if isinstance(value, torch.Tensor):
                    loss_components_sum[key] += value.item()
                else:
                    loss_components_sum[key] += value
            
            all_predictions.extend(logits.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())
            
            # ì–¸ì–´ ì •ë³´ ìˆ˜ì§‘
            if 'language' in batch:
                if isinstance(batch['language'], list):
                    all_languages.extend(batch['language'])
                else:
                    all_languages.extend(['Unknown'] * len(batch['labels']))
            else:
                all_languages.extend(['Unknown'] * len(batch['labels']))
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    num_batches = len(test_loader)
    avg_total_loss = total_loss / num_batches
    
    # Loss components í‰ê·  ê³„ì‚°
    avg_loss_components = {}
    for key, value in loss_components_sum.items():
        avg_loss_components[key] = value / num_batches
    
    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels), all_languages)
    metrics.update(avg_loss_components)
    
    # ROC ê³¡ì„  ë° Confusion Matrix ìƒì„± (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
    try:
        from trainer import plot_roc_curve, plot_confusion_matrix
        plot_roc_curve(
            predictions=np.array(all_predictions), 
            labels=np.array(all_labels), 
            title=f"{title_prefix} ROC Curve",
            save_path=os.path.join(config.output_dir, f"{title_prefix.lower()}_roc_curve.png")
        )
        
        plot_confusion_matrix(
            predictions=np.array(all_predictions), 
            labels=np.array(all_labels), 
            title=f"{title_prefix} Confusion Matrix",
            save_path=os.path.join(config.output_dir, f"{title_prefix.lower()}_confusion_matrix.png")
        )
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    return avg_total_loss, metrics

def save_checkpoint(model, optimizer, epoch, metrics, config, is_best=False):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config
    }
    
    filename = f"true_siglip2_checkpoint_epoch_{epoch:03d}_auc_{metrics['auc']:.3f}.pt"
    if is_best:
        filename = f"true_siglip2_best_model_auc_{metrics['auc']:.3f}_epoch_{epoch:03d}.pt"
    
    filepath = os.path.join(config.checkpoint_dir, filename)
    torch.save(checkpoint, filepath)
    print(f"ğŸ’¾ True SigLIP2 ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
    
    return filepath

def train_model(config: SigLIPSAMConfig):
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜ - True SigLIP2"""
    print("=== ì§„ì •í•œ SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.cuda.is_available():
        config.device = "cuda"
        print(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        config.device = "cpu"
        print("CPU ì‚¬ìš©")
    
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(config.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.random_seed)
    
    # SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(config.model_name)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader, test_loader = create_dataloaders(
        data_dir=config.data_dir,
        processor=processor,
        config=config,
        cross_lingual_mode=config.cross_lingual_mode,
        train_languages=config.train_languages,
        test_languages=config.test_languages
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_loader.dataset)} ìƒ˜í”Œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_loader.dataset)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_loader.dataset)} ìƒ˜í”Œ")
    
    # True SigLIP2 ëª¨ë¸ ìƒì„±
    print("ì§„ì •í•œ SigLIP2 ëª¨ë¸ ìƒì„± ì¤‘...")
    model = TrueSigLIP2DementiaClassifier(config)
    model.to(config.device)
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    from data_processor import compute_class_weights
    
    if hasattr(train_loader.dataset, 'dataset'):
        original_dataset = train_loader.dataset.dataset
    else:
        original_dataset = train_loader.dataset
    
    class_weights = compute_class_weights(original_dataset, config)
    model.setup_loss_function(class_weights)
    
    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer = model.create_optimizer(config)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    total_steps = len(train_loader) * config.num_epochs
    scheduler = model.create_scheduler(optimizer, config, total_steps)
    
    # Mixed precision ìŠ¤ì¼€ì¼ëŸ¬ (SAM ì‚¬ìš© ì‹œ ë¹„í™œì„±í™”)
    use_mixed_precision = config.mixed_precision and config.optimizer_type != "sam"
    scaler = GradScaler('cuda') if use_mixed_precision else None
    
    if config.optimizer_type == "sam" and config.mixed_precision:
        print("âš ï¸ SAM ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ Mixed Precisionì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤")
        use_mixed_precision = False
    
    # wandb ì„¤ì •
    setup_wandb(config)
    
    # í›ˆë ¨ ë£¨í”„
    best_val_auc = 0.0
    best_model_path = None
    early_stopping_patience = getattr(config, 'early_stopping_patience', 15)
    epochs_without_improvement = 0
    
    for epoch in range(config.num_epochs):
        print(f"\n=== Epoch {epoch+1}/{config.num_epochs} ===")
        
        # í›ˆë ¨
        train_loss, train_metrics = train_epoch(model, train_loader, optimizer, config, scaler, use_mixed_precision)
        
        # ê²€ì¦
        val_loss, val_metrics = evaluate(model, val_loader, config, use_mixed_precision, title_prefix="Val")
        
        # í…ŒìŠ¤íŠ¸ (ì°¸ê³ ìš©)
        test_loss, test_metrics = evaluate(model, test_loader, config, use_mixed_precision, title_prefix="Test")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # wandb ë¡œê¹…
        wandb_log = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_accuracy': train_metrics['accuracy'],
            'train_auc': train_metrics['auc'],
            'val_loss': val_loss,
            'val_accuracy': val_metrics['accuracy'],
            'val_auc': val_metrics['auc'],
            'test_loss': test_loss,
            'test_accuracy': test_metrics['accuracy'],
            'test_auc': test_metrics['auc'],
            'learning_rate': optimizer.param_groups[0]['lr']
        }
        
        # Multi-Loss components ì¶”ê°€
        for prefix, metrics_dict in [('train', train_metrics), ('val', val_metrics), ('test', test_metrics)]:
            for key, value in metrics_dict.items():
                if any(loss_type in key for loss_type in ['classification_loss', 'silc_', 'sigmoid_', 'loca_']):
                    wandb_log[f'{prefix}_{key}'] = value
        
        # ì–¸ì–´ë³„ ë©”íŠ¸ë¦­ ì¶”ê°€ (í…ŒìŠ¤íŠ¸ì—ì„œë§Œ)
        for key, value in test_metrics.items():
            if any(lang in key for lang in ['English', 'Greek', 'Spanish', 'Mandarin']):
                wandb_log[f'test_{key}'] = value
        
        wandb.log(wandb_log)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"í›ˆë ¨ - Loss: {train_loss:.4f}, Acc: {train_metrics['accuracy']:.4f}, AUC: {train_metrics['auc']:.4f}")
        print(f"ê²€ì¦ - Loss: {val_loss:.4f}, Acc: {val_metrics['accuracy']:.4f}, AUC: {val_metrics['auc']:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ - Loss: {test_loss:.4f}, Acc: {test_metrics['accuracy']:.4f}, AUC: {test_metrics['auc']:.4f}")
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ ë° Early Stopping
        if val_metrics['auc'] > best_val_auc:
            best_val_auc = val_metrics['auc']
            best_model_path = save_checkpoint(model, optimizer, epoch + 1, val_metrics, config, is_best=True)
            epochs_without_improvement = 0
            print(f"ğŸ† ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ ëª¨ë¸! Validation AUC: {best_val_auc:.4f}")
        else:
            epochs_without_improvement += 1
            print(f"â³ ê°œì„  ì—†ìŒ: {epochs_without_improvement}/{early_stopping_patience} epochs")
        
        # Early Stopping ì²´í¬
        if epochs_without_improvement >= early_stopping_patience:
            print(f"\nğŸ›‘ Early Stopping! {early_stopping_patience} epochs ë™ì•ˆ validation AUC ê°œì„  ì—†ìŒ")
            print(f"ğŸ† ìµœì¢… ë² ìŠ¤íŠ¸ Validation AUC: {best_val_auc:.4f}")
            break
        
        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % config.save_interval == 0:
            save_checkpoint(model, optimizer, epoch + 1, val_metrics, config, is_best=False)
    
    print(f"\n=== ì§„ì •í•œ SigLIP2 í›ˆë ¨ ì™„ë£Œ ===")
    print(f"ğŸ† ë² ìŠ¤íŠ¸ Validation AUC: {best_val_auc:.4f}")
    print(f"ğŸ’¾ ë² ìŠ¤íŠ¸ ëª¨ë¸: {best_model_path}")
    
    # wandb ì¢…ë£Œ
    wandb.finish()
    
    return model, best_model_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì§„ì •í•œ SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨")
    
    # ê¸°ë³¸ ì„¤ì • (ê¸°ì¡´ trainer.pyì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
    parser.add_argument("--data_dir", type=str, default="../../training_dset", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str, default="../modules/outputs/siglip-sam/true-siglip2", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="google/siglip2-base-patch16-naflex", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=32, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=100, help="ì—í¬í¬ ìˆ˜")
    
    # ì–¸ì–´ë³„ íŒŒì„œ ì„ íƒ ì˜µì…˜
    parser.add_argument("--parser", type=str, default="all", 
                       choices=["all", "English", "Greek", "Spanish", "Mandarin", "cross_lingual"],
                       help="ì‚¬ìš©í•  ì–¸ì–´ íŒŒì„œ")
    parser.add_argument("--languages", nargs="+", default=None, help="íŠ¹ì • ì–¸ì–´ ëª©ë¡")
    
    # Cross-lingual ëª¨ë“œ ì˜µì…˜
    parser.add_argument("--train_languages", nargs="+", default=["English", "Spanish", "Mandarin"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í›ˆë ¨ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    parser.add_argument("--test_languages", nargs="+", default=["Greek"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ì˜µì…˜
    parser.add_argument("--loss_type", type=str, default="focal",
                       choices=["cross_entropy", "focal", "bce"],
                       help="ì†ì‹¤ í•¨ìˆ˜ íƒ€ì…")
    parser.add_argument("--focal_alpha", type=float, default=1.0, help="Focal Loss alpha íŒŒë¼ë¯¸í„°")
    parser.add_argument("--focal_gamma", type=float, default=2.0, help="Focal Loss gamma íŒŒë¼ë¯¸í„°")
    parser.add_argument("--auto_class_weights", action="store_true", help="í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ë³´ì •")
    
    # ì˜µí‹°ë§ˆì´ì € ì„ íƒ ì˜µì…˜
    parser.add_argument("--optimizer_type", type=str, default="sam",
                       choices=["adamw", "sam"],
                       help="ì˜µí‹°ë§ˆì´ì € íƒ€ì…")
    parser.add_argument("--sam_rho", type=float, default=0.05, help="SAM rho íŒŒë¼ë¯¸í„°")
    parser.add_argument("--sam_adaptive", action="store_true", help="Adaptive SAM ì‚¬ìš©")
    
    # True SigLIP2 ì „ìš© ì˜µì…˜
    parser.add_argument("--ema_momentum", type=float, default=0.999, help="EMA Teacher momentum")
    parser.add_argument("--silc_weight", type=float, default=0.2, help="SILC/TIPS Loss ê°€ì¤‘ì¹˜")
    parser.add_argument("--sigmoid_weight", type=float, default=1.0, help="Sigmoid Loss ê°€ì¤‘ì¹˜")
    parser.add_argument("--loca_weight", type=float, default=1.0, help="LoCa Loss ê°€ì¤‘ì¹˜")
    parser.add_argument("--classification_weight", type=float, default=1.0, help="Classification Loss ê°€ì¤‘ì¹˜")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SigLIPSAMConfig()
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config.data_dir = args.data_dir
    if args.output_dir:
        config.output_dir = args.output_dir
        config.checkpoint_dir = f"{args.output_dir}/checkpoints"
    if args.model_name:
        config.model_name = args.model_name
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.num_epochs:
        config.num_epochs = args.num_epochs
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    if args.loss_type:
        config.loss_type = args.loss_type
    if args.focal_alpha:
        config.focal_alpha = args.focal_alpha
    if args.focal_gamma:
        config.focal_gamma = args.focal_gamma
    config.auto_class_weights = args.auto_class_weights
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    if args.optimizer_type:
        config.optimizer_type = args.optimizer_type
    if args.sam_rho:
        config.sam_rho = args.sam_rho
    config.sam_adaptive = args.sam_adaptive
    
    # True SigLIP2 ì„¤ì •
    config.ema_momentum = args.ema_momentum
    config.silc_weight = args.silc_weight
    config.sigmoid_weight = args.sigmoid_weight
    config.loca_weight = args.loca_weight
    config.classification_weight = args.classification_weight
    
    # ì–¸ì–´ íŒŒì„œ ì„¤ì • (ê¸°ì¡´ trainer.pyì™€ ë™ì¼)
    if args.parser == "cross_lingual":
        config.cross_lingual_mode = True
        config.train_languages = args.train_languages
        config.test_languages = args.test_languages
        
        train_langs_str = "_".join(args.train_languages)
        test_langs_str = "_".join(args.test_languages)
        config.output_dir = f"{config.output_dir}/CrossLingual_Train_{train_langs_str}_Test_{test_langs_str}"
        config.checkpoint_dir = f"{config.output_dir}/checkpoints"
        
        print("ğŸŒ Cross-Lingual ëª¨ë“œ í™œì„±í™”")
        print(f"  í›ˆë ¨ ì–¸ì–´: {args.train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {args.test_languages}")
        
        config.languages = args.train_languages + args.test_languages
        
    elif args.parser == "all":
        if args.languages:
            config.languages = args.languages
        else:
            config.languages = ["English", "Greek", "Spanish", "Mandarin"]
    else:
        config.languages = [args.parser]
    
    print(f"ì„ íƒëœ ì–¸ì–´: {config.languages}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
    print(f"ì˜µí‹°ë§ˆì´ì €: {config.optimizer_type}")
    print(f"ì†ì‹¤ í•¨ìˆ˜: {config.loss_type}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.output_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    
    # ëª¨ë¸ í›ˆë ¨
    model, best_model_path = train_model(config)

if __name__ == "__main__":
    main()
