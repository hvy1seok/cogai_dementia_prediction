"""
SigLIP2Ïö© Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨Í∏∞
Ïò§ÎîîÏò§Î•º Î©úÏä§ÌéôÌÜ†Í∑∏Îû®ÏúºÎ°ú Î≥ÄÌôòÌïòÍ≥† ÌÖçÏä§Ìä∏ÏôÄ Ìï®Íªò Ï≤òÎ¶¨
training_dset Ìè¥Îçî Íµ¨Ï°∞Ïóê ÎßûÏ∂∞ Ïñ∏Ïñ¥Î≥Ñ ÌååÏÑú ÏÇ¨Ïö©
"""
import os
import torch
import librosa
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
from transformers import AutoProcessor  # SigLIP2 ÏßÄÏõê
from torch.utils.data import Dataset, DataLoader
import soundfile as sf
from language_parsers import parse_all_languages, get_language_parser

class AudioToMelSpectrogram:
    """Ïò§ÎîîÏò§Î•º Î©úÏä§ÌéôÌÜ†Í∑∏Îû®ÏúºÎ°ú Î≥ÄÌôòÌïòÎäî ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 n_mels: int = 128,
                 n_fft: int = 2048,
                 hop_length: int = 512,
                 fmin: float = 0.0,
                 fmax: float = 8000.0,
                 image_size: int = 224):
        
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.fmin = fmin
        self.fmax = fmax
        self.image_size = image_size
    
    def audio_to_melspectrogram(self, audio_path: str) -> np.ndarray:
        """Ïò§ÎîîÏò§ ÌååÏùºÏùÑ Î©úÏä§ÌéôÌÜ†Í∑∏Îû®ÏúºÎ°ú Î≥ÄÌôò"""
        try:
            # Ïò§ÎîîÏò§ Î°úÎìú
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # Î©úÏä§ÌéôÌÜ†Í∑∏Îû® Í≥ÑÏÇ∞
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_mels=self.n_mels,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                fmin=self.fmin,
                fmax=self.fmax
            )
            
            # dB Ïä§ÏºÄÏùºÎ°ú Î≥ÄÌôò
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            return mel_spec_db
            
        except Exception as e:
            print(f"Ïò§ÎîîÏò§ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            # Îπà Î©úÏä§ÌéôÌÜ†Í∑∏Îû® Î∞òÌôò
            return np.zeros((self.n_mels, 100))
    
    def melspectrogram_to_image(self, mel_spec: np.ndarray) -> Image.Image:
        """Î©úÏä§ÌéôÌÜ†Í∑∏Îû®ÏùÑ PIL Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò"""
        # Ï†ïÍ∑úÌôî (0-255 Î≤îÏúÑÎ°ú)
        mel_spec_norm = ((mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) * 255).astype(np.uint8)
        
        # PIL Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò
        image = Image.fromarray(mel_spec_norm, mode='L')
        
        # RGBÎ°ú Î≥ÄÌôò (SigLIP2Îäî RGB Ïù¥ÎØ∏ÏßÄÎ•º ÏöîÍµ¨)
        image_rgb = image.convert('RGB')
        
        # ÌÅ¨Í∏∞ Ï°∞Ï†ï
        image_resized = image_rgb.resize((self.image_size, self.image_size), Image.Resampling.LANCZOS)
        
        return image_resized

class DementiaDataset(Dataset):
    """ÏπòÎß§ ÏßÑÎã®Ïö© Îç∞Ïù¥ÌÑ∞ÏÖã"""
    
    def __init__(self, 
                 data_dir: str,
                 processor: AutoProcessor,  # SigLIP2 ÏßÄÏõê
                 audio_processor: AudioToMelSpectrogram,
                 split: str = "train",
                 max_length: int = 512,
                 languages: Optional[List[str]] = None):
        
        self.data_dir = data_dir
        self.processor = processor
        self.audio_processor = audio_processor
        self.split = split
        self.max_length = max_length
        self.languages = languages or ["English", "Greek", "Spanish", "Mandarin"]
        
        # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        self.data = self._load_data()
        
    def _load_data(self) -> List[Dict]:
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ - Ïñ∏Ïñ¥Î≥Ñ ÌååÏÑú ÏÇ¨Ïö©"""
        print("Ïñ∏Ïñ¥Î≥Ñ ÌååÏÑúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ training_dset Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...")
        
        # Ïñ∏Ïñ¥Î≥Ñ ÌååÏÑúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ ÌååÏã±
        data = parse_all_languages(self.data_dir, self.languages)
        
        print(f"Ï¥ù {len(data)}Í∞ú ÏÉòÌîå Î°úÎìú ÏôÑÎ£å")
        
        # Ïñ∏Ïñ¥Î≥Ñ ÌÜµÍ≥Ñ Ï∂úÎ†•
        language_stats = {}
        for item in data:
            lang = item['language']
            language_stats[lang] = language_stats.get(lang, 0) + 1
        
        print("Ïñ∏Ïñ¥Î≥Ñ ÏÉòÌîå Ïàò:")
        for lang, count in language_stats.items():
            print(f"  {lang}: {count}Í∞ú")
        
        return data
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict:
        item = self.data[idx]
        
        # Ïò§ÎîîÏò§ ÌååÏùº Í≤ΩÎ°ú ÌôïÏù∏ Î∞è Ï≤òÎ¶¨
        audio_path = item['audio_path']
        
        # .npy ÌååÏùºÏù∏ Í≤ΩÏö∞ (Ï†ÑÏ≤òÎ¶¨Îêú Ïò§ÎîîÏò§ ÌäπÏßï) ÏßÅÏ†ë Î°úÎìú
        if audio_path.endswith('.npy'):
            try:
                audio_spec = np.load(audio_path)
                # Ïù¥ÎØ∏ Î©úÏä§ÌéôÌÜ†Í∑∏Îû® ÌòïÌÉúÎùºÎ©¥ Î∞îÎ°ú Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò
                if len(audio_spec.shape) == 2:
                    image = self.audio_processor.melspectrogram_to_image(audio_spec)
                else:
                    # 3Ï∞®ÏõêÏù∏ Í≤ΩÏö∞ (3, H, W) -> (H, W) Î≥ÄÌôò
                    if audio_spec.shape[0] == 3:
                        # RGB Ï±ÑÎÑê ÌèâÍ∑† ÎòêÎäî Ï≤´ Î≤àÏß∏ Ï±ÑÎÑê ÏÇ¨Ïö©
                        audio_spec_2d = np.mean(audio_spec, axis=0)
                    else:
                        audio_spec_2d = audio_spec[0] if len(audio_spec.shape) == 3 else audio_spec
                    image = self.audio_processor.melspectrogram_to_image(audio_spec_2d)
            except Exception as e:
                print(f"Ïò§ÎîîÏò§ Î°úÎìú Ïò§Î•ò {audio_path}: {e}")
                # Îπà Î©úÏä§ÌéôÌÜ†Í∑∏Îû®ÏúºÎ°ú ÎåÄÏ≤¥
                empty_spec = np.zeros((self.audio_processor.n_mels, 100))
                image = self.audio_processor.melspectrogram_to_image(empty_spec)
        else:
            # ÏùºÎ∞ò Ïò§ÎîîÏò§ ÌååÏùºÏù∏ Í≤ΩÏö∞ Î©úÏä§ÌéôÌÜ†Í∑∏Îû®ÏúºÎ°ú Î≥ÄÌôò
            mel_spec = self.audio_processor.audio_to_melspectrogram(audio_path)
            image = self.audio_processor.melspectrogram_to_image(mel_spec)
        
        # ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨ (Ïñ∏Ïñ¥ Î¨¥Í¥ÄÏùÑ ÏúÑÌï¥ ÏÜåÎ¨∏Ïûê Î≥ÄÌôò)
        text = item['text'].lower().strip()
        
        # SigLIP2 ÌîÑÎ°úÏÑ∏ÏÑúÎ°ú Ï≤òÎ¶¨
        inputs = self.processor(
            text=text,
            images=image,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )
        
        # Î∞∞Ïπò Ï∞®Ïõê Ï†úÍ±∞
        for key in inputs:
            if isinstance(inputs[key], torch.Tensor):
                inputs[key] = inputs[key].squeeze(0)
        
        # SigLIP2Îäî attention_maskÍ∞Ä ÏóÜÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú ÏÉùÏÑ±
        if 'attention_mask' not in inputs and 'input_ids' in inputs:
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
        
        # ÎîîÎ≤ÑÍπÖ: Ï≤´ Î≤àÏß∏ ÏïÑÏù¥ÌÖúÏóêÏÑú ÌÇ§ ÌôïÏù∏
        if not hasattr(self, '_debug_printed'):
            print(f"üîç SigLIP2 ÌîÑÎ°úÏÑ∏ÏÑú Ï∂úÎ†• ÌÇ§Îì§: {list(inputs.keys())}")
            self._debug_printed = True
        
        # ÎùºÎ≤® Ï∂îÍ∞Ä
        inputs['labels'] = torch.tensor(item['label'], dtype=torch.long)
        inputs['language'] = item['language']
        
        return inputs

def create_dataloaders(data_dir: str,
                      processor: AutoProcessor,  # SigLIP2 ÏßÄÏõê
                      config,
                      train_split: float = 0.8,
                      val_split: float = 0.1,
                      test_split: float = 0.1) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±"""
    
    audio_processor = AudioToMelSpectrogram(
        sample_rate=config.sample_rate,
        n_mels=config.n_mels,
        n_fft=config.n_fft,
        hop_length=config.hop_length,
        fmin=config.fmin,
        fmax=config.fmax,
        image_size=config.image_size
    )
    
    # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±
    full_dataset = DementiaDataset(
        data_dir=data_dir,
        processor=processor,
        audio_processor=audio_processor,
        max_length=config.max_length,
        languages=config.languages
    )
    
    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
    total_size = len(full_dataset)
    train_size = int(train_split * total_size)
    val_size = int(val_split * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(config.random_seed)
    )
    
    # Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    return train_loader, val_loader, test_loader 