"""
SigLIP2ìš© ë°ì´í„° ì²˜ë¦¬ê¸°
ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ê³  í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì²˜ë¦¬
training_dset í´ë” êµ¬ì¡°ì— ë§ì¶° ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©
"""
import os
import torch
import librosa
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
from transformers import AutoProcessor, AutoTokenizer  # SigLIP2 ì§€ì›
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from collections import Counter
import soundfile as sf
from language_parsers import parse_all_languages, get_language_parser
from sklearn.utils.class_weight import compute_class_weight

class AudioToMelSpectrogram:
    """ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 n_mels: int = 128,
                 n_fft: int = 2048,
                 hop_length: int = 512,
                 fmin: float = 0.0,
                 fmax: float = 8000.0,
                 image_size: int = 224):
        
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.fmin = fmin
        self.fmax = fmax
        self.image_size = image_size
    
    def audio_to_melspectrogram(self, audio_path: str) -> np.ndarray:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # ë©œìŠ¤í™í† ê·¸ë¨ ê³„ì‚°
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_mels=self.n_mels,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                fmin=self.fmin,
                fmax=self.fmax
            )
            
            # dB ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            return mel_spec_db
            
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ ë°˜í™˜
            return np.zeros((self.n_mels, 100))
    
    def melspectrogram_to_image(self, mel_spec: np.ndarray) -> Image.Image:
        """ë©œìŠ¤í™í† ê·¸ë¨ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        # ì •ê·œí™” (0-255 ë²”ìœ„ë¡œ)
        mel_spec_norm = ((mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) * 255).astype(np.uint8)
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        image = Image.fromarray(mel_spec_norm, mode='L')
        
        # RGBë¡œ ë³€í™˜ (SigLIP2ëŠ” RGB ì´ë¯¸ì§€ë¥¼ ìš”êµ¬)
        image_rgb = image.convert('RGB')
        
        # í¬ê¸° ì¡°ì •
        image_resized = image_rgb.resize((self.image_size, self.image_size), Image.Resampling.LANCZOS)
        
        return image_resized

class DementiaDataset(Dataset):
    """ì¹˜ë§¤ ì§„ë‹¨ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 data_dir: str,
                 processor: AutoProcessor,  # SigLIP2 ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ
                 tokenizer: AutoTokenizer,  # Gemma í† í¬ë‚˜ì´ì €
                 audio_processor: AudioToMelSpectrogram,
                 split: str = "train",
                 max_length: int = 512,
                 languages: Optional[List[str]] = None):
        
        self.data_dir = data_dir
        self.processor = processor
        self.tokenizer = tokenizer  # Gemma í† í¬ë‚˜ì´ì € ì¶”ê°€
        self.audio_processor = audio_processor
        self.split = split
        self.max_length = max_length
        self.languages = languages or ["English", "Greek", "Spanish", "Mandarin"]
        
        # ë°ì´í„° ë¡œë“œ
        self.data = self._load_data()
        
    def _load_data(self) -> List[Dict]:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ - ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©"""
        print("ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ training_dset ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íŒŒì‹±
        data = parse_all_languages(self.data_dir, self.languages)
        
        print(f"ì´ {len(data)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
        
        # ì–¸ì–´ë³„ í†µê³„ ì¶œë ¥
        language_stats = {}
        for item in data:
            lang = item['language']
            language_stats[lang] = language_stats.get(lang, 0) + 1
        
        print("ì–¸ì–´ë³„ ìƒ˜í”Œ ìˆ˜:")
        for lang, count in language_stats.items():
            print(f"  {lang}: {count}ê°œ")
        
        return data
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict:
        item = self.data[idx]
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ì²˜ë¦¬
        audio_path = item['audio_path']
        
        # .npy íŒŒì¼ì¸ ê²½ìš° (ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ íŠ¹ì§•) ì§ì ‘ ë¡œë“œ
        if audio_path.endswith('.npy'):
            try:
                audio_spec = np.load(audio_path)
                # ì´ë¯¸ ë©œìŠ¤í™í† ê·¸ë¨ í˜•íƒœë¼ë©´ ë°”ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                if len(audio_spec.shape) == 2:
                    image = self.audio_processor.melspectrogram_to_image(audio_spec)
                else:
                    # 3ì°¨ì›ì¸ ê²½ìš° (3, H, W) -> (H, W) ë³€í™˜
                    if audio_spec.shape[0] == 3:
                        # RGB ì±„ë„ í‰ê·  ë˜ëŠ” ì²« ë²ˆì§¸ ì±„ë„ ì‚¬ìš©
                        audio_spec_2d = np.mean(audio_spec, axis=0)
                    else:
                        audio_spec_2d = audio_spec[0] if len(audio_spec.shape) == 3 else audio_spec
                    image = self.audio_processor.melspectrogram_to_image(audio_spec_2d)
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ë¡œë“œ ì˜¤ë¥˜ {audio_path}: {e}")
                # ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ëŒ€ì²´
                empty_spec = np.zeros((self.audio_processor.n_mels, 100))
                image = self.audio_processor.melspectrogram_to_image(empty_spec)
        else:
            # ì¼ë°˜ ì˜¤ë””ì˜¤ íŒŒì¼ì¸ ê²½ìš° ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜
            mel_spec = self.audio_processor.audio_to_melspectrogram(audio_path)
            image = self.audio_processor.melspectrogram_to_image(mel_spec)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì–¸ì–´ ë¬´ê´€ì„ ìœ„í•´ ì†Œë¬¸ì ë³€í™˜)
        text = item['text'].lower().strip()
        
        # Gemma í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (256K vocab, multilingual)
        text_inputs = self.tokenizer(
            text,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )
        
        # SigLIP2 ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬
        image_inputs = self.processor(
            images=image,
            return_tensors="pt"
        )
        
        # ì…ë ¥ ê²°í•©
        inputs = {}
        
        # í…ìŠ¤íŠ¸ ì…ë ¥ (Gemma í† í¬ë‚˜ì´ì €)
        for key in text_inputs:
            if isinstance(text_inputs[key], torch.Tensor):
                inputs[key] = text_inputs[key].squeeze(0)
        
        # ì´ë¯¸ì§€ ì…ë ¥ (SigLIP2 í”„ë¡œì„¸ì„œ)
        for key in image_inputs:
            if isinstance(image_inputs[key], torch.Tensor):
                inputs[key] = image_inputs[key].squeeze(0)
        
        # attention_mask í™•ì¸ ë° ìƒì„±
        if 'attention_mask' not in inputs and 'input_ids' in inputs:
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
        
        # ë””ë²„ê¹… ì™„ë£Œ - ì œê±°ë¨
        
        # ë¼ë²¨ ì¶”ê°€
        inputs['labels'] = torch.tensor(item['label'], dtype=torch.long)
        inputs['language'] = item['language']
        
        # Patient ID ì¶”ê°€ (contrastive learningìš©)
        inputs['patient_id'] = item.get('patient_id', item['file_id'])
        
        return inputs

def create_stratified_split(dataset, train_split: float = 0.7, val_split: float = 0.1, test_split: float = 0.2, random_seed: int = 42):
    """
    í™˜ì ë‹¨ìœ„ + ì–¸ì–´ë³„ + ë¼ë²¨ë³„ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ stratified split ìˆ˜í–‰ (train:val:test = 7:1:2)
    Speaker-independent evaluationì„ ìœ„í•´ ë™ì¼ í™˜ìì˜ ëª¨ë“  ìƒ˜í”Œì´ í•œ ì„¸íŠ¸ì—ë§Œ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥
    """
    # ë°ì´í„°ì—ì„œ í™˜ì, ì–¸ì–´, ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
    patients = []
    languages = []
    labels = []
    
    for i in range(len(dataset)):
        item = dataset.data[i]  # DementiaDatasetì˜ data ì†ì„± ì ‘ê·¼
        patients.append(item.get('patient_id', item['file_id']))  # í™˜ì ID, ì—†ìœ¼ë©´ file_id ì‚¬ìš©
        languages.append(item['language'])
        labels.append(item['label'])
    
    # í™˜ìë³„ë¡œ ê·¸ë£¹í™”
    from collections import defaultdict
    patient_groups = defaultdict(list)
    for i, patient_id in enumerate(patients):
        patient_groups[patient_id].append(i)
    
    print(f"\nğŸ‘¥ í™˜ì ë‹¨ìœ„ ë¶„í• :")
    print(f"  ì „ì²´ í™˜ì ìˆ˜: {len(patient_groups)}")
    print(f"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
    
    # í™˜ìë³„ ë©”íƒ€ë°ì´í„° ìƒì„±
    patient_metadata = []
    for patient_id, indices in patient_groups.items():
        first_idx = indices[0]
        patient_lang = languages[first_idx]
        patient_label = labels[first_idx]
        sample_count = len(indices)
        
        patient_metadata.append({
            'patient_id': patient_id,
            'language': patient_lang,
            'label': patient_label,
            'indices': indices,
            'sample_count': sample_count
        })
    
    # í™˜ì ë‹¨ìœ„ë¡œ stratify í‚¤ ìƒì„± (ì–¸ì–´-ë¼ë²¨ ì¡°í•©)
    patient_stratify_keys = [f"{p['language']}_{p['label']}" for p in patient_metadata]
    patient_indices_list = list(range(len(patient_metadata)))
    
    # í™˜ì ë‹¨ìœ„ë¡œ ë¶„í•  ìˆ˜í–‰
    from sklearn.model_selection import train_test_split
    
    if test_split == 0.0:
        # Cross-lingual ëª¨ë“œ: train/valë§Œ ë¶„í•  (testëŠ” ë‹¤ë¥¸ ì–¸ì–´)
        if train_split == 0.0:
            # train_split=0ì¸ ê²½ìš°: ëª¨ë“  ë°ì´í„°ë¥¼ valë¡œ ì‚¬ìš© (Zero-shotì—ì„œ íƒ€ê²Ÿ ì–¸ì–´ìš©)
            val_patient_indices = patient_indices_list  # ëª¨ë“  í™˜ìë¥¼ valë¡œ ì‚¬ìš©
            train_patient_indices = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
        else:
            # ì¼ë°˜ì ì¸ train/val ë¶„í• 
            train_patient_indices, val_patient_indices = train_test_split(
                patient_indices_list,
                test_size=val_split / (train_split + val_split),  # val ë¹„ìœ¨ ì¡°ì •
                stratify=patient_stratify_keys,
                random_state=random_seed
            )
        test_patient_indices = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
    else:
        # ì¼ë°˜ ëª¨ë“œ: train/val/test 3-way ë¶„í• 
        # ì²« ë²ˆì§¸ ë¶„í• : train vs (val + test) - í™˜ì ë‹¨ìœ„
        train_patient_indices, temp_patient_indices = train_test_split(
            patient_indices_list,
            test_size=val_split + test_split,
            stratify=patient_stratify_keys,
            random_state=random_seed
        )
        
        # temp í™˜ìë“¤ì˜ stratify í‚¤ ìƒì„±
        temp_patient_stratify_keys = [patient_stratify_keys[i] for i in temp_patient_indices]
        
        # ë‘ ë²ˆì§¸ ë¶„í• : val vs test - í™˜ì ë‹¨ìœ„
        val_patient_indices, test_patient_indices = train_test_split(
            temp_patient_indices,
            test_size=test_split / (val_split + test_split),  # test ë¹„ìœ¨ ì¡°ì •
            stratify=temp_patient_stratify_keys,
            random_state=random_seed
        )
    
    # í™˜ì ì¸ë±ìŠ¤ë¥¼ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    train_indices = []
    val_indices = []
    test_indices = []
    
    for patient_idx in train_patient_indices:
        train_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in val_patient_indices:
        val_indices.extend(patient_metadata[patient_idx]['indices'])
    
    for patient_idx in test_patient_indices:
        test_indices.extend(patient_metadata[patient_idx]['indices'])
    
    # ë¶„í•  ê²°ê³¼ í†µê³„ ì¶œë ¥ (í™˜ì ë‹¨ìœ„ í¬í•¨)
    if test_split == 0.0:
        split_ratio = f"{train_split*100:.0f}:{val_split*100:.0f}"
        print(f"\nğŸ“Š í™˜ì ë‹¨ìœ„ Stratified Split ê²°ê³¼ ({split_ratio}):")
    else:
        split_ratio = f"{train_split*100:.0f}:{val_split*100:.0f}:{test_split*100:.0f}"
        print(f"\nğŸ“Š í™˜ì ë‹¨ìœ„ Stratified Split ê²°ê³¼ ({split_ratio}):")
    
    print(f"  ì „ì²´ ë°ì´í„°: {len(dataset)} ìƒ˜í”Œ, {len(patient_groups)} í™˜ì")
    print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_indices)} ìƒ˜í”Œ ({len(train_indices)/len(dataset)*100:.1f}%), {len(train_patient_indices)} í™˜ì")
    print(f"  ê²€ì¦ ë°ì´í„°: {len(val_indices)} ìƒ˜í”Œ ({len(val_indices)/len(dataset)*100:.1f}%), {len(val_patient_indices)} í™˜ì")
    
    if test_split > 0.0:
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_indices)} ìƒ˜í”Œ ({len(test_indices)/len(dataset)*100:.1f}%), {len(test_patient_indices)} í™˜ì")
    else:
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: Cross-lingual ëª¨ë“œ - ë³„ë„ ì–¸ì–´ë¡œ êµ¬ì„±")
    
    # í™˜ìë³„ ìƒ˜í”Œ ìˆ˜ ë¶„í¬
    train_samples_per_patient = [len(patient_metadata[i]['indices']) for i in train_patient_indices]
    val_samples_per_patient = [len(patient_metadata[i]['indices']) for i in val_patient_indices]
    test_samples_per_patient = [len(patient_metadata[i]['indices']) for i in test_patient_indices]
    
    print(f"\nğŸ“ˆ í™˜ìë³„ ìƒ˜í”Œ ìˆ˜:")
    if train_samples_per_patient:
        print(f"  í›ˆë ¨: í‰ê·  {sum(train_samples_per_patient)/len(train_samples_per_patient):.1f}ê°œ/í™˜ì")
    if val_samples_per_patient:
        print(f"  ê²€ì¦: í‰ê·  {sum(val_samples_per_patient)/len(val_samples_per_patient):.1f}ê°œ/í™˜ì")
    if test_samples_per_patient:
        print(f"  í…ŒìŠ¤íŠ¸: í‰ê·  {sum(test_samples_per_patient)/len(test_samples_per_patient):.1f}ê°œ/í™˜ì")
    
    # í™˜ì ë¶„í•  ê²€ì¦: ì¤‘ë³µ í™•ì¸
    train_patients = set([patient_metadata[i]['patient_id'] for i in train_patient_indices])
    val_patients = set([patient_metadata[i]['patient_id'] for i in val_patient_indices])
    
    if test_split > 0.0:
        test_patients = set([patient_metadata[i]['patient_id'] for i in test_patient_indices])
        overlap_train_test = train_patients & test_patients
        overlap_val_test = val_patients & test_patients
    else:
        test_patients = set()
        overlap_train_test = set()
        overlap_val_test = set()
    
    overlap_train_val = train_patients & val_patients
    
    if overlap_train_val or overlap_train_test or overlap_val_test:
        print(f"âš ï¸ í™˜ì ì¤‘ë³µ ë°œê²¬!")
        if overlap_train_val:
            print(f"   Train-Val ì¤‘ë³µ: {overlap_train_val}")
        if overlap_train_test:
            print(f"   Train-Test ì¤‘ë³µ: {overlap_train_test}")
        if overlap_val_test:
            print(f"   Val-Test ì¤‘ë³µ: {overlap_val_test}")
    else:
        if test_split > 0.0:
            print(f"âœ… í™˜ì ë‹¨ìœ„ ë¶„í•  ì„±ê³µ: ì¤‘ë³µ ì—†ìŒ")
        else:
            print(f"âœ… í™˜ì ë‹¨ìœ„ ë¶„í•  ì„±ê³µ (Train-Val): ì¤‘ë³µ ì—†ìŒ")
    
    # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì–¸ì–´ë³„ ë¶„í¬ í™•ì¸
    train_lang_dist = Counter([languages[i] for i in train_indices])
    val_lang_dist = Counter([languages[i] for i in val_indices])
    test_lang_dist = Counter([languages[i] for i in test_indices])
    train_label_dist = Counter([labels[i] for i in train_indices])
    val_label_dist = Counter([labels[i] for i in val_indices])
    test_label_dist = Counter([labels[i] for i in test_indices])
    
    print(f"\nğŸ“Š ì–¸ì–´ë³„ ë¶„í¬:")
    for lang in set(languages):
        train_count = train_lang_dist[lang]
        val_count = val_lang_dist[lang]
        test_count = test_lang_dist[lang]
        total_count = train_count + val_count + test_count
        if total_count > 0:
            if test_split > 0.0:
                print(f"  {lang}: í›ˆë ¨ {train_count}ê°œ ({train_count/total_count*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/total_count*100:.1f}%), "
                      f"í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_count/total_count*100:.1f}%)")
            else:
                print(f"  {lang}: í›ˆë ¨ {train_count}ê°œ ({train_count/(train_count+val_count)*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/(train_count+val_count)*100:.1f}%)")
    
    print(f"\nğŸ“Š ë¼ë²¨ë³„ ë¶„í¬:")
    label_names = {0: 'ì •ìƒ', 1: 'ì¹˜ë§¤'}
    for label in [0, 1]:
        train_count = train_label_dist[label]
        val_count = val_label_dist[label]
        test_count = test_label_dist[label]
        total_count = train_count + val_count + test_count
        if total_count > 0:
            if test_split > 0.0:
                print(f"  {label_names[label]}: í›ˆë ¨ {train_count}ê°œ ({train_count/total_count*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/total_count*100:.1f}%), "
                      f"í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_count/total_count*100:.1f}%)")
            else:
                print(f"  {label_names[label]}: í›ˆë ¨ {train_count}ê°œ ({train_count/(train_count+val_count)*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/(train_count+val_count)*100:.1f}%)")
    
    return train_indices, val_indices, test_indices

def create_sample_based_split(dataset, train_split: float = 0.7, val_split: float = 0.1, test_split: float = 0.2, random_seed: int = 42):
    """
    ìƒ˜í”Œ(íŒŒì¼) ë‹¨ìœ„ë¡œ stratified split ìˆ˜í–‰ (train:val:test = 7:1:2)
    í™˜ì ë‹¨ìœ„ê°€ ì•„ë‹Œ íŒŒì¼ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ë” ë§ì€ í•™ìŠµ ë°ì´í„° í™•ë³´
    âš ï¸ Speaker-dependent ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ (ë™ì¼ í™˜ìì˜ ìƒ˜í”Œì´ train/val/testì— ë¶„ì‚°)
    """
    # ë°ì´í„°ì—ì„œ ì–¸ì–´, ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
    languages = []
    labels = []
    
    for i in range(len(dataset)):
        item = dataset.data[i]
        languages.append(item['language'])
        labels.append(item['label'])
    
    print(f"\nğŸ“„ ìƒ˜í”Œ ë‹¨ìœ„ ë¶„í• :")
    print(f"  ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
    
    # ìƒ˜í”Œ ë‹¨ìœ„ë¡œ stratify í‚¤ ìƒì„± (ì–¸ì–´-ë¼ë²¨ ì¡°í•©)
    stratify_keys = [f"{lang}_{label}" for lang, label in zip(languages, labels)]
    sample_indices = list(range(len(dataset)))
    
    # ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ë¶„í•  ìˆ˜í–‰
    from sklearn.model_selection import train_test_split
    
    if test_split == 0.0:
        # Cross-lingual ëª¨ë“œ: train/valë§Œ ë¶„í•  (testëŠ” ë‹¤ë¥¸ ì–¸ì–´)
        if train_split == 0.0:
            # train_split=0ì¸ ê²½ìš°: ëª¨ë“  ë°ì´í„°ë¥¼ valë¡œ ì‚¬ìš© (Zero-shotì—ì„œ íƒ€ê²Ÿ ì–¸ì–´ìš©)
            val_indices = sample_indices  # ëª¨ë“  ìƒ˜í”Œì„ valë¡œ ì‚¬ìš©
            train_indices = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
        else:
            # ì¼ë°˜ì ì¸ train/val ë¶„í• 
            train_indices, val_indices = train_test_split(
                sample_indices,
                test_size=val_split / (train_split + val_split),  # val ë¹„ìœ¨ ì¡°ì •
                stratify=stratify_keys,
                random_state=random_seed
            )
        test_indices = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
    else:
        # ì¼ë°˜ ëª¨ë“œ: train/val/test 3-way ë¶„í• 
        # ì²« ë²ˆì§¸ ë¶„í• : train vs (val + test)
        train_indices, temp_indices = train_test_split(
            sample_indices,
            test_size=val_split + test_split,
            stratify=stratify_keys,
            random_state=random_seed
        )
        
        # temp ìƒ˜í”Œë“¤ì˜ stratify í‚¤ ìƒì„±
        temp_stratify_keys = [stratify_keys[i] for i in temp_indices]
        
        # ë‘ ë²ˆì§¸ ë¶„í• : val vs test
        val_indices, test_indices = train_test_split(
            temp_indices,
            test_size=test_split / (val_split + test_split),  # test ë¹„ìœ¨ ì¡°ì •
            stratify=temp_stratify_keys,
            random_state=random_seed
        )
    
    # í†µê³„ ê³„ì‚°
    from collections import Counter
    
    train_lang_dist = Counter([languages[i] for i in train_indices])
    val_lang_dist = Counter([languages[i] for i in val_indices])
    test_lang_dist = Counter([languages[i] for i in test_indices])
    
    train_label_dist = Counter([labels[i] for i in train_indices])
    val_label_dist = Counter([labels[i] for i in val_indices])
    test_label_dist = Counter([labels[i] for i in test_indices])
    
    print(f"\nğŸ“Š ìƒ˜í”Œ ë‹¨ìœ„ ë¶„í•  ê²°ê³¼:")
    print(f"  í›ˆë ¨: {len(train_indices)}ê°œ ìƒ˜í”Œ")
    print(f"  ê²€ì¦: {len(val_indices)}ê°œ ìƒ˜í”Œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ ìƒ˜í”Œ")
    
    print(f"\nğŸ“Š ì–¸ì–´ë³„ ë¶„í¬ (ìƒ˜í”Œ ë‹¨ìœ„):")
    for lang in set(languages):
        train_count = train_lang_dist[lang]
        val_count = val_lang_dist[lang]
        test_count = test_lang_dist[lang]
        total_count = train_count + val_count + test_count
        if total_count > 0:
            if test_split > 0.0:
                print(f"  {lang}: í›ˆë ¨ {train_count}ê°œ ({train_count/total_count*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/total_count*100:.1f}%), "
                      f"í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_count/total_count*100:.1f}%)")
            else:
                print(f"  {lang}: í›ˆë ¨ {train_count}ê°œ ({train_count/(train_count+val_count)*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/(train_count+val_count)*100:.1f}%)")
    
    print(f"\nğŸ“Š ë¼ë²¨ë³„ ë¶„í¬ (ìƒ˜í”Œ ë‹¨ìœ„):")
    label_names = {0: 'ì •ìƒ', 1: 'ì¹˜ë§¤'}
    for label in [0, 1]:
        train_count = train_label_dist[label]
        val_count = val_label_dist[label]
        test_count = test_label_dist[label]
        total_count = train_count + val_count + test_count
        if total_count > 0:
            if test_split > 0.0:
                print(f"  {label_names[label]}: í›ˆë ¨ {train_count}ê°œ ({train_count/total_count*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/total_count*100:.1f}%), "
                      f"í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_count/total_count*100:.1f}%)")
            else:
                print(f"  {label_names[label]}: í›ˆë ¨ {train_count}ê°œ ({train_count/(train_count+val_count)*100:.1f}%), "
                      f"ê²€ì¦ {val_count}ê°œ ({val_count/(train_count+val_count)*100:.1f}%)")
    
    print(f"\nâš ï¸ ìƒ˜í”Œ ë‹¨ìœ„ ë¶„í•  ì£¼ì˜ì‚¬í•­:")
    print(f"  - ë™ì¼ í™˜ìì˜ ìƒ˜í”Œì´ train/val/testì— ë¶„ì‚°ë  ìˆ˜ ìˆìŒ")
    print(f"  - Speaker-dependent ê²°ê³¼ë¡œ ì‹¤ì œ ì„ìƒ ì ìš©ì„±ì€ ì œí•œì ")
    print(f"  - ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥")
    
    return train_indices, val_indices, test_indices

def compute_class_weights(dataset, config):
    """í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚°"""
    if not config.auto_class_weights:
        return None
    
    # ëª¨ë“  ë¼ë²¨ ìˆ˜ì§‘
    labels = [item['label'] for item in dataset.data]
    unique_labels = np.unique(labels)
    
    # sklearnì„ ì‚¬ìš©í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (inverse frequency)
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=unique_labels,
        y=labels
    )
    
    # ë¼ë²¨ë³„ í†µê³„ ì¶œë ¥
    label_counts = Counter(labels)
    total_samples = len(labels)
    
    print(f"\nğŸ“Š í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„:")
    label_names = {0: 'ì •ìƒ', 1: 'ì¹˜ë§¤'}
    
    for i, (label, weight) in enumerate(zip(unique_labels, class_weights)):
        count = label_counts[label]
        percentage = count / total_samples * 100
        print(f"  {label_names[label]} (ë¼ë²¨ {label}): {count}ê°œ ({percentage:.1f}%) â†’ ê°€ì¤‘ì¹˜: {weight:.3f}")
    
    # ë¶ˆê· í˜• ì •ë„ ë¶„ì„
    dementia_count = label_counts[1]
    normal_count = label_counts[0]
    imbalance_ratio = max(dementia_count, normal_count) / min(dementia_count, normal_count)
    
    if imbalance_ratio > 1.5:
        more_class = 'ì¹˜ë§¤' if dementia_count > normal_count else 'ì •ìƒ'
        print(f"âš ï¸ {more_class} ë°ì´í„°ê°€ {imbalance_ratio:.1f}ë°° ë” ë§ìŒ â†’ ìë™ ê°€ì¤‘ì¹˜ ì ìš©")
    else:
        print(f"âœ… í´ë˜ìŠ¤ ë¶„í¬ê°€ ë¹„êµì  ê· ë“±í•¨ (ë¹„ìœ¨: {imbalance_ratio:.1f})")
    
    return class_weights

def create_dataloaders(data_dir: str,
                      processor: AutoProcessor,  # SigLIP2 ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ
                      tokenizer: AutoTokenizer,  # Gemma í† í¬ë‚˜ì´ì €
                      config,
                      cross_lingual_mode: bool = False,
                      train_languages: List[str] = None,
                      test_languages: List[str] = None) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """ë°ì´í„°ë¡œë” ìƒì„± (ì¼ë°˜ ëª¨ë“œ ë˜ëŠ” Cross-Lingual ëª¨ë“œ)"""
    
    audio_processor = AudioToMelSpectrogram(
        sample_rate=config.sample_rate,
        n_mels=config.n_mels,
        n_fft=config.n_fft,
        hop_length=config.hop_length,
        fmin=config.fmin,
        fmax=config.fmax,
        image_size=config.image_size
    )
    
    if cross_lingual_mode:
        print("ğŸŒ Cross-Lingual ëª¨ë“œ: ì–¸ì–´ë³„ë¡œ ë¶„ë¦¬ëœ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±")
        print(f"  í›ˆë ¨ ì–¸ì–´: {train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {test_languages}")
        
        # í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± (train + val)
        train_full_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            tokenizer=tokenizer,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=train_languages
        )
        
        # í›ˆë ¨ ë°ì´í„°ë¥¼ train:val = 7:1ë¡œ ë¶„í• 
        train_indices, val_indices, _ = create_stratified_split(
            train_full_dataset,
            train_split=0.875,  # 7/(7+1) = 0.875
            val_split=0.125,    # 1/(7+1) = 0.125
            test_split=0.0,     # Cross-lingualì—ì„œëŠ” testëŠ” ë‹¤ë¥¸ ì–¸ì–´
            random_seed=config.random_seed
        )
        
        train_dataset = Subset(train_full_dataset, train_indices)
        val_dataset = Subset(train_full_dataset, val_indices)
        
        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±
        test_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            tokenizer=tokenizer,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=test_languages
        )
        
        print(f"ğŸ“Š Cross-Lingual ë°ì´í„° ë¶„í• :")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {train_languages})")
        print(f"  ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {train_languages})")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)} ìƒ˜í”Œ (ì–¸ì–´: {test_languages})")
        
    else:
        print("ğŸ¯ ì¼ë°˜ ëª¨ë“œ: Stratified Split ìˆ˜í–‰ ì¤‘...")
        
        # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
        full_dataset = DementiaDataset(
            data_dir=data_dir,
            processor=processor,
            tokenizer=tokenizer,
            audio_processor=audio_processor,
            max_length=config.max_length,
            languages=config.languages
        )
        
        # ë¶„í•  ë°©ì‹ì— ë”°ë¥¸ ë°ì´í„° ë¶„í• 
        if config.split_by_patient:
            print("ğŸ‘¥ í™˜ì ë‹¨ìœ„ Stratified Split ìˆ˜í–‰ ì¤‘...")
            train_indices, val_indices, test_indices = create_stratified_split(
                full_dataset, 
                train_split=config.train_split,
                val_split=config.val_split,
                test_split=config.test_split,
                random_seed=config.random_seed
            )
        else:
            print("ğŸ“„ ìƒ˜í”Œ ë‹¨ìœ„ Stratified Split ìˆ˜í–‰ ì¤‘...")
            train_indices, val_indices, test_indices = create_sample_based_split(
                full_dataset, 
                train_split=config.train_split,
                val_split=config.val_split,
                test_split=config.test_split,
                random_seed=config.random_seed
            )
        
        # Subsetìœ¼ë¡œ ë°ì´í„°ì…‹ ë¶„í• 
        train_dataset = Subset(full_dataset, train_indices)
        val_dataset = Subset(full_dataset, val_indices)
        test_dataset = Subset(full_dataset, test_indices)
        
        print(f"ğŸ“Š ì¼ë°˜ ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ ({config.train_split*100:.0f}%)")
        print(f"  ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ìƒ˜í”Œ ({config.val_split*100:.0f}%)")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)} ìƒ˜í”Œ ({config.test_split*100:.0f}%)")
        print(f"  ì „ì²´ ë°ì´í„°: {len(full_dataset)} ìƒ˜í”Œ")
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    return train_loader, val_loader, test_loader 