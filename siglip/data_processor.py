"""
SigLIP2ìš© ë°ì´í„° ì²˜ë¦¬ê¸°
ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ê³  í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì²˜ë¦¬
training_dset í´ë” êµ¬ì¡°ì— ë§ì¶° ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©
"""
import os
import torch
import librosa
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
from transformers import AutoProcessor  # SigLIP2 ì§€ì›
from torch.utils.data import Dataset, DataLoader
import soundfile as sf
from language_parsers import parse_all_languages, get_language_parser

class AudioToMelSpectrogram:
    """ì˜¤ë””ì˜¤ë¥¼ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 n_mels: int = 128,
                 n_fft: int = 2048,
                 hop_length: int = 512,
                 fmin: float = 0.0,
                 fmax: float = 8000.0,
                 image_size: int = 224):
        
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.fmin = fmin
        self.fmax = fmax
        self.image_size = image_size
    
    def audio_to_melspectrogram(self, audio_path: str) -> np.ndarray:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # ë©œìŠ¤í™í† ê·¸ë¨ ê³„ì‚°
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_mels=self.n_mels,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                fmin=self.fmin,
                fmax=self.fmax
            )
            
            # dB ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            return mel_spec_db
            
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ ë°˜í™˜
            return np.zeros((self.n_mels, 100))
    
    def melspectrogram_to_image(self, mel_spec: np.ndarray) -> Image.Image:
        """ë©œìŠ¤í™í† ê·¸ë¨ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        # ì •ê·œí™” (0-255 ë²”ìœ„ë¡œ)
        mel_spec_norm = ((mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) * 255).astype(np.uint8)
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        image = Image.fromarray(mel_spec_norm, mode='L')
        
        # RGBë¡œ ë³€í™˜ (SigLIP2ëŠ” RGB ì´ë¯¸ì§€ë¥¼ ìš”êµ¬)
        image_rgb = image.convert('RGB')
        
        # í¬ê¸° ì¡°ì •
        image_resized = image_rgb.resize((self.image_size, self.image_size), Image.Resampling.LANCZOS)
        
        return image_resized

class DementiaDataset(Dataset):
    """ì¹˜ë§¤ ì§„ë‹¨ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 data_dir: str,
                 processor: AutoProcessor,  # SigLIP2 ì§€ì›
                 audio_processor: AudioToMelSpectrogram,
                 split: str = "train",
                 max_length: int = 512,
                 languages: Optional[List[str]] = None):
        
        self.data_dir = data_dir
        self.processor = processor
        self.audio_processor = audio_processor
        self.split = split
        self.max_length = max_length
        self.languages = languages or ["English", "Greek", "Spanish", "Mandarin"]
        
        # ë°ì´í„° ë¡œë“œ
        self.data = self._load_data()
        
    def _load_data(self) -> List[Dict]:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ - ì–¸ì–´ë³„ íŒŒì„œ ì‚¬ìš©"""
        print("ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ training_dset ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì–¸ì–´ë³„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íŒŒì‹±
        data = parse_all_languages(self.data_dir, self.languages)
        
        print(f"ì´ {len(data)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
        
        # ì–¸ì–´ë³„ í†µê³„ ì¶œë ¥
        language_stats = {}
        for item in data:
            lang = item['language']
            language_stats[lang] = language_stats.get(lang, 0) + 1
        
        print("ì–¸ì–´ë³„ ìƒ˜í”Œ ìˆ˜:")
        for lang, count in language_stats.items():
            print(f"  {lang}: {count}ê°œ")
        
        return data
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict:
        item = self.data[idx]
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ì²˜ë¦¬
        audio_path = item['audio_path']
        
        # .npy íŒŒì¼ì¸ ê²½ìš° (ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ íŠ¹ì§•) ì§ì ‘ ë¡œë“œ
        if audio_path.endswith('.npy'):
            try:
                audio_spec = np.load(audio_path)
                # ì´ë¯¸ ë©œìŠ¤í™í† ê·¸ë¨ í˜•íƒœë¼ë©´ ë°”ë¡œ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                if len(audio_spec.shape) == 2:
                    image = self.audio_processor.melspectrogram_to_image(audio_spec)
                else:
                    # 3ì°¨ì›ì¸ ê²½ìš° (3, H, W) -> (H, W) ë³€í™˜
                    if audio_spec.shape[0] == 3:
                        # RGB ì±„ë„ í‰ê·  ë˜ëŠ” ì²« ë²ˆì§¸ ì±„ë„ ì‚¬ìš©
                        audio_spec_2d = np.mean(audio_spec, axis=0)
                    else:
                        audio_spec_2d = audio_spec[0] if len(audio_spec.shape) == 3 else audio_spec
                    image = self.audio_processor.melspectrogram_to_image(audio_spec_2d)
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ë¡œë“œ ì˜¤ë¥˜ {audio_path}: {e}")
                # ë¹ˆ ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ëŒ€ì²´
                empty_spec = np.zeros((self.audio_processor.n_mels, 100))
                image = self.audio_processor.melspectrogram_to_image(empty_spec)
        else:
            # ì¼ë°˜ ì˜¤ë””ì˜¤ íŒŒì¼ì¸ ê²½ìš° ë©œìŠ¤í™í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜
            mel_spec = self.audio_processor.audio_to_melspectrogram(audio_path)
            image = self.audio_processor.melspectrogram_to_image(mel_spec)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì–¸ì–´ ë¬´ê´€ì„ ìœ„í•´ ì†Œë¬¸ì ë³€í™˜)
        text = item['text'].lower().strip()
        
        # SigLIP2 í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬
        inputs = self.processor(
            text=text,
            images=image,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )
        
        # ë°°ì¹˜ ì°¨ì› ì œê±°
        for key in inputs:
            if isinstance(inputs[key], torch.Tensor):
                inputs[key] = inputs[key].squeeze(0)
        
        # SigLIP2ëŠ” attention_maskê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒì„±
        if 'attention_mask' not in inputs and 'input_ids' in inputs:
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
        
        # ë””ë²„ê¹… ì™„ë£Œ - ì œê±°ë¨
        
        # ë¼ë²¨ ì¶”ê°€
        inputs['labels'] = torch.tensor(item['label'], dtype=torch.long)
        inputs['language'] = item['language']
        
        return inputs

def create_dataloaders(data_dir: str,
                      processor: AutoProcessor,  # SigLIP2 ì§€ì›
                      config,
                      train_split: float = 0.8,
                      test_split: float = 0.2) -> Tuple[DataLoader, DataLoader]:
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    
    audio_processor = AudioToMelSpectrogram(
        sample_rate=config.sample_rate,
        n_mels=config.n_mels,
        n_fft=config.n_fft,
        hop_length=config.hop_length,
        fmin=config.fmin,
        fmax=config.fmax,
        image_size=config.image_size
    )
    
    # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
    full_dataset = DementiaDataset(
        data_dir=data_dir,
        processor=processor,
        audio_processor=audio_processor,
        max_length=config.max_length,
        languages=config.languages
    )
    
    # ë°ì´í„° ë¶„í•  (Train:Test = 8:2)
    total_size = len(full_dataset)
    train_size = int(train_split * total_size)
    test_size = total_size - train_size
    
    train_dataset, test_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, test_size],
        generator=torch.Generator().manual_seed(config.random_seed)
    )
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    print(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"  í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ ({train_split*100:.0f}%)")
    print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)} ìƒ˜í”Œ ({test_split*100:.0f}%)")
    print(f"  ì „ì²´ ë°ì´í„°: {total_size} ìƒ˜í”Œ")
    
    return train_loader, test_loader 