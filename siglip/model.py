"""
SigLIP2 Í∏∞Î∞ò ÏπòÎß§ ÏßÑÎã® Î™®Îç∏ (PyTorch Lightning)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoConfig
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
import wandb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import numpy as np
from typing import Dict, List, Optional

class SigLIP2DementiaClassifier(pl.LightningModule):
    """SigLIP2 Í∏∞Î∞ò ÏπòÎß§ ÏßÑÎã® Î∂ÑÎ•òÍ∏∞"""
    
    def __init__(self, 
                 model_name: str = "google/siglip2-base-patch16-224",
                 num_classes: int = 2,
                 learning_rate: float = 2e-5,
                 weight_decay: float = 0.01,
                 warmup_steps: int = 100,
                 max_epochs: int = 10,
                 use_language_embedding: bool = True):
        
        super().__init__()
        self.save_hyperparameters()
        
        # SigLIP2 Î™®Îç∏ Î°úÎìú - Ïó¨Îü¨ Î∞©Î≤ï ÏãúÎèÑ
        model_loaded = False
        
        # Î∞©Î≤ï 1: ÏßÅÏ†ë SigLIP2 Î°úÎìú
        try:
            from transformers import Siglip2Model, Siglip2Config
            print("üîÑ SigLIP2 Î™®Îç∏ ÏßÅÏ†ë Î°úÎìú ÏãúÎèÑ...")
            self.siglip2 = Siglip2Model.from_pretrained(model_name)
            print("‚úÖ SigLIP2 Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ!")
            model_loaded = True
        except Exception as e:
            print(f"‚ö†Ô∏è Î∞©Î≤ï 1 Ïã§Ìå®: {e}")
        
        # Î∞©Î≤ï 2: ConfigÎßå Í∞ÄÏ†∏ÏôÄÏÑú ÏÉà SigLIP2 Î™®Îç∏ ÏÉùÏÑ±
        if not model_loaded:
            try:
                from transformers import Siglip2Model, Siglip2Config
                print("üîÑ SigLIP2 ConfigÎ°ú ÏÉà Î™®Îç∏ ÏÉùÏÑ± ÏãúÎèÑ...")
                config = Siglip2Config()  # Í∏∞Î≥∏ ÏÑ§Ï†ï ÏÇ¨Ïö©
                self.siglip2 = Siglip2Model(config)
                print("‚úÖ ÏÉà SigLIP2 Î™®Îç∏ ÏÉùÏÑ± ÏÑ±Í≥µ! (ÏÇ¨Ï†ÑÌõàÎ†® Í∞ÄÏ§ëÏπò ÏóÜÏùå)")
                model_loaded = True
            except Exception as e:
                print(f"‚ö†Ô∏è Î∞©Î≤ï 2 Ïã§Ìå®: {e}")
        
        # Î∞©Î≤ï 3: AutoModel Ìè¥Î∞±
        if not model_loaded:
            print("üîÑ AutoModel Ìè¥Î∞± ÏÇ¨Ïö©...")
            self.siglip2 = AutoModel.from_pretrained(model_name)
            print(f"Î°úÎìúÎêú Î™®Îç∏ ÌÉÄÏûÖ: {type(self.siglip2)}")
            print(f"Î™®Îç∏ ÏÑ§Ï†ï: {self.siglip2.config.model_type if hasattr(self.siglip2.config, 'model_type') else 'Unknown'}")
        
        # Ïñ∏Ïñ¥ ÏûÑÎ≤†Îî© (Ïñ∏Ïñ¥ Î¨¥Í¥Ä ÌïôÏäµÏùÑ ÏúÑÌï¥)
        if use_language_embedding:
            self.language_embedding = nn.Embedding(10, 768)  # ÏµúÎåÄ 10Í∞ú Ïñ∏Ïñ¥ ÏßÄÏõê
        else:
            self.language_embedding = None
        
        # Î∂ÑÎ•ò Ìó§Îìú
        hidden_size = self.siglip2.config.projection_dim
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_classes)
        )
        
        # Ïñ∏Ïñ¥ ID Îß§Ìïë
        self.language_to_id = {
            'English': 0, 'Greek': 1, 'Korean': 2, 'Spanish': 3, 'French': 4,
            'German': 5, 'Italian': 6, 'Portuguese': 7, 'Japanese': 8, 'Chinese': 9
        }
        
        # Î©îÌä∏Î¶≠ Ï¥àÍ∏∞Ìôî
        self.train_accuracy = pl.metrics.Accuracy()
        self.val_accuracy = pl.metrics.Accuracy()
        self.test_accuracy = pl.metrics.Accuracy()
        
    def forward(self, input_ids, attention_mask, pixel_values, language_ids=None):
        """ÏàúÏ†ÑÌåå"""
        # SigLIP2 Î™®Îç∏ ÌÜµÍ≥º
        outputs = self.siglip2(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values
        )
        
        # Î©ÄÌã∞Î™®Îã¨ ÏûÑÎ≤†Îî© Ï∂îÏ∂ú
        multimodal_embeddings = outputs.logits_per_image  # [batch_size, hidden_size]
        
        # Ïñ∏Ïñ¥ ÏûÑÎ≤†Îî© Ï∂îÍ∞Ä (ÏÑ†ÌÉùÏ†Å)
        if self.language_embedding is not None and language_ids is not None:
            lang_embeddings = self.language_embedding(language_ids)
            multimodal_embeddings = multimodal_embeddings + lang_embeddings
        
        # Î∂ÑÎ•ò
        logits = self.classifier(multimodal_embeddings)
        
        return logits
    
    def training_step(self, batch, batch_idx):
        """ÌõàÎ†® Ïä§ÌÖù"""
        # Ïñ∏Ïñ¥ ID Î≥ÄÌôò
        language_ids = self._get_language_ids(batch['language'])
        
        # ÏàúÏ†ÑÌåå
        logits = self(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            pixel_values=batch['pixel_values'],
            language_ids=language_ids
        )
        
        # ÏÜêÏã§ Í≥ÑÏÇ∞
        loss = F.cross_entropy(logits, batch['labels'])
        
        # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
        acc = self.train_accuracy(logits.softmax(dim=-1), batch['labels'])
        
        # Î°úÍπÖ
        self.log('train_loss', loss, prog_bar=True)
        self.log('train_acc', acc, prog_bar=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        """Í≤ÄÏ¶ù Ïä§ÌÖù"""
        # Ïñ∏Ïñ¥ ID Î≥ÄÌôò
        language_ids = self._get_language_ids(batch['language'])
        
        # ÏàúÏ†ÑÌåå
        logits = self(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            pixel_values=batch['pixel_values'],
            language_ids=language_ids
        )
        
        # ÏÜêÏã§ Í≥ÑÏÇ∞
        loss = F.cross_entropy(logits, batch['labels'])
        
        # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
        acc = self.val_accuracy(logits.softmax(dim=-1), batch['labels'])
        
        # ÏòàÏ∏°Í∞í Ï†ÄÏû• (ÎÇòÏ§ëÏóê Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞Ïö©)
        self.validation_step_outputs.append({
            'logits': logits,
            'labels': batch['labels'],
            'loss': loss
        })
        
        # Î°úÍπÖ
        self.log('val_loss', loss, prog_bar=True)
        self.log('val_acc', acc, prog_bar=True)
        
        return loss
    
    def test_step(self, batch, batch_idx):
        """ÌÖåÏä§Ìä∏ Ïä§ÌÖù"""
        # Ïñ∏Ïñ¥ ID Î≥ÄÌôò
        language_ids = self._get_language_ids(batch['language'])
        
        # ÏàúÏ†ÑÌåå
        logits = self(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            pixel_values=batch['pixel_values'],
            language_ids=language_ids
        )
        
        # ÏÜêÏã§ Í≥ÑÏÇ∞
        loss = F.cross_entropy(logits, batch['labels'])
        
        # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
        acc = self.test_accuracy(logits.softmax(dim=-1), batch['labels'])
        
        # ÏòàÏ∏°Í∞í Ï†ÄÏû•
        self.test_step_outputs.append({
            'logits': logits,
            'labels': batch['labels'],
            'loss': loss
        })
        
        # Î°úÍπÖ
        self.log('test_loss', loss, prog_bar=True)
        self.log('test_acc', acc, prog_bar=True)
        
        return loss
    
    def on_validation_epoch_start(self):
        """Í≤ÄÏ¶ù ÏóêÌè¨ÌÅ¨ ÏãúÏûë Ïãú"""
        self.validation_step_outputs = []
    
    def on_validation_epoch_end(self):
        """Í≤ÄÏ¶ù ÏóêÌè¨ÌÅ¨ Ï¢ÖÎ£å Ïãú"""
        self._compute_validation_metrics()
    
    def on_test_epoch_start(self):
        """ÌÖåÏä§Ìä∏ ÏóêÌè¨ÌÅ¨ ÏãúÏûë Ïãú"""
        self.test_step_outputs = []
    
    def on_test_epoch_end(self):
        """ÌÖåÏä§Ìä∏ ÏóêÌè¨ÌÅ¨ Ï¢ÖÎ£å Ïãú"""
        self._compute_test_metrics()
    
    def _get_language_ids(self, languages: List[str]) -> torch.Tensor:
        """Ïñ∏Ïñ¥Î•º IDÎ°ú Î≥ÄÌôò"""
        if self.language_embedding is None:
            return None
        
        language_ids = []
        for lang in languages:
            lang_id = self.language_to_id.get(lang, 0)  # Í∏∞Î≥∏Í∞íÏùÄ English
            language_ids.append(lang_id)
        
        return torch.tensor(language_ids, device=self.device)
    
    def _compute_validation_metrics(self):
        """Í≤ÄÏ¶ù Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""
        all_logits = torch.cat([x['logits'] for x in self.validation_step_outputs])
        all_labels = torch.cat([x['labels'] for x in self.validation_step_outputs])
        
        # ÏòàÏ∏°Í∞í
        probs = F.softmax(all_logits, dim=-1)
        preds = torch.argmax(all_logits, dim=-1)
        
        # Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        accuracy = accuracy_score(all_labels.cpu(), preds.cpu())
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels.cpu(), preds.cpu(), average='weighted'
        )
        
        # ROC AUC (Ïù¥ÏßÑ Î∂ÑÎ•òÏù∏ Í≤ΩÏö∞)
        if all_logits.shape[1] == 2:
            auc = roc_auc_score(all_labels.cpu(), probs[:, 1].cpu())
        else:
            auc = 0.0
        
        # Î°úÍπÖ
        self.log('val_accuracy_final', accuracy)
        self.log('val_precision', precision)
        self.log('val_recall', recall)
        self.log('val_f1', f1)
        self.log('val_auc', auc)
        
        # wandbÏóê ÏÉÅÏÑ∏ Î©îÌä∏Î¶≠ Î°úÍπÖ
        if self.logger:
            self.logger.experiment.log({
                'val/accuracy': accuracy,
                'val/precision': precision,
                'val/recall': recall,
                'val/f1': f1,
                'val/auc': auc
            })
    
    def _compute_test_metrics(self):
        """ÌÖåÏä§Ìä∏ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""
        all_logits = torch.cat([x['logits'] for x in self.test_step_outputs])
        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])
        
        # ÏòàÏ∏°Í∞í
        probs = F.softmax(all_logits, dim=-1)
        preds = torch.argmax(all_logits, dim=-1)
        
        # Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        accuracy = accuracy_score(all_labels.cpu(), preds.cpu())
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels.cpu(), preds.cpu(), average='weighted'
        )
        
        # ROC AUC (Ïù¥ÏßÑ Î∂ÑÎ•òÏù∏ Í≤ΩÏö∞)
        if all_logits.shape[1] == 2:
            auc = roc_auc_score(all_labels.cpu(), probs[:, 1].cpu())
        else:
            auc = 0.0
        
        # Î°úÍπÖ
        self.log('test_accuracy_final', accuracy)
        self.log('test_precision', precision)
        self.log('test_recall', recall)
        self.log('test_f1', f1)
        self.log('test_auc', auc)
        
        # wandbÏóê ÏÉÅÏÑ∏ Î©îÌä∏Î¶≠ Î°úÍπÖ
        if self.logger:
            self.logger.experiment.log({
                'test/accuracy': accuracy,
                'test/precision': precision,
                'test/recall': recall,
                'test/f1': f1,
                'test/auc': auc
            })
    
    def configure_optimizers(self):
        """ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÑ§Ï†ï"""
        # Í∞ÄÏ§ëÏπò Í∞êÏá†Î•º Ï†ÅÏö©ÌïòÏßÄ ÏïäÏùÑ ÌååÎùºÎØ∏ÌÑ∞Îì§
        no_decay = ['bias', 'LayerNorm.weight']
        
        # ÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î£π ÏÉùÏÑ±
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': self.hparams.weight_decay,
            },
            {
                'params': [p for n, p in self.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0,
            },
        ]
        
        optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters,
            lr=self.hparams.learning_rate,
            weight_decay=self.hparams.weight_decay
        )
        
        # ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎü¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.hparams.max_epochs
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val_loss',
            }
        }

def create_model(config) -> SigLIP2DementiaClassifier:
    """Î™®Îç∏ ÏÉùÏÑ±"""
    return SigLIP2DementiaClassifier(
        model_name=config.model_name,
        num_classes=2,  # ÏπòÎß§ Ïó¨Î∂Ä (0: Ï†ïÏÉÅ, 1: ÏπòÎß§)
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        warmup_steps=config.warmup_steps,
        max_epochs=config.num_epochs,
        use_language_embedding=True  # Ïñ∏Ïñ¥ Î¨¥Í¥Ä ÌïôÏäµÏùÑ ÏúÑÌï¥ ÌôúÏÑ±Ìôî
    ) 