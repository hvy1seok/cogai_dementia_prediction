"""
SigLIP2 ê¸°ë°˜ ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ (PyTorch Lightning)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoConfig
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
import wandb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import numpy as np
from typing import Dict, List, Optional
from torchmetrics import Accuracy

# Lion Optimizer ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from lion_pytorch import Lion
    LION_AVAILABLE = True
    print("ğŸ¦ lion-pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    LION_AVAILABLE = False
    print("âš ï¸ lion-pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install lion-pytorchë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# SAM Optimizer êµ¬í˜„ (ì°¸ì¡°: https://github.com/davda54/sam)
class SAM(torch.optim.Optimizer):
    """SAM: Sharpness-Aware Minimization"""
    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):
        assert rho >= 0.0, f"Invalid rho, should be non-negative: {rho}"
        
        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)
        super(SAM, self).__init__(params, defaults)
        
        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)
        self.param_groups = self.base_optimizer.param_groups
        self.defaults.update(self.base_optimizer.defaults)
    
    @torch.no_grad()
    def first_step(self, zero_grad=False):
        grad_norm = self._grad_norm()
        for group in self.param_groups:
            scale = group["rho"] / (grad_norm + 1e-12)
            
            for p in group["params"]:
                if p.grad is None: continue
                self.state[p]["old_p"] = p.data.clone()
                e_w = (torch.pow(p, 2) if group["adaptive"] else 1.0) * p.grad * scale.to(p)
                p.add_(e_w)  # climb to the local maximum "w + e(w)"
        
        if zero_grad: self.zero_grad()
    
    @torch.no_grad()
    def second_step(self, zero_grad=False):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None: continue
                p.data = self.state[p]["old_p"]  # get back to "w" from "w + e(w)"
        
        self.base_optimizer.step()  # do the actual "sharpness-aware" update
        
        if zero_grad: self.zero_grad()
    
    @torch.no_grad()
    def step(self, closure=None):
        assert closure is not None, "Sharpness Aware Minimization requires closure, but it was not provided"
        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass
        
        self.first_step(zero_grad=True)
        closure()
        self.second_step()
    
    def _grad_norm(self):
        shared_device = self.param_groups[0]["params"][0].device  # put everything on the same device, in case of model parallelism
        norm = torch.norm(
                    torch.stack([
                        ((torch.abs(p) if group["adaptive"] else 1.0) * p.grad).norm(dtype=torch.float32)
                        for group in self.param_groups for p in group["params"]
                        if p.grad is not None
                    ]),
                    dtype=torch.float32
                )
        return norm.to(shared_device)
    
    def load_state_dict(self, state_dict):
        super().load_state_dict(state_dict)
        self.base_optimizer.param_groups = self.param_groups

class FocalLoss(nn.Module):
    """
    Focal Loss êµ¬í˜„ - ë¶ˆê· í˜• ë°ì´í„°ì…‹ì— íš¨ê³¼ì 
    alphaê°€ ë¦¬ìŠ¤íŠ¸/í…ì„œì¸ ê²½ìš° í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
    """
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        if isinstance(alpha, (list, tuple)):
            # í…ì„œë¥¼ ëª¨ë“ˆ ë²„í¼ë¡œ ë“±ë¡í•˜ì—¬ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
            self.register_buffer('alpha', torch.tensor(alpha, dtype=torch.float32))
        else:
            self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        
        # í´ë˜ìŠ¤ë³„ alpha ê°€ì¤‘ì¹˜ ì ìš©
        if hasattr(self, 'alpha') and isinstance(self.alpha, torch.Tensor):
            # alphaê°€ í…ì„œì¸ ê²½ìš° í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš© (ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ìˆìŒ)
            alpha_t = self.alpha.gather(0, targets.long())
            focal_loss = alpha_t * (1-pt)**self.gamma * ce_loss
        else:
            # alphaê°€ ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹
            focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class SigLIPDementiaClassifier(pl.LightningModule):
    """
    SigLIP2 ê¸°ë°˜ ë‹¤êµ­ì–´ ì¹˜ë§¤ ì§„ë‹¨ ë¶„ë¥˜ê¸°
    - Base: SigLIP2 (google/siglip2-base-patch16-naflex)
    - Native: Multilingual vision-language understanding
    """
    
    def __init__(self, 
                 model_name: str = "google/siglip2-base-patch16-naflex",
                 num_classes: int = 2,
                 learning_rate: float = 2e-5,
                 weight_decay: float = 0.01,
                 warmup_steps: int = 100,
                 max_epochs: int = 10,
                 use_language_embedding: bool = True,
                 loss_type: str = "cross_entropy",  # "cross_entropy", "focal", "bce"
                 focal_alpha: float = 1.0,
                 focal_gamma: float = 2.0,
                 optimizer_type: str = "adamw",  # "adamw", "lion", "sam"
                 sam_rho: float = 0.05):
        
        super().__init__()
        self.save_hyperparameters()
        
        # SigLIP2 ëª¨ë¸ ë¡œë“œ (ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        print("ğŸ”„ SigLIP2 ëª¨ë¸ ë¡œë“œ ì‹œë„...")
        self.siglip_model = AutoModel.from_pretrained(model_name)
        print(f"âœ… SigLIP2 ëª¨ë¸ ë¡œë“œ ì„±ê³µ! íƒ€ì…: {type(self.siglip_model)}")
        print(f"ğŸ“Š ëª¨ë¸ í¬ê¸°: {self.siglip_model.config.vision_config.hidden_size if hasattr(self.siglip_model.config, 'vision_config') else 'ì•Œ ìˆ˜ ì—†ìŒ'}")
        
        # SigLIP2ëŠ” ë„¤ì´í‹°ë¸Œ ë‹¤êµ­ì–´ ì§€ì› - ì¶”ê°€ ì–¸ì–´ ì„ë² ë”© ì„ íƒì  ì‚¬ìš©
        if use_language_embedding:
            # ì„ íƒì  ì–¸ì–´ë³„ fine-tuningì„ ìœ„í•œ ì„ë² ë”©
            self.language_embedding = nn.Embedding(10, 512)  # SigLIP2 í¬ê¸°ì— ë§ì¶¤
            self.language_projection = nn.Linear(512, 768)
        else:
            self.language_embedding = None
            self.language_projection = None
        
        # ë¶„ë¥˜ í—¤ë“œ - SigLIP2ì˜ hidden_sizeëŠ” configì—ì„œ ë¯¸ë¦¬ ì•Œ ìˆ˜ ìˆìŒ
        # SigLIP2 ëª¨ë¸ì˜ configì—ì„œ hidden_size ì¶”ì¶œ
        if hasattr(self.siglip_model.config, 'hidden_size'):
            actual_hidden_size = self.siglip_model.config.hidden_size
        elif hasattr(self.siglip_model.config, 'vision_config') and hasattr(self.siglip_model.config.vision_config, 'hidden_size'):
            actual_hidden_size = self.siglip_model.config.vision_config.hidden_size
        else:
            # í´ë°±: ì¼ë°˜ì ì¸ SigLIP2 hidden_size
            actual_hidden_size = 768
        
        print(f"ğŸ“ Hidden size: {actual_hidden_size}")
        
        # ë¶„ë¥˜ê¸° ë¯¸ë¦¬ ìƒì„± (ë™ì  ìƒì„± ë¬¸ì œ í•´ê²°)
        self.classifier = nn.Sequential(
            nn.Linear(actual_hidden_size, actual_hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(actual_hidden_size // 2, self.hparams.num_classes)
        )
        
        self.hidden_size_detected = True
        self.actual_hidden_size = actual_hidden_size
        
        # ì–¸ì–´ ID ë§¤í•‘
        self.language_to_id = {
            'English': 0, 'Greek': 1, 'Korean': 2, 'Spanish': 3, 'French': 4,
            'German': 5, 'Italian': 6, 'Portuguese': 7, 'Japanese': 8, 'Chinese': 9
        }
        
        # ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‚˜ì¤‘ì— í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ì´ˆê¸°í™”
        self.loss_type = loss_type
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        self.criterion = None  # ë‚˜ì¤‘ì— ì„¤ì •
        
        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        self.train_accuracy = Accuracy(task='multiclass', num_classes=num_classes)
        self.val_accuracy = Accuracy(task='multiclass', num_classes=num_classes)
        self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)
    
    def setup_loss_function(self, class_weights=None):
        """ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™” - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©"""
        if self.loss_type == "focal":
            if class_weights is not None:
                # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìë™ ì ìš©
                alpha = class_weights
                print(f"ğŸ¯ Focal Loss ì‚¬ìš©: alpha={alpha} (ìë™ ê³„ì‚°), gamma={self.focal_gamma}")
                print(f"   ì •ìƒ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {alpha[0]:.3f}, ì¹˜ë§¤ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {alpha[1]:.3f}")
            else:
                # ìˆ˜ë™ ì„¤ì • ë˜ëŠ” ê· ë“± ê°€ì¤‘ì¹˜
                alpha = self.focal_alpha
                print(f"ğŸ¯ Focal Loss ì‚¬ìš©: alpha={alpha} (ìˆ˜ë™ ì„¤ì •), gamma={self.focal_gamma}")
            
            self.criterion = FocalLoss(alpha=alpha, gamma=self.focal_gamma)
        elif self.loss_type == "bce":
            self.criterion = nn.BCEWithLogitsLoss()
            print("âš–ï¸ BCE Loss ì‚¬ìš©")
        else:
            self.criterion = nn.CrossEntropyLoss()
            print("ğŸ“Š Cross Entropy Loss ì‚¬ìš©")
        
    def forward(self, input_ids, attention_mask=None, pixel_values=None, pixel_attention_mask=None, spatial_shapes=None, language_ids=None):
        """ìˆœì „íŒŒ - SigLIP2 ë„¤ì´í‹°ë¸Œ ë‹¤êµ­ì–´ ì§€ì›"""
        # SigLIP2 ëª¨ë¸ í†µê³¼ (ëª¨ë“  í•„ìš”í•œ ì…ë ¥ í¬í•¨)
        model_inputs = {
            'input_ids': input_ids,
            'pixel_values': pixel_values
        }
        if attention_mask is not None:
            model_inputs['attention_mask'] = attention_mask
        if pixel_attention_mask is not None:
            model_inputs['pixel_attention_mask'] = pixel_attention_mask
        if spatial_shapes is not None:
            model_inputs['spatial_shapes'] = spatial_shapes
            
        outputs = self.siglip_model(**model_inputs)
        
        # SigLIP2ì˜ ë©€í‹°ëª¨ë‹¬ íŠ¹ì§• ì¶”ì¶œ (ê³ ì • ì°¨ì› ì‚¬ìš©)
        if hasattr(outputs, 'image_embeds') and hasattr(outputs, 'text_embeds'):
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”© ê²°í•© (ê³ ì • ì°¨ì›!)
            multimodal_embeddings = (outputs.image_embeds + outputs.text_embeds) / 2
        elif hasattr(outputs, 'pooler_output'):
            multimodal_embeddings = outputs.pooler_output
        else:
            # í´ë°±: ë§ˆì§€ë§‰ íˆë“  ìƒíƒœì˜ í‰ê· 
            multimodal_embeddings = outputs.last_hidden_state.mean(dim=1)
        
        # logits_per_imageëŠ” ê°€ë³€ ì°¨ì›ì´ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!
        
        # ì°¨ì› ê²€ì¦ (ë””ë²„ê¹…ìš©)
        expected_size = self.actual_hidden_size
        actual_size = multimodal_embeddings.shape[-1]
        if actual_size != expected_size:
            print(f"âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_size}, ì‹¤ì œ {actual_size}")
            # ì°¨ì› ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš° ì²˜ë¦¬
            if actual_size > expected_size:
                multimodal_embeddings = multimodal_embeddings[:, :expected_size]
            elif actual_size < expected_size:
                # íŒ¨ë”© ë˜ëŠ” projection í•„ìš”
                padding = torch.zeros(multimodal_embeddings.shape[0], expected_size - actual_size, 
                                    device=multimodal_embeddings.device)
                multimodal_embeddings = torch.cat([multimodal_embeddings, padding], dim=1)
        
        # ì–¸ì–´ ì„ë² ë”©ì€ SigLIP2 ë„¤ì´í‹°ë¸Œ ë‹¤êµ­ì–´ ëŠ¥ë ¥ìœ¼ë¡œ ëŒ€ì²´
        
        # ë¶„ë¥˜
        logits = self.classifier(multimodal_embeddings)
        return logits
    
    def training_step(self, batch, batch_idx):
        """í›ˆë ¨ ìŠ¤í…"""
        # ì–¸ì–´ ID ë³€í™˜
        language_ids = self._get_language_ids(batch['language'])
        
        # ì•ˆì „í•œ ì…ë ¥ ì¤€ë¹„
        input_ids = batch['input_ids']
        pixel_values = batch['pixel_values']
        attention_mask = batch.get('attention_mask', None)
        pixel_attention_mask = batch.get('pixel_attention_mask', None)
        spatial_shapes = batch.get('spatial_shapes', None)
        
        # ìˆœì „íŒŒ
        logits = self(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            pixel_attention_mask=pixel_attention_mask,
            spatial_shapes=spatial_shapes,
            language_ids=language_ids
        )
        
        # ì†ì‹¤ ê³„ì‚°
        if self.hparams.loss_type == "bce":
            # BCEëŠ” ì´ì§„ ë¶„ë¥˜ìš©ì´ë¯€ë¡œ ë¼ë²¨ì„ floatìœ¼ë¡œ ë³€í™˜í•˜ê³  ë¡œì§“ì˜ ë‘ ë²ˆì§¸ í´ë˜ìŠ¤ë§Œ ì‚¬ìš©
            labels_bce = batch['labels'].float()
            logits_bce = logits[:, 1]  # ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥ ë§Œ ì‚¬ìš©
            loss = self.criterion(logits_bce, labels_bce)
        else:
            loss = self.criterion(logits, batch['labels'])
        
        # ì •í™•ë„ ê³„ì‚°
        acc = self.train_accuracy(logits.softmax(dim=-1), batch['labels'])
        
        # ë¡œê¹…
        self.log('train_loss', loss, prog_bar=True, batch_size=batch['input_ids'].size(0))
        self.log('train_acc', acc, prog_bar=True, batch_size=batch['input_ids'].size(0))
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        """ê²€ì¦ ìŠ¤í…"""
        # ì–¸ì–´ ID ë³€í™˜
        language_ids = self._get_language_ids(batch['language'])
        
        # ì•ˆì „í•œ ì…ë ¥ ì¤€ë¹„
        input_ids = batch['input_ids']
        pixel_values = batch['pixel_values']
        attention_mask = batch.get('attention_mask', None)
        pixel_attention_mask = batch.get('pixel_attention_mask', None)
        spatial_shapes = batch.get('spatial_shapes', None)
        
        # ìˆœì „íŒŒ
        logits = self(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            pixel_attention_mask=pixel_attention_mask,
            spatial_shapes=spatial_shapes,
            language_ids=language_ids
        )
        
        # ì†ì‹¤ ê³„ì‚°
        if self.hparams.loss_type == "bce":
            # BCEëŠ” ì´ì§„ ë¶„ë¥˜ìš©ì´ë¯€ë¡œ ë¼ë²¨ì„ floatìœ¼ë¡œ ë³€í™˜í•˜ê³  ë¡œì§“ì˜ ë‘ ë²ˆì§¸ í´ë˜ìŠ¤ë§Œ ì‚¬ìš©
            labels_bce = batch['labels'].float()
            logits_bce = logits[:, 1]  # ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥ ë§Œ ì‚¬ìš©
            loss = self.criterion(logits_bce, labels_bce)
        else:
            loss = self.criterion(logits, batch['labels'])
        
        # ì •í™•ë„ ê³„ì‚°
        acc = self.val_accuracy(logits.softmax(dim=-1), batch['labels'])
        
        # ì˜ˆì¸¡ê°’ ì €ì¥ (ì–¸ì–´ ì •ë³´ í¬í•¨)
        self.validation_step_outputs.append({
            'logits': logits,
            'labels': batch['labels'],
            'languages': batch['language'],  # ì–¸ì–´ë³„ ë¶„ì„ìš©
            'loss': loss
        })
        
        # ë¡œê¹…
        self.log('val_loss', loss, prog_bar=True, batch_size=batch['input_ids'].size(0))
        self.log('val_acc', acc, prog_bar=True, batch_size=batch['input_ids'].size(0))
        
        return loss
    
    def test_step(self, batch, batch_idx):
        """í…ŒìŠ¤íŠ¸ ìŠ¤í…"""
        # ì–¸ì–´ ID ë³€í™˜
        language_ids = self._get_language_ids(batch['language'])
        
        # ì•ˆì „í•œ ì…ë ¥ ì¤€ë¹„
        input_ids = batch['input_ids']
        pixel_values = batch['pixel_values']
        attention_mask = batch.get('attention_mask', None)
        pixel_attention_mask = batch.get('pixel_attention_mask', None)
        spatial_shapes = batch.get('spatial_shapes', None)
        
        # ìˆœì „íŒŒ
        logits = self(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values,
            pixel_attention_mask=pixel_attention_mask,
            spatial_shapes=spatial_shapes,
            language_ids=language_ids
        )
        
        # ì†ì‹¤ ê³„ì‚°
        if self.hparams.loss_type == "bce":
            # BCEëŠ” ì´ì§„ ë¶„ë¥˜ìš©ì´ë¯€ë¡œ ë¼ë²¨ì„ floatìœ¼ë¡œ ë³€í™˜í•˜ê³  ë¡œì§“ì˜ ë‘ ë²ˆì§¸ í´ë˜ìŠ¤ë§Œ ì‚¬ìš©
            labels_bce = batch['labels'].float()
            logits_bce = logits[:, 1]  # ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥ ë§Œ ì‚¬ìš©
            loss = self.criterion(logits_bce, labels_bce)
        else:
            loss = self.criterion(logits, batch['labels'])
        
        # ì •í™•ë„ ê³„ì‚°
        acc = self.test_accuracy(logits.softmax(dim=-1), batch['labels'])
        
        # AUC ê³„ì‚° (ë°°ì¹˜ë³„)
        probs = F.softmax(logits, dim=-1)
        if logits.shape[1] == 2 and len(torch.unique(batch['labels'])) > 1:
            try:
                batch_auc = roc_auc_score(batch['labels'].cpu(), probs[:, 1].cpu())
                self.log('test_auc', batch_auc, prog_bar=True, sync_dist=True)
            except ValueError:
                # ë°°ì¹˜ì— í•œ í´ë˜ìŠ¤ë§Œ ìˆëŠ” ê²½ìš° AUC ê³„ì‚° ë¶ˆê°€
                pass
        
        # ì˜ˆì¸¡ê°’ ì €ì¥ (ì–¸ì–´ ì •ë³´ í¬í•¨)
        self.test_step_outputs.append({
            'logits': logits,
            'labels': batch['labels'],
            'languages': batch['language'],  # ì–¸ì–´ë³„ ë¶„ì„ìš©
            'loss': loss
        })
        
        # ë¡œê¹…
        self.log('test_loss', loss, prog_bar=True, batch_size=batch['input_ids'].size(0))
        self.log('test_acc', acc, prog_bar=True, batch_size=batch['input_ids'].size(0))
        
        return loss
    
    def on_validation_epoch_start(self):
        """ê²€ì¦ ì—í¬í¬ ì‹œì‘ ì‹œ"""
        self.validation_step_outputs = []
    
    def on_validation_epoch_end(self):
        """ê²€ì¦ ì—í¬í¬ ì¢…ë£Œ ì‹œ"""
        self._compute_validation_metrics()
    
    def on_test_epoch_start(self):
        """í…ŒìŠ¤íŠ¸ ì—í¬í¬ ì‹œì‘ ì‹œ"""
        self.test_step_outputs = []
    
    def on_test_epoch_end(self):
        """í…ŒìŠ¤íŠ¸ ì—í¬í¬ ì¢…ë£Œ ì‹œ"""
        self._compute_test_metrics()
    
    def _get_language_ids(self, languages: List[str]) -> torch.Tensor:
        """ì–¸ì–´ë¥¼ IDë¡œ ë³€í™˜"""
        if self.language_embedding is None:
            return None
        
        language_ids = []
        for lang in languages:
            lang_id = self.language_to_id.get(lang, 0)  # ê¸°ë³¸ê°’ì€ English
            language_ids.append(lang_id)
        
        return torch.tensor(language_ids, device=self.device)
    
    def _compute_validation_metrics(self):
        """ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚° - ìµœì  threshold ê¸°ë°˜"""
        all_logits = torch.cat([x['logits'] for x in self.validation_step_outputs])
        all_labels = torch.cat([x['labels'] for x in self.validation_step_outputs])
        
        # ì–¸ì–´ ì •ë³´ ìˆ˜ì§‘
        all_languages = []
        for x in self.validation_step_outputs:
            if isinstance(x['languages'], list):
                all_languages.extend(x['languages'])
            else:
                # ë‹¨ì¼ ë°°ì¹˜ì˜ ê²½ìš°
                all_languages.extend([x['languages']] * len(x['labels']))
        
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
        probs = F.softmax(all_logits, dim=-1)
        
        if all_logits.shape[1] == 2:
            # ì´ì§„ ë¶„ë¥˜: ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥  ì‚¬ìš©
            y_scores = probs[:, 1].cpu().numpy()
            y_true = all_labels.cpu().numpy()
            
            # ROC AUC ê³„ì‚°
            auc = roc_auc_score(y_true, y_scores)
            
            # ìµœì  threshold ì°¾ê¸° (Youden's J statistic)
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(y_true, y_scores)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
            
            # ìµœì  thresholdë¡œ ì˜ˆì¸¡
            optimal_preds = (y_scores >= optimal_threshold).astype(int)
            
            # ê¸°ë³¸ threshold (0.5)ë¡œë„ ì˜ˆì¸¡
            default_preds = (y_scores >= 0.5).astype(int)
            
            # ìµœì  threshold ê¸°ë°˜ ë©”íŠ¸ë¦­
            optimal_accuracy = accuracy_score(y_true, optimal_preds)
            optimal_precision, optimal_recall, optimal_f1, _ = precision_recall_fscore_support(
                y_true, optimal_preds, average='weighted', zero_division=0
            )
            
            # ê¸°ë³¸ threshold ê¸°ë°˜ ë©”íŠ¸ë¦­ (ë¹„êµìš©)
            default_accuracy = accuracy_score(y_true, default_preds)
            default_precision, default_recall, default_f1, _ = precision_recall_fscore_support(
                y_true, default_preds, average='weighted', zero_division=0
            )
            
        else:
            # ë‹¤ì¤‘ ë¶„ë¥˜
            auc = 0.0
            optimal_threshold = 0.5
            optimal_preds = torch.argmax(all_logits, dim=-1).cpu().numpy()
            default_preds = optimal_preds
            y_true = all_labels.cpu().numpy()
            
            optimal_accuracy = accuracy_score(y_true, optimal_preds)
            optimal_precision, optimal_recall, optimal_f1, _ = precision_recall_fscore_support(
                y_true, optimal_preds, average='weighted', zero_division=0
            )
            default_accuracy = optimal_accuracy
            default_precision, default_recall, default_f1 = optimal_precision, optimal_recall, optimal_f1
        
        # ë¡œê¹… - ìµœì  threshold ê¸°ë°˜ (ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒìš©)
        batch_size = len(y_true)
        self.log('val_accuracy', optimal_accuracy, batch_size=batch_size)
        self.log('val_precision', optimal_precision, batch_size=batch_size)
        self.log('val_recall', optimal_recall, batch_size=batch_size)
        self.log('val_f1', optimal_f1, batch_size=batch_size)
        self.log('val_auc', auc, batch_size=batch_size)  # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€
        self.log('val_optimal_threshold', optimal_threshold, batch_size=batch_size)
        
        # ì¶”ê°€ ë¡œê¹… - ë¹„êµìš©
        self.log('val_accuracy_default', default_accuracy, batch_size=batch_size)
        
        # wandbì— ìƒì„¸ ë©”íŠ¸ë¦­ ë¡œê¹…
        if self.logger:
            self.logger.experiment.log({
                # ìµœì  threshold ê¸°ë°˜ (ë©”ì¸ ì§€í‘œ)
                'val/accuracy_optimal': optimal_accuracy,
                'val/precision_optimal': optimal_precision,
                'val/recall_optimal': optimal_recall,
                'val/f1_optimal': optimal_f1,
                'val/auc': auc,
                'val/optimal_threshold': optimal_threshold,
                
                # ë¹„êµ ì§€í‘œ
                'val/accuracy_default_0.5': default_accuracy,
                'val/precision_default_0.5': default_precision,
                'val/recall_default_0.5': default_recall,
                'val/f1_default_0.5': default_f1,
            })
    
    def _compute_test_metrics(self):
        """í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚° - ìµœì  threshold ê¸°ë°˜ + ì–¸ì–´ë³„ ë¶„ì„"""
        all_logits = torch.cat([x['logits'] for x in self.test_step_outputs])
        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])
        
        # ì–¸ì–´ ì •ë³´ ìˆ˜ì§‘
        all_languages = []
        for x in self.test_step_outputs:
            if isinstance(x['languages'], list):
                all_languages.extend(x['languages'])
            else:
                # ë‹¨ì¼ ë°°ì¹˜ì˜ ê²½ìš°
                all_languages.extend([x['languages']] * len(x['labels']))
        
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
        probs = F.softmax(all_logits, dim=-1)
        
        if all_logits.shape[1] == 2:
            # ì´ì§„ ë¶„ë¥˜: ì¹˜ë§¤ í´ë˜ìŠ¤ í™•ë¥  ì‚¬ìš©
            y_scores = probs[:, 1].cpu().numpy()
            y_true = all_labels.cpu().numpy()
            
            # ROC AUC ê³„ì‚°
            auc = roc_auc_score(y_true, y_scores)
            
            # ìµœì  threshold ì°¾ê¸° (Youden's J statistic)
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(y_true, y_scores)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
            
            # ìµœì  thresholdë¡œ ì˜ˆì¸¡
            optimal_preds = (y_scores >= optimal_threshold).astype(int)
            
            # ê¸°ë³¸ threshold (0.5)ë¡œë„ ì˜ˆì¸¡
            default_preds = (y_scores >= 0.5).astype(int)
            
            # ìµœì  threshold ê¸°ë°˜ ë©”íŠ¸ë¦­
            optimal_accuracy = accuracy_score(y_true, optimal_preds)
            optimal_precision, optimal_recall, optimal_f1, _ = precision_recall_fscore_support(
                y_true, optimal_preds, average='weighted', zero_division=0
            )
            
            # ê¸°ë³¸ threshold ê¸°ë°˜ ë©”íŠ¸ë¦­ (ë¹„êµìš©)
            default_accuracy = accuracy_score(y_true, default_preds)
            default_precision, default_recall, default_f1, _ = precision_recall_fscore_support(
                y_true, default_preds, average='weighted', zero_division=0
            )
            
            # argmax ê¸°ë°˜ ë©”íŠ¸ë¦­ (ê¸°ì¡´ ë°©ì‹)
            argmax_preds = torch.argmax(all_logits, dim=-1).cpu().numpy()
            argmax_accuracy = accuracy_score(y_true, argmax_preds)
            argmax_precision, argmax_recall, argmax_f1, _ = precision_recall_fscore_support(
                y_true, argmax_preds, average='weighted', zero_division=0
            )
            
        else:
            # ë‹¤ì¤‘ ë¶„ë¥˜
            auc = 0.0
            optimal_threshold = 0.5
            y_scores = probs.max(dim=-1)[0].cpu().numpy()
            optimal_preds = torch.argmax(all_logits, dim=-1).cpu().numpy()
            default_preds = optimal_preds
            argmax_preds = optimal_preds
            y_true = all_labels.cpu().numpy()
            
            optimal_accuracy = accuracy_score(y_true, optimal_preds)
            optimal_precision, optimal_recall, optimal_f1, _ = precision_recall_fscore_support(
                y_true, optimal_preds, average='weighted', zero_division=0
            )
            default_accuracy = optimal_accuracy
            default_precision, default_recall, default_f1 = optimal_precision, optimal_recall, optimal_f1
            argmax_accuracy = optimal_accuracy
            argmax_precision, argmax_recall, argmax_f1 = optimal_precision, optimal_recall, optimal_f1
        
        # ë¡œê¹… - ìµœì  threshold ê¸°ë°˜ (ë©”ì¸)
        batch_size = len(y_true)
        self.log('test_accuracy', optimal_accuracy, batch_size=batch_size)
        self.log('test_precision', optimal_precision, batch_size=batch_size)
        self.log('test_recall', optimal_recall, batch_size=batch_size)
        self.log('test_f1', optimal_f1, batch_size=batch_size)
        self.log('test_auc', auc, batch_size=batch_size)
        self.log('test_optimal_threshold', optimal_threshold, batch_size=batch_size)
        
        # ì¶”ê°€ ë¡œê¹… - ë¹„êµìš©
        self.log('test_accuracy_default', default_accuracy, batch_size=batch_size)
        self.log('test_accuracy_argmax', argmax_accuracy, batch_size=batch_size)
        
        # wandbì— ìƒì„¸ ë©”íŠ¸ë¦­ ë¡œê¹…
        if self.logger:
            self.logger.experiment.log({
                # ìµœì  threshold ê¸°ë°˜ (ë©”ì¸ ì§€í‘œ)
                'test/accuracy_optimal': optimal_accuracy,
                'test/precision_optimal': optimal_precision,
                'test/recall_optimal': optimal_recall,
                'test/f1_optimal': optimal_f1,
                'test/auc': auc,
                'test/optimal_threshold': optimal_threshold,
                
                # ë¹„êµ ì§€í‘œë“¤
                'test/accuracy_default_0.5': default_accuracy,
                'test/precision_default_0.5': default_precision,
                'test/recall_default_0.5': default_recall,
                'test/f1_default_0.5': default_f1,
                
                'test/accuracy_argmax': argmax_accuracy,
                'test/precision_argmax': argmax_precision,
                'test/recall_argmax': argmax_recall,
                'test/f1_argmax': argmax_f1,
            })
        
        # ì „ì²´ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ìµœì  threshold = {optimal_threshold:.3f}):")
        print(f"   AUC: {auc:.4f}")
        print(f"   Accuracy: {optimal_accuracy:.4f}")
        print(f"   Precision: {optimal_precision:.4f}")
        print(f"   Recall: {optimal_recall:.4f}")
        print(f"   F1: {optimal_f1:.4f}")
        
        print(f"\nğŸ“Š Threshold ë¹„êµ:")
        print(f"   ìµœì  threshold ({optimal_threshold:.3f}): Acc={optimal_accuracy:.4f}")
        print(f"   ê¸°ë³¸ threshold (0.500): Acc={default_accuracy:.4f}")
        print(f"   Argmax ë°©ì‹: Acc={argmax_accuracy:.4f}")
        
        # ì–¸ì–´ë³„ ê²°ê³¼ ê³„ì‚° ë° ì¶œë ¥
        self._compute_language_specific_metrics(y_scores, y_true, all_languages, optimal_threshold)
    
    def _compute_language_specific_metrics(self, y_scores, y_true, all_languages, optimal_threshold):
        """ì–¸ì–´ë³„ í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì¶œë ¥"""
        from collections import defaultdict
        import numpy as np
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
        
        # ì–¸ì–´ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
        language_data = defaultdict(lambda: {'scores': [], 'labels': [], 'indices': []})
        
        for i, (score, label, lang) in enumerate(zip(y_scores, y_true, all_languages)):
            language_data[lang]['scores'].append(score)
            language_data[lang]['labels'].append(label)
            language_data[lang]['indices'].append(i)
        
        print(f"\nğŸŒ ì–¸ì–´ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"{'='*80}")
        
        # wandb ë¡œê¹…ìš© ì–¸ì–´ë³„ ë©”íŠ¸ë¦­
        language_metrics = {}
        
        for lang in sorted(language_data.keys()):
            lang_scores = np.array(language_data[lang]['scores'])
            lang_labels = np.array(language_data[lang]['labels'])
            
            if len(lang_scores) == 0:
                continue
                
            # ì–¸ì–´ë³„ AUC ê³„ì‚°
            try:
                lang_auc = roc_auc_score(lang_labels, lang_scores)
            except ValueError:
                lang_auc = 0.0
            
            # ìµœì  thresholdë¡œ ì˜ˆì¸¡
            lang_optimal_preds = (lang_scores >= optimal_threshold).astype(int)
            
            # ê¸°ë³¸ threshold (0.5)ë¡œ ì˜ˆì¸¡
            lang_default_preds = (lang_scores >= 0.5).astype(int)
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            lang_optimal_acc = accuracy_score(lang_labels, lang_optimal_preds)
            lang_default_acc = accuracy_score(lang_labels, lang_default_preds)
            
            lang_precision, lang_recall, lang_f1, _ = precision_recall_fscore_support(
                lang_labels, lang_optimal_preds, average='weighted', zero_division=0
            )
            
            # í´ë˜ìŠ¤ë³„ ë¶„í¬
            from collections import Counter
            label_dist = Counter(lang_labels)
            normal_count = label_dist[0]
            dementia_count = label_dist[1]
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š {lang} ({len(lang_scores)}ê°œ ìƒ˜í”Œ)")
            print(f"   ì •ìƒ: {normal_count}ê°œ, ì¹˜ë§¤: {dementia_count}ê°œ")
            print(f"   AUC: {lang_auc:.4f}")
            print(f"   Accuracy (ìµœì ): {lang_optimal_acc:.4f}")
            print(f"   Accuracy (0.5): {lang_default_acc:.4f}")
            print(f"   Precision: {lang_precision:.4f}")
            print(f"   Recall: {lang_recall:.4f}")
            print(f"   F1: {lang_f1:.4f}")
            
            # wandb ë¡œê¹…ìš© ë©”íŠ¸ë¦­ ì €ì¥
            language_metrics[f'{lang}_auc'] = lang_auc
            language_metrics[f'{lang}_accuracy_optimal'] = lang_optimal_acc
            language_metrics[f'{lang}_accuracy_default'] = lang_default_acc
            language_metrics[f'{lang}_precision'] = lang_precision
            language_metrics[f'{lang}_recall'] = lang_recall
            language_metrics[f'{lang}_f1'] = lang_f1
            language_metrics[f'{lang}_sample_count'] = len(lang_scores)
            language_metrics[f'{lang}_normal_count'] = normal_count
            language_metrics[f'{lang}_dementia_count'] = dementia_count
        
        print(f"{'='*80}")
        
        # wandbì— ì–¸ì–´ë³„ ë©”íŠ¸ë¦­ ë¡œê¹…
        if self.logger:
            self.logger.experiment.log({
                'language_specific_metrics': language_metrics
            })
    
    def configure_optimizers(self):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•˜ì§€ ì•Šì„ íŒŒë¼ë¯¸í„°ë“¤
        no_decay = ['bias', 'LayerNorm.weight']
        
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ìƒì„±
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': self.hparams.weight_decay,
            },
            {
                'params': [p for n, p in self.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0,
            },
        ]
        
        # ì˜µí‹°ë§ˆì´ì € ì„ íƒ
        if self.hparams.optimizer_type == "lion":
            if not LION_AVAILABLE:
                print("âš ï¸ lion-pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. AdamWë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                optimizer = torch.optim.AdamW(
                    optimizer_grouped_parameters,
                    lr=self.hparams.learning_rate,
                    weight_decay=self.hparams.weight_decay
                )
                print(f"âš¡ AdamW Optimizer ì‚¬ìš© (Lion ëŒ€ì²´): lr={self.hparams.learning_rate}")
            else:
                optimizer = Lion(
                    optimizer_grouped_parameters,
                    lr=self.hparams.learning_rate,
                    weight_decay=self.hparams.weight_decay
                )
                print(f"ğŸ¦ Lion Optimizer ì‚¬ìš© (lion-pytorch): lr={self.hparams.learning_rate}")
        elif self.hparams.optimizer_type == "sam":
            # SAMì€ PyTorch Lightningê³¼ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ ë” ê°•í•œ ì •ê·œí™”ë¥¼ ê°€ì§„ AdamWë¡œ ëŒ€ì²´
            print("âš ï¸ SAMì€ PyTorch Lightningê³¼ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("ğŸ”„ ë” ê°•í•œ ì •ê·œí™”(higher weight decay)ë¥¼ ê°€ì§„ AdamWë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            
            # SAMì˜ ì •ê·œí™” íš¨ê³¼ë¥¼ ëª¨ë°©í•˜ê¸° ìœ„í•´ weight decayë¥¼ ì¦ê°€
            enhanced_weight_decay = self.hparams.weight_decay * 2.0
            
            optimizer = torch.optim.AdamW(
                optimizer_grouped_parameters,
                lr=self.hparams.learning_rate,
                weight_decay=enhanced_weight_decay,
                betas=(0.9, 0.999),
                eps=1e-8
            )
            print(f"âš¡ Enhanced AdamW Optimizer ì‚¬ìš© (SAM ëŒ€ì²´): lr={self.hparams.learning_rate}, wd={enhanced_weight_decay:.4f}")
        else:
            optimizer = torch.optim.AdamW(
                optimizer_grouped_parameters,
                lr=self.hparams.learning_rate,
                weight_decay=self.hparams.weight_decay
            )
            print(f"âš¡ AdamW Optimizer ì‚¬ìš©: lr={self.hparams.learning_rate}")
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.hparams.max_epochs
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val_loss',
            }
        }

def create_model(config) -> SigLIPDementiaClassifier:
    """ëª¨ë¸ ìƒì„±"""
    return SigLIPDementiaClassifier(
        model_name=config.model_name,
        num_classes=2,  # ì¹˜ë§¤ ì—¬ë¶€ (0: ì •ìƒ, 1: ì¹˜ë§¤)
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        warmup_steps=config.warmup_steps,
        max_epochs=config.num_epochs,
        use_language_embedding=True,  # ì–¸ì–´ ë¬´ê´€ í•™ìŠµì„ ìœ„í•´ í™œì„±í™”
        loss_type=config.loss_type,
        focal_alpha=config.focal_alpha,
        focal_gamma=config.focal_gamma,
        optimizer_type=config.optimizer_type,
        sam_rho=config.sam_rho
    )

def create_callbacks(training_config, checkpoint_dir):
    """PyTorch Lightning callbacks ìƒì„±"""
    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
    import os
    
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ModelCheckpoint callback (validation AUC ê¸°ì¤€)
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename="best_model_{epoch:02d}_{val_auc:.3f}",
        monitor="val_auc",
        mode="max",
        save_top_k=1,
        save_last=True,
        verbose=True
    )
    
    # EarlyStopping callback (validation AUC ê¸°ì¤€)
    early_stopping_callback = EarlyStopping(
        monitor="val_auc",
        mode="max",
        patience=getattr(training_config, 'early_stopping_patience', 15),
        verbose=True,
        strict=False  # metricì´ ì—†ì–´ë„ ì˜¤ë¥˜ ë°œìƒ ì•ˆí•¨
    )
    
    # Learning Rate Monitor
    lr_monitor = LearningRateMonitor(logging_interval="epoch")
    
    callbacks = [
        checkpoint_callback,
        early_stopping_callback,
        lr_monitor
    ]
    
    print(f"âœ… Callbacks ìƒì„± ì™„ë£Œ:")
    print(f"   - ModelCheckpoint: validation AUC ê¸°ì¤€ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥")
    print(f"   - EarlyStopping: validation AUC ê¸°ì¤€ {getattr(training_config, 'early_stopping_patience', 15)} epochs patience")
    print(f"   - LearningRateMonitor: í•™ìŠµë¥  ì¶”ì ")
    
    return callbacks, checkpoint_callback 