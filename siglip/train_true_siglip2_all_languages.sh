#!/bin/bash
# ì§„ì •í•œ SigLIP2 - ì „ì²´ ì–¸ì–´ í†µí•© í›ˆë ¨ (PyTorch Lightning)
# EMA Teacher-Student + Self-Distillation + Caption Generation + Multi-Loss

echo "=== ì§„ì •í•œ SigLIP2 ì „ì²´ ì–¸ì–´ í†µí•© í›ˆë ¨ ì‹œì‘ (PyTorch Lightning) ==="
echo "ì‹œì‘ ì‹œê°„: $(date '+%Y-%m-%d %H:%M:%S')"

# ì„¤ì •
DATA_DIR="../../training_dset"
OUTPUT_DIR="../modules/outputs/siglip/True_SigLIP2_All_Languages_Lightning"
MODEL_NAME="google/siglip2-base-patch16-naflex"
BATCH_SIZE=64
LEARNING_RATE=2e-5
NUM_EPOCHS=100
LANGUAGES="English Greek Spanish Mandarin"

# Focal Loss ì„¤ì •
LOSS_TYPE="focal"
FOCAL_ALPHA=1.0
FOCAL_GAMMA=2.0
AUTO_CLASS_WEIGHTS="--auto_class_weights"

# ì˜µí‹°ë§ˆì´ì € ì„¤ì • (PyTorch Lightningì—ì„œëŠ” AdamWë§Œ ì§€ì›)
OPTIMIZER_TYPE="adamw"

# True SigLIP2 Multi-Loss ê°€ì¤‘ì¹˜
EMA_MOMENTUM=0.999
SILC_WEIGHT=0.2      # SILC/TIPS Loss (20%)
SIGMOID_WEIGHT=1.0   # Sigmoid Contrastive Loss (100%)
LOCA_WEIGHT=1.0      # LoCa Caption Loss (100%)
CLASSIFICATION_WEIGHT=1.0  # Classification Loss (100%)

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$OUTPUT_DIR"
mkdir -p "$OUTPUT_DIR/checkpoints"

echo ""
echo "ğŸ”¥ ì§„ì •í•œ SigLIP2 ì „ì²´ ì–¸ì–´ í†µí•© í›ˆë ¨ ì„¤ì • (PyTorch Lightning):"
echo "  í›ˆë ¨ ì–¸ì–´: $LANGUAGES (ëª¨ë“  ì–¸ì–´ í†µí•©)"
echo "  ë°ì´í„° ë””ë ‰í† ë¦¬: $DATA_DIR"
echo "  ì¶œë ¥ ë””ë ‰í† ë¦¬: $OUTPUT_DIR"
echo "  ëª¨ë¸: $MODEL_NAME"
echo "  í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €: google/gemma-2b (256K vocab, multilingual)"
echo "  ë°°ì¹˜ í¬ê¸°: $BATCH_SIZE"
echo "  í•™ìŠµë¥ : $LEARNING_RATE"
echo "  ì—í¬í¬ ìˆ˜: $NUM_EPOCHS"
echo "  ì˜µí‹°ë§ˆì´ì €: $OPTIMIZER_TYPE"
echo "  ì†ì‹¤ í•¨ìˆ˜: $LOSS_TYPE + Multi-Loss"
echo "  Early Stopping: Validation AUC ê¸°ì¤€ 15 epochs patience"
echo ""
echo "ğŸ¯ ì§„ì •í•œ SigLIP2 Multi-Loss êµ¬ì¡°:"
echo "  ğŸ§‘â€ğŸ« EMA Teacher-Student: momentum=$EMA_MOMENTUM"
echo "  ğŸ“š SILC/TIPS Loss: ${SILC_WEIGHT} (Self-Distillation + Masked Prediction)"
echo "  ğŸ”— Sigmoid Loss: ${SIGMOID_WEIGHT} (Cross-Modal Contrastive)"
echo "  ğŸ“ LoCa Loss: ${LOCA_WEIGHT} (Caption + Dense Caption + Referring)"
echo "  ğŸ¯ Classification Loss: ${CLASSIFICATION_WEIGHT} (Dementia Diagnosis)"
echo ""
echo "âœ¨ ì§„ì •í•œ SigLIP2 í•µì‹¬ ê¸°ëŠ¥:"
echo "  âœ… Gemma í† í¬ë‚˜ì´ì €ë¡œ ë‹¤êµ­ì–´ í‘œí˜„ ëŠ¥ë ¥ ê·¹ëŒ€í™” (256K vocab)"
echo "  âœ… EMA Teacher-Student êµ¬ì¡°ë¡œ Self-Distillation"
echo "  âœ… Masked Predictionìœ¼ë¡œ Self-Supervised Learning"
echo "  âœ… Auto-Regressive Decoderë¡œ Caption Generation"
echo "  âœ… Cross-Modal Alignment + Language-Agnostic Learning"
echo "  âœ… Dense Captioning + Referring Expressions"
echo "  âœ… PyTorch Lightningìœ¼ë¡œ ì•ˆì •ì ì¸ í›ˆë ¨"
echo ""

# Python ëª…ë ¹ì–´ í™•ì¸
PYTHON_CMD=""
if command -v python3 &> /dev/null; then
    PYTHON_CMD="python3"
elif command -v python &> /dev/null; then
    PYTHON_CMD="python"
else
    echo "âŒ Pythonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Python 3.8+ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    exit 1
fi

echo "Python ëª…ë ¹ì–´: $PYTHON_CMD"
echo ""

echo "ğŸš€ ì§„ì •í•œ SigLIP2 ì „ì²´ ì–¸ì–´ í†µí•© ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (PyTorch Lightning)..."
echo "âš¡ EMA Teacher-Student + Multi-Loss + Lightningìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!"
echo "================================"

# í›ˆë ¨ ì‹¤í–‰
$PYTHON_CMD true_siglip2_trainer.py \
    --data_dir "$DATA_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_name "$MODEL_NAME" \
    --batch_size $BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --num_epochs $NUM_EPOCHS \
    --parser all \
    --languages $LANGUAGES \
    --optimizer_type "$OPTIMIZER_TYPE" \
    --loss_type "$LOSS_TYPE" \
    --focal_alpha $FOCAL_ALPHA \
    --focal_gamma $FOCAL_GAMMA \
    $AUTO_CLASS_WEIGHTS \
    --ema_momentum $EMA_MOMENTUM \
    --silc_weight $SILC_WEIGHT \
    --sigmoid_weight $SIGMOID_WEIGHT \
    --loca_weight $LOCA_WEIGHT \
    --classification_weight $CLASSIFICATION_WEIGHT

# ê²°ê³¼ í™•ì¸
if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… ì§„ì •í•œ SigLIP2 ì „ì²´ ì–¸ì–´ í†µí•© ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ! (PyTorch Lightning)"
    echo "ì™„ë£Œ ì‹œê°„: $(date '+%Y-%m-%d %H:%M:%S')"
    echo "ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: $OUTPUT_DIR/checkpoints"
    echo ""
    echo "ğŸŒ í›ˆë ¨ ì™„ë£Œëœ ì–¸ì–´: $LANGUAGES"
    echo "ğŸ”¥ ì§„ì •í•œ SigLIP2 + PyTorch Lightning í•µì‹¬ ì„±ê³¼:"
    echo "   âœ… EMA Teacher-Student Self-Distillation"
    echo "   âœ… SILC/TIPS Masked Prediction Learning"
    echo "   âœ… Auto-Regressive Caption Generation"
    echo "   âœ… Cross-Modal Sigmoid Contrastive Learning"
    echo "   âœ… LoCa Dense Captioning + Referring Expressions"
    echo "   âœ… Multi-Loss í†µí•©ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±"
    echo "   âœ… PyTorch Lightningìœ¼ë¡œ ì•ˆì •ì ì´ê³  í™•ì¥ê°€ëŠ¥í•œ í›ˆë ¨"
    echo ""
    echo "ğŸ“Š ì§„ì •í•œ SigLIP2 vs ê¸°ì¡´ ëª¨ë¸ ë¹„êµ:"
    echo "   ğŸ†š ê¸°ì¡´: ë‹¨ìˆœ embedding averaging + basic contrastive"
    echo "   ğŸ”¥ SigLIP2: Teacher-Student + Multi-Loss + Caption Generation"
    echo "   âš¡ Lightning: ìë™ ì²´í¬í¬ì¸íŒ…, Early Stopping, wandb ë¡œê¹…"
    echo "   ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼: ë” ê°•ë ¥í•œ representation + ë” ë‚˜ì€ ì¼ë°˜í™”"
    echo ""
    echo "ğŸ¯ ì´ ëª¨ë¸ì˜ í˜ì‹ ì  ì¥ì :"
    echo "   âœ¨ Self-Supervised Learningìœ¼ë¡œ robust feature í•™ìŠµ"
    echo "   âœ¨ Caption generationìœ¼ë¡œ language understanding ê°•í™”"
    echo "   âœ¨ EMA Teacherë¡œ ì•ˆì •ì ì¸ í•™ìŠµ"
    echo "   âœ¨ Multi-Lossë¡œ ë‹¤ê°ë„ ìµœì í™”"
    echo "   âœ¨ PyTorch Lightningìœ¼ë¡œ production-ready ì½”ë“œ"
else
    echo ""
    echo "âŒ ì§„ì •í•œ SigLIP2 ì „ì²´ ì–¸ì–´ í†µí•© ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨"
    exit 1
fi
