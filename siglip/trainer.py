"""
SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
PyTorch Lightningê³¼ wandb ì‚¬ìš©
"""
import os
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.strategies import DDPStrategy
import wandb
from transformers import AutoProcessor  # SigLIP2 ì§€ì›
import argparse
from datetime import datetime

from config import SigLIPConfig, TrainingConfig
from data_processor import create_dataloaders
from model import create_model, SigLIPDementiaClassifier, create_callbacks
from language_parsers import get_language_parser, parse_all_languages

def setup_wandb(config: SigLIPConfig, training_config: TrainingConfig):
    """wandb ì„¤ì • - ì‹¤í—˜ ì„¤ì •ì´ í¬í•¨ëœ ìƒì„¸í•œ ì´ë¦„ ìƒì„±"""
    # ì‹¤í–‰ ì´ë¦„ ìƒì„± - ì„¤ì • ì •ë³´ í¬í•¨
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ì–¸ì–´ ì •ë³´
    if hasattr(training_config, 'cross_lingual_mode') and training_config.cross_lingual_mode:
        train_langs = "_".join(training_config.train_languages) if training_config.train_languages else "Unknown"
        test_langs = "_".join(training_config.test_languages) if training_config.test_languages else "Unknown"
        lang_info = f"CrossLingual_Train{train_langs}_Test{test_langs}"
    else:
        lang_info = "_".join(config.languages) if len(config.languages) <= 2 else f"{len(config.languages)}langs"
    
    # ëª¨ë¸ ë° ì„¤ì • ì •ë³´
    model_info = config.model_name.split("/")[-1] if "/" in config.model_name else config.model_name
    loss_info = config.loss_type
    opt_info = config.optimizer_type
    
    run_name = f"siglip2_{lang_info}_{model_info}_{loss_info}_{opt_info}_bs{config.batch_size}_lr{config.learning_rate}_{timestamp}"
    
    wandb.init(
        project="dementia-prediction-siglip2",
        name=run_name,
        tags=[
            f"loss_{config.loss_type}",
            f"optimizer_{config.optimizer_type}",
            f"batch_size_{config.batch_size}",
            f"languages_{len(config.languages)}",
            "cross_lingual" if (hasattr(training_config, 'cross_lingual_mode') and training_config.cross_lingual_mode) else "standard"
        ],
        config={
            "model_name": config.model_name,
            "learning_rate": config.learning_rate,
            "batch_size": config.batch_size,
            "num_epochs": config.num_epochs,
            "languages": config.languages,
            "loss_type": config.loss_type,
            "optimizer_type": config.optimizer_type,
            "focal_alpha": config.focal_alpha,
            "focal_gamma": config.focal_gamma,
            "sam_rho": config.sam_rho,
            "sample_rate": config.sample_rate,
            "n_mels": config.n_mels,
            "image_size": config.image_size,
            "max_length": config.max_length,
            "weight_decay": config.weight_decay,
            "warmup_steps": config.warmup_steps,
            "gradient_accumulation_steps": training_config.gradient_accumulation_steps,
            "max_grad_norm": training_config.max_grad_norm,
            "fp16": training_config.fp16,
            "bf16": training_config.bf16,
            # Cross-lingual ì„¤ì •
            "cross_lingual_mode": getattr(training_config, 'cross_lingual_mode', False),
            "train_languages": getattr(training_config, 'train_languages', None),
            "test_languages": getattr(training_config, 'test_languages', None),
        }
    )
    
    # WandbLoggerëŠ” ì´ë¯¸ ì´ˆê¸°í™”ëœ wandb ì‹¤í–‰ì„ ì‚¬ìš©
    return WandbLogger()

def create_callbacks(training_config: TrainingConfig, checkpoint_dir: str):
    """ì½œë°± ìƒì„±"""
    callbacks = []
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (validation AUC ê¸°ì¤€ ë² ìŠ¤íŠ¸ ëª¨ë¸)
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename="siglip2-dementia-best-auc-{val_auc:.3f}-epoch{epoch:02d}",
        monitor="val_auc",  # validation AUC ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
        mode="max",  # AUC ìµœëŒ€ê°’ ì¶”ì 
        save_top_k=1,  # ë² ìŠ¤íŠ¸ ëª¨ë¸ 1ê°œë§Œ ì €ì¥
        save_last=False,  # ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥ ì•ˆí•¨ (ë² ìŠ¤íŠ¸ë§Œ)
        verbose=True,
        auto_insert_metric_name=False  # íŒŒì¼ëª…ì— ë©”íŠ¸ë¦­ ì´ë¦„ ìë™ ì¶”ê°€ ë°©ì§€
    )
    callbacks.append(checkpoint_callback)
    
    # ì¡°ê¸° ì¢…ë£Œ (validation AUC ê¸°ì¤€)
    early_stop_callback = EarlyStopping(
        monitor="val_auc",
        min_delta=training_config.early_stopping_threshold,
        patience=training_config.early_stopping_patience,
        mode="max",
        verbose=True
    )
    callbacks.append(early_stop_callback)
    
    # í•™ìŠµë¥  ëª¨ë‹ˆí„°ë§
    lr_monitor = LearningRateMonitor(logging_interval="step")
    callbacks.append(lr_monitor)
    
    return callbacks, checkpoint_callback

def train_model(config: SigLIPConfig, training_config: TrainingConfig):
    """ëª¨ë¸ í›ˆë ¨"""
    print("=== SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # ì‹œë“œ ì„¤ì •
    pl.seed_everything(config.random_seed)
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        print("GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(config.model_name)  # SigLIP2 ì§€ì›
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    
    # Cross-lingual ëª¨ë“œ í™•ì¸
    cross_lingual_mode = hasattr(training_config, 'cross_lingual_mode') and training_config.cross_lingual_mode
    train_languages = getattr(training_config, 'train_languages', None)
    test_languages = getattr(training_config, 'test_languages', None)
    
    train_loader, val_loader, test_loader = create_dataloaders(
        data_dir=config.data_dir,
        processor=processor,
        config=config,
        cross_lingual_mode=cross_lingual_mode,
        train_languages=train_languages,
        test_languages=test_languages
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_loader.dataset)} ìƒ˜í”Œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_loader.dataset)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_loader.dataset)} ìƒ˜í”Œ")
    
    # ëª¨ë¸ ìƒì„±
    print("ëª¨ë¸ ìƒì„± ì¤‘...")
    model = create_model(config)
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    from data_processor import compute_class_weights
    
    # í›ˆë ¨ ë°ì´í„°ì…‹ì—ì„œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    if hasattr(train_loader.dataset, 'dataset'):
        # Subsetì¸ ê²½ìš° ì›ë³¸ ë°ì´í„°ì…‹ ì ‘ê·¼
        original_dataset = train_loader.dataset.dataset
    else:
        original_dataset = train_loader.dataset
    
    class_weights = compute_class_weights(original_dataset, config)
    model.setup_loss_function(class_weights)
    
    # wandb ì„¤ì •
    print("wandb ì„¤ì • ì¤‘...")
    wandb_logger = setup_wandb(config, training_config)
    
    # ì½œë°± ìƒì„±
    print("ì½œë°± ìƒì„± ì¤‘...")
    callbacks, checkpoint_callback = create_callbacks(training_config, config.checkpoint_dir)
    
    # í›ˆë ¨ ì„¤ì •
    trainer_kwargs = {
        "max_epochs": config.num_epochs,
        "logger": wandb_logger,
        "callbacks": callbacks,
        "accelerator": "auto",
        "devices": "auto",
        "precision": "16-mixed" if training_config.fp16 else "32",
        "gradient_clip_val": training_config.max_grad_norm,
        "accumulate_grad_batches": training_config.gradient_accumulation_steps,
        "log_every_n_steps": 10,  # 32 ë°°ì¹˜ë³´ë‹¤ ì‘ê²Œ ì„¤ì •
        "check_val_every_n_epoch": 1,  # ë§¤ ì—í¬í¬ë§ˆë‹¤ validation
        "enable_progress_bar": True,
        "enable_model_summary": True,
    }
    
    # ë©€í‹° GPU ì„¤ì •
    if torch.cuda.device_count() > 1:
        trainer_kwargs["strategy"] = DDPStrategy(find_unused_parameters=True)  # ë™ì  ë¶„ë¥˜ê¸° ë•Œë¬¸ì— True í•„ìš”
        print(f"ë©€í‹° GPU í›ˆë ¨ ì„¤ì •: {torch.cuda.device_count()}ê°œ GPU ì‚¬ìš©")
    
    # í›ˆë ¨ê¸° ìƒì„±
    trainer = pl.Trainer(**trainer_kwargs)
    
    # í›ˆë ¨ ì‹œì‘
    print("í›ˆë ¨ ì‹œì‘...")
    trainer.fit(model, train_loader, val_loader)
    
    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
    print("ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    best_model_path = checkpoint_callback.best_model_path
    if best_model_path and os.path.exists(best_model_path):
        print(f"âœ… ë² ìŠ¤íŠ¸ ëª¨ë¸ ê²½ë¡œ: {best_model_path}")
        try:
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (classifierê°€ ë¯¸ë¦¬ ìƒì„±ë˜ë¯€ë¡œ ì •ìƒ ë¡œë“œ ê°€ëŠ¥)
            model = SigLIPDementiaClassifier.load_from_checkpoint(best_model_path)
            print("ğŸ† ë² ìŠ¤íŠ¸ AUC ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        except Exception as e:
            print(f"âš ï¸ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("âš ï¸ í˜„ì¬ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.")
    else:
        print("âš ï¸ ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    trainer.test(model, test_loader)
    
    # wandb ì¢…ë£Œ
    wandb.finish()
    
    print("=== í›ˆë ¨ ì™„ë£Œ ===")
    return model, trainer

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨")
    parser.add_argument("--config", type=str, default=None, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_dir", type=str, default="../training_dset", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str, default="../modules/outputs/siglip", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="google/siglip2-base-patch16-224", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=16, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=10, help="ì—í¬í¬ ìˆ˜")
    # ì–¸ì–´ë³„ íŒŒì„œ ì„ íƒ ì˜µì…˜
    parser.add_argument("--parser", type=str, default="all", 
                       choices=["all", "English", "Greek", "Spanish", "Mandarin", "cross_lingual"],
                       help="ì‚¬ìš©í•  ì–¸ì–´ íŒŒì„œ (all: ëª¨ë“  ì–¸ì–´, ê°œë³„ ì–¸ì–´ ì„ íƒ ê°€ëŠ¥, cross_lingual: ì–¸ì–´ ê°„ ì¼ë°˜í™” í…ŒìŠ¤íŠ¸)")
    parser.add_argument("--languages", nargs="+", default=None, help="íŠ¹ì • ì–¸ì–´ ëª©ë¡ (parser=allì¼ ë•Œ ì‚¬ìš©)")
    # Cross-lingual ëª¨ë“œ ì˜µì…˜
    parser.add_argument("--train_languages", nargs="+", default=["English", "Spanish", "Mandarin"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í›ˆë ¨ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    parser.add_argument("--test_languages", nargs="+", default=["Greek"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ì˜µì…˜
    parser.add_argument("--loss_type", type=str, default="cross_entropy",
                       choices=["cross_entropy", "focal", "bce"],
                       help="ì†ì‹¤ í•¨ìˆ˜ íƒ€ì… (cross_entropy, focal, bce)")
    parser.add_argument("--focal_alpha", type=float, default=1.0, help="Focal Loss alpha íŒŒë¼ë¯¸í„°")
    parser.add_argument("--focal_gamma", type=float, default=2.0, help="Focal Loss gamma íŒŒë¼ë¯¸í„°")
    parser.add_argument("--auto_class_weights", action="store_true", help="í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ë³´ì •")
    # ì˜µí‹°ë§ˆì´ì € ì„ íƒ ì˜µì…˜
    parser.add_argument("--optimizer_type", type=str, default="adamw",
                       choices=["adamw", "lion", "sam"],
                       help="ì˜µí‹°ë§ˆì´ì € íƒ€ì… (adamw, lion, sam)")
    parser.add_argument("--sam_rho", type=float, default=0.05, help="SAM rho íŒŒë¼ë¯¸í„°")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SigLIPConfig()
    training_config = TrainingConfig()
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config.data_dir = args.data_dir
    if args.output_dir:
        config.output_dir = args.output_dir
    if args.model_name:
        config.model_name = args.model_name
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.num_epochs:
        config.num_epochs = args.num_epochs
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    if args.loss_type:
        config.loss_type = args.loss_type
    if args.focal_alpha:
        config.focal_alpha = args.focal_alpha
    if args.focal_gamma:
        config.focal_gamma = args.focal_gamma
    config.auto_class_weights = args.auto_class_weights
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    if args.optimizer_type:
        config.optimizer_type = args.optimizer_type
    if args.sam_rho:
        config.sam_rho = args.sam_rho
    
    # ì–¸ì–´ íŒŒì„œ ì„¤ì •
    if args.parser == "cross_lingual":
        # Cross-lingual ëª¨ë“œ
        training_config.cross_lingual_mode = True
        training_config.train_languages = args.train_languages
        training_config.test_languages = args.test_languages
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ë¦„ ì—…ë°ì´íŠ¸
        train_langs_str = "_".join(args.train_languages)
        test_langs_str = "_".join(args.test_languages)
        config.output_dir = f"{config.output_dir}/CrossLingual_Train_{train_langs_str}_Test_{test_langs_str}"
        config.checkpoint_dir = f"{config.output_dir}/checkpoints"
        
        print("ğŸŒ Cross-Lingual ëª¨ë“œ í™œì„±í™”")
        print(f"  í›ˆë ¨ ì–¸ì–´: {args.train_languages}")
        print(f"  í…ŒìŠ¤íŠ¸ ì–¸ì–´: {args.test_languages}")
        print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_dir}")
        
        # config.languagesëŠ” ëª¨ë“  ì–¸ì–´ í¬í•¨ (ë°ì´í„° í™•ì¸ìš©)
        config.languages = args.train_languages + args.test_languages
        
    elif args.parser == "all":
        if args.languages:
            config.languages = args.languages
        else:
            config.languages = ["English", "Greek", "Spanish", "Mandarin"]
    else:
        # ë‹¨ì¼ ì–¸ì–´ ì„ íƒ
        config.languages = [args.parser]
    
    print(f"ì„ íƒëœ ì–¸ì–´: {config.languages}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(config.data_dir):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.data_dir}")
        return
    
    # ì„ íƒëœ ì–¸ì–´ë³„ ë°ì´í„° í™•ì¸
    print("\n=== ì–¸ì–´ë³„ ë°ì´í„° í™•ì¸ ===")
    for language in config.languages:
        try:
            parser = get_language_parser(language, config.data_dir)
            data = parser.parse_data()
            print(f"{language}: {len(data)}ê°œ ìƒ˜í”Œ")
            if data:
                normal_count = sum(1 for d in data if d['label'] == 0)
                dementia_count = sum(1 for d in data if d['label'] == 1)
                print(f"  - ì •ìƒ: {normal_count}ê°œ, ì¹˜ë§¤: {dementia_count}ê°œ")
        except Exception as e:
            print(f"{language}: ì˜¤ë¥˜ - {e}")
    
    # í›ˆë ¨ ì‹¤í–‰
    model, trainer = train_model(config, training_config)
    
    print(f"í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ì´ {config.checkpoint_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 