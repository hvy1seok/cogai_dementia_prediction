"""
SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
PyTorch Lightningê³¼ wandb ì‚¬ìš©
"""
import os
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.strategies import DDPStrategy
import wandb
from transformers import AutoProcessor  # SigLIP2 ì§€ì›
import argparse
from datetime import datetime

from config import SigLIPConfig, TrainingConfig
from data_processor import create_dataloaders
from model import create_model
from language_parsers import get_language_parser, parse_all_languages

def setup_wandb(config: SigLIPConfig, training_config: TrainingConfig):
    """wandb ì„¤ì •"""
    wandb.init(
        project="dementia-prediction-siglip2",
        name=f"siglip2-dementia-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
        config={
            "model_name": config.model_name,
            "learning_rate": config.learning_rate,
            "batch_size": config.batch_size,
            "num_epochs": config.num_epochs,
            "languages": config.languages,
            "sample_rate": config.sample_rate,
            "n_mels": config.n_mels,
            "image_size": config.image_size,
            "max_length": config.max_length,
            "weight_decay": config.weight_decay,
            "warmup_steps": config.warmup_steps,
            "gradient_accumulation_steps": training_config.gradient_accumulation_steps,
            "max_grad_norm": training_config.max_grad_norm,
            "fp16": training_config.fp16,
            "bf16": training_config.bf16,
        }
    )
    
    return WandbLogger(project="dementia-prediction-siglip2")

def create_callbacks(training_config: TrainingConfig, checkpoint_dir: str):
    """ì½œë°± ìƒì„±"""
    callbacks = []
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (AUC ê¸°ì¤€ ë² ìŠ¤íŠ¸ ëª¨ë¸)
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename="siglip2-dementia-best-auc-{test_auc:.3f}-epoch{epoch:02d}",
        monitor="test_auc",  # AUC ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
        mode="max",  # AUC ìµœëŒ€ê°’ ì¶”ì 
        save_top_k=3,  # ìƒìœ„ 3ê°œ ëª¨ë¸ë§Œ ì €ì¥
        save_last=True,  # ë§ˆì§€ë§‰ ëª¨ë¸ë„ ì €ì¥
        verbose=True,
        auto_insert_metric_name=False  # íŒŒì¼ëª…ì— ë©”íŠ¸ë¦­ ì´ë¦„ ìë™ ì¶”ê°€ ë°©ì§€
    )
    callbacks.append(checkpoint_callback)
    
    # ì¡°ê¸° ì¢…ë£Œ ë¹„í™œì„±í™” (validation ì—†ìœ¼ë¯€ë¡œ ì œê±°)
    
    # í•™ìŠµë¥  ëª¨ë‹ˆí„°ë§
    lr_monitor = LearningRateMonitor(logging_interval="step")
    callbacks.append(lr_monitor)
    
    return callbacks

def train_model(config: SigLIPConfig, training_config: TrainingConfig):
    """ëª¨ë¸ í›ˆë ¨"""
    print("=== SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # ì‹œë“œ ì„¤ì •
    pl.seed_everything(config.random_seed)
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        print("GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(config.model_name)  # SigLIP2 ì§€ì›
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    train_loader, test_loader = create_dataloaders(
        data_dir=config.data_dir,
        processor=processor,
        config=config,
        train_split=0.8,
        test_split=0.2
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_loader.dataset)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_loader.dataset)} ìƒ˜í”Œ")
    
    # ëª¨ë¸ ìƒì„±
    print("ëª¨ë¸ ìƒì„± ì¤‘...")
    model = create_model(config)
    
    # wandb ì„¤ì •
    print("wandb ì„¤ì • ì¤‘...")
    wandb_logger = setup_wandb(config, training_config)
    
    # ì½œë°± ìƒì„±
    print("ì½œë°± ìƒì„± ì¤‘...")
    callbacks = create_callbacks(training_config, config.checkpoint_dir)
    
    # í›ˆë ¨ ì„¤ì •
    trainer_kwargs = {
        "max_epochs": config.num_epochs,
        "logger": wandb_logger,
        "callbacks": callbacks,
        "accelerator": "auto",
        "devices": "auto",
        "precision": "16-mixed" if training_config.fp16 else "32",
        "gradient_clip_val": training_config.max_grad_norm,
        "accumulate_grad_batches": training_config.gradient_accumulation_steps,
        "log_every_n_steps": 10,  # 32 ë°°ì¹˜ë³´ë‹¤ ì‘ê²Œ ì„¤ì •
        "check_val_every_n_epoch": None,  # validation ë¹„í™œì„±í™”
        "enable_progress_bar": True,
        "enable_model_summary": True,
    }
    
    # ë©€í‹° GPU ì„¤ì •
    if torch.cuda.device_count() > 1:
        trainer_kwargs["strategy"] = DDPStrategy(find_unused_parameters=True)  # ë™ì  ë¶„ë¥˜ê¸° ë•Œë¬¸ì— True í•„ìš”
        print(f"ë©€í‹° GPU í›ˆë ¨ ì„¤ì •: {torch.cuda.device_count()}ê°œ GPU ì‚¬ìš©")
    
    # í›ˆë ¨ê¸° ìƒì„±
    trainer = pl.Trainer(**trainer_kwargs)
    
    # í›ˆë ¨ ì‹œì‘ (validation ì—†ì´)
    print("í›ˆë ¨ ì‹œì‘...")
    trainer.fit(model, train_loader)
    
    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
    print("ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    best_model_path = checkpoint_callback.best_model_path
    if best_model_path and os.path.exists(best_model_path):
        print(f"âœ… ë² ìŠ¤íŠ¸ ëª¨ë¸ ê²½ë¡œ: {best_model_path}")
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ
        model = SigLIPDementiaClassifier.load_from_checkpoint(best_model_path)
        print("ğŸ† ë² ìŠ¤íŠ¸ AUC ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    else:
        print("âš ï¸ ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    trainer.test(model, test_loader)
    
    # wandb ì¢…ë£Œ
    wandb.finish()
    
    print("=== í›ˆë ¨ ì™„ë£Œ ===")
    return model, trainer

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨")
    parser.add_argument("--config", type=str, default=None, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_dir", type=str, default="../training_dset", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str, default="../modules/outputs/siglip", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="google/siglip2-base-patch16-224", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=16, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=10, help="ì—í¬í¬ ìˆ˜")
    # ì–¸ì–´ë³„ íŒŒì„œ ì„ íƒ ì˜µì…˜
    parser.add_argument("--parser", type=str, default="all", 
                       choices=["all", "English", "Greek", "Spanish", "Mandarin"],
                       help="ì‚¬ìš©í•  ì–¸ì–´ íŒŒì„œ (all: ëª¨ë“  ì–¸ì–´, ê°œë³„ ì–¸ì–´ ì„ íƒ ê°€ëŠ¥)")
    parser.add_argument("--languages", nargs="+", default=None, help="íŠ¹ì • ì–¸ì–´ ëª©ë¡ (parser=allì¼ ë•Œ ì‚¬ìš©)")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SigLIPConfig()
    training_config = TrainingConfig()
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config.data_dir = args.data_dir
    if args.output_dir:
        config.output_dir = args.output_dir
    if args.model_name:
        config.model_name = args.model_name
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.num_epochs:
        config.num_epochs = args.num_epochs
    
    # ì–¸ì–´ íŒŒì„œ ì„¤ì •
    if args.parser == "all":
        if args.languages:
            config.languages = args.languages
        else:
            config.languages = ["English", "Greek", "Spanish", "Mandarin"]
    else:
        # ë‹¨ì¼ ì–¸ì–´ ì„ íƒ
        config.languages = [args.parser]
    
    print(f"ì„ íƒëœ ì–¸ì–´: {config.languages}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(config.data_dir):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.data_dir}")
        return
    
    # ì„ íƒëœ ì–¸ì–´ë³„ ë°ì´í„° í™•ì¸
    print("\n=== ì–¸ì–´ë³„ ë°ì´í„° í™•ì¸ ===")
    for language in config.languages:
        try:
            parser = get_language_parser(language, config.data_dir)
            data = parser.parse_data()
            print(f"{language}: {len(data)}ê°œ ìƒ˜í”Œ")
            if data:
                normal_count = sum(1 for d in data if d['label'] == 0)
                dementia_count = sum(1 for d in data if d['label'] == 1)
                print(f"  - ì •ìƒ: {normal_count}ê°œ, ì¹˜ë§¤: {dementia_count}ê°œ")
        except Exception as e:
            print(f"{language}: ì˜¤ë¥˜ - {e}")
    
    # í›ˆë ¨ ì‹¤í–‰
    model, trainer = train_model(config, training_config)
    
    print(f"í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ì´ {config.checkpoint_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 