"""
ì§„ì •í•œ SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ (PyTorch Lightning ë²„ì „)
- EMA Teacher-Student í•™ìŠµ
- Multi-Loss: SILC/TIPS + Sigmoid + LoCa + Classification
- Caption generation ë° dense captioning
- PyTorch Lightning ê¸°ë°˜ í›ˆë ¨
"""

import os
import sys
import argparse
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
import torch
from transformers import AutoProcessor
from datetime import datetime

from config import SigLIPConfig, TrainingConfig
from true_siglip2_model import create_true_siglip2_model
from data_processor import create_dataloaders

def setup_wandb_logger(config: SigLIPConfig, training_config: TrainingConfig):
    """wandb ë¡œê±° ì„¤ì • - True SigLIP2 ì „ìš©"""
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ì–¸ì–´ ì •ë³´
    lang_info = "_".join(config.languages) if len(config.languages) <= 2 else f"{len(config.languages)}langs"
    
    model_info = "TrueSigLIP2"
    loss_info = f"{config.loss_type}_MultiLoss"
    opt_info = config.optimizer_type
    
    run_name = f"true-siglip2-lightning_{lang_info}_{model_info}_{loss_info}_{opt_info}_bs{config.batch_size}_lr{config.learning_rate}_{timestamp}"
    
    wandb_logger = WandbLogger(
        project="dementia-prediction-true-siglip2-lightning",
        name=run_name,
        tags=[
            f"loss_{config.loss_type}",
            f"optimizer_{config.optimizer_type}",
            f"batch_size_{config.batch_size}",
            f"languages_{len(config.languages)}",
            "true_siglip2",
            "ema_teacher_student",
            "multi_loss",
            "caption_generation",
            "pytorch_lightning"
        ],
        config={
            "model_name": config.model_name,
            "learning_rate": config.learning_rate,
            "batch_size": config.batch_size,
            "num_epochs": getattr(config, 'num_epochs', 100),
            "languages": config.languages,
            "loss_type": config.loss_type,
            "optimizer_type": config.optimizer_type,
            "early_stopping_patience": training_config.early_stopping_patience,
            "ema_momentum": getattr(config, 'ema_momentum', 0.999),
            "silc_weight": getattr(config, 'silc_weight', 0.2),
            "sigmoid_weight": getattr(config, 'sigmoid_weight', 1.0),
            "loca_weight": getattr(config, 'loca_weight', 1.0),
            "classification_weight": getattr(config, 'classification_weight', 1.0),
        }
    )
    
    return wandb_logger

def create_callbacks(training_config: TrainingConfig, checkpoint_dir: str, config: SigLIPConfig):
    """ì½œë°± ìƒì„± - True SigLIP2 ì „ìš©"""
    callbacks = []
    
    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€ì— ë”°ë¥¸ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ê²°ì •
    if hasattr(config, 'best_model_metric'):
        if config.best_model_metric == "avg_lang_auc":
            monitor_metric = 'val_avg_lang_auc'
            filename_template = 'true-siglip2-{epoch:02d}-{val_avg_lang_auc:.3f}'
            print(f"ğŸ“Š ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€: ì–¸ì–´ë³„ í‰ê·  AUC ({config.target_languages})")
        elif config.best_model_metric == "val_macro_f1":
            monitor_metric = 'val_macro_f1'
            filename_template = 'true-siglip2-{epoch:02d}-{val_macro_f1:.3f}'
            print(f"ğŸ“Š ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€: ì „ì²´ Macro F1")
        elif config.best_model_metric == "avg_lang_macro_f1":
            monitor_metric = 'val_avg_lang_macro_f1'
            filename_template = 'true-siglip2-{epoch:02d}-{val_avg_lang_macro_f1:.3f}'
            print(f"ğŸ“Š ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€: ì–¸ì–´ë³„ í‰ê·  Macro F1 ({config.target_languages})")
        else:
            monitor_metric = 'val_auc'
            filename_template = 'true-siglip2-{epoch:02d}-{val_auc:.3f}'
            print(f"ğŸ“Š ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€: ì „ì²´ AUC")
    else:
        monitor_metric = 'val_auc'
        filename_template = 'true-siglip2-{epoch:02d}-{val_auc:.3f}'
        print(f"ğŸ“Š ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€: ì „ì²´ AUC")
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename=filename_template,
        monitor=monitor_metric,
        mode='max',
        save_top_k=3,
        save_last=True,
        verbose=True
    )
    callbacks.append(checkpoint_callback)
    
    # Early Stopping ì½œë°±
    early_stopping = EarlyStopping(
        monitor=monitor_metric,
        mode='max',
        patience=training_config.early_stopping_patience,
        verbose=True,
        min_delta=training_config.early_stopping_threshold
    )
    callbacks.append(early_stopping)
    
    print(f"â³ Early Stopping: {monitor_metric} ê¸°ì¤€ {training_config.early_stopping_patience} epochs patience")
    
    return callbacks

def train_model(config: SigLIPConfig, training_config: TrainingConfig):
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜ - True SigLIP2 PyTorch Lightning"""
    print("=== ì§„ì •í•œ SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (PyTorch Lightning) ===")
    
    # ì‹œë“œ ì„¤ì •
    pl.seed_everything(config.random_seed, workers=True)
    
    # SigLIP2 í”„ë¡œì„¸ì„œ ë° Gemma í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("SigLIP2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(config.model_name)
    
    print("Gemma í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer)
    print(f"âœ… Gemma í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ! Vocab size: {tokenizer.vocab_size}")
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader, test_loader = create_dataloaders(
        data_dir=config.data_dir,
        processor=processor,
        tokenizer=tokenizer,
        config=config,
        cross_lingual_mode=config.cross_lingual_mode,
        train_languages=config.train_languages,
        test_languages=config.test_languages
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_loader.dataset)} ìƒ˜í”Œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_loader.dataset)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_loader.dataset)} ìƒ˜í”Œ")
    
    # True SigLIP2 ëª¨ë¸ ìƒì„±
    print("ì§„ì •í•œ SigLIP2 ëª¨ë¸ ìƒì„± ì¤‘...")
    model = create_true_siglip2_model(config)
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    from data_processor import compute_class_weights
    
    if hasattr(train_loader.dataset, 'dataset'):
        original_dataset = train_loader.dataset.dataset
    else:
        original_dataset = train_loader.dataset
    
    class_weights = compute_class_weights(original_dataset, config)
    model.setup_loss_function(class_weights)
    
    # wandb ë¡œê±° ì„¤ì •
    wandb_logger = setup_wandb_logger(config, training_config)
    
    # ì½œë°± ì„¤ì •
    callbacks = create_callbacks(training_config, config.checkpoint_dir, config)
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    # ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” ì—í­ ë‹¨ìœ„ë¡œ ê²€ì¦ ìˆ˜í–‰
    val_check_interval = None if len(train_loader) < 50 else training_config.eval_steps
    
    trainer = pl.Trainer(
        max_epochs=getattr(config, 'num_epochs', 100),
        logger=wandb_logger,
        callbacks=callbacks,
        accelerator='auto',
        devices='auto',
        precision='16-mixed' if training_config.fp16 else 32,
        gradient_clip_val=training_config.max_grad_norm,
        accumulate_grad_batches=training_config.gradient_accumulation_steps,
        val_check_interval=val_check_interval,
        check_val_every_n_epoch=1 if val_check_interval is None else None,
        log_every_n_steps=min(training_config.logging_steps, len(train_loader)),
        deterministic=False,  # CUDA upsample ì—°ì‚°ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
        enable_progress_bar=True,
        enable_model_summary=True
    )
    
    # ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ì„± ë³´ì¥ (deterministic ì•Œê³ ë¦¬ì¦˜ ëŒ€ì‹ )
    pl.seed_everything(42, workers=True)
    print("âœ… ì‹œë“œ ì„¤ì • ì™„ë£Œ (ì¬í˜„ì„± ë³´ì¥)")
    
    # í›ˆë ¨ ì‹œì‘
    print("ğŸš€ ì§„ì •í•œ SigLIP2 í›ˆë ¨ ì‹œì‘...")
    print("âš¡ EMA Teacher-Student + Multi-Lossë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!")
    trainer.fit(model, train_loader, val_loader)
    
    # í…ŒìŠ¤íŠ¸
    print("ğŸ” ìµœì¢… í…ŒìŠ¤íŠ¸ ìˆ˜í–‰...")
    test_results = trainer.test(model, test_loader, ckpt_path='best')
    
    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ê²½ë¡œ ì¶œë ¥
    best_model_path = callbacks[0].best_model_path
    print(f"\n=== ì§„ì •í•œ SigLIP2 í›ˆë ¨ ì™„ë£Œ ===")
    print(f"ğŸ’¾ ë² ìŠ¤íŠ¸ ëª¨ë¸: {best_model_path}")
    
    return model, best_model_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì§„ì •í•œ SigLIP2 ì¹˜ë§¤ ì§„ë‹¨ ëª¨ë¸ í›ˆë ¨ (PyTorch Lightning)")
    
    # ê¸°ë³¸ ì„¤ì •
    parser.add_argument("--data_dir", type=str, default="../../training_dset", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--output_dir", type=str, default="../modules/outputs/siglip/true-siglip2", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="google/siglip2-base-patch16-naflex", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=32, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=100, help="ì—í¬í¬ ìˆ˜")
    
    # ì–¸ì–´ë³„ íŒŒì„œ ì„ íƒ ì˜µì…˜
    parser.add_argument("--parser", type=str, default="all", 
                       choices=["all", "English", "Greek", "Spanish", "Mandarin"],
                       help="ì‚¬ìš©í•  ì–¸ì–´ íŒŒì„œ")
    parser.add_argument("--languages", nargs="+", default=None, help="íŠ¹ì • ì–¸ì–´ ëª©ë¡")
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ì˜µì…˜
    parser.add_argument("--loss_type", type=str, default="focal",
                       choices=["cross_entropy", "focal", "bce"],
                       help="ì†ì‹¤ í•¨ìˆ˜ íƒ€ì…")
    parser.add_argument("--focal_alpha", type=float, default=1.0, help="Focal Loss alpha íŒŒë¼ë¯¸í„°")
    parser.add_argument("--focal_gamma", type=float, default=2.0, help="Focal Loss gamma íŒŒë¼ë¯¸í„°")
    parser.add_argument("--auto_class_weights", action="store_true", help="í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ë³´ì •")
    
    # ì˜µí‹°ë§ˆì´ì € ì„ íƒ ì˜µì…˜
    parser.add_argument("--optimizer_type", type=str, default="adamw",
                       choices=["adamw"],  # PyTorch Lightningì—ì„œëŠ” AdamWë§Œ ì§€ì›
                       help="ì˜µí‹°ë§ˆì´ì € íƒ€ì…")
    
    # True SigLIP2 ì „ìš© ì˜µì…˜
    parser.add_argument("--ema_momentum", type=float, default=0.999, help="EMA Teacher momentum")
    parser.add_argument("--silc_weight", type=float, default=0.2, help="SILC/TIPS Loss ê°€ì¤‘ì¹˜")
    parser.add_argument("--sigmoid_weight", type=float, default=1.0, help="Sigmoid Loss ê°€ì¤‘ì¹˜")
    parser.add_argument("--loca_weight", type=float, default=1.0, help="LoCa Loss ê°€ì¤‘ì¹˜")
    parser.add_argument("--classification_weight", type=float, default=1.0, help="Classification Loss ê°€ì¤‘ì¹˜")
    
    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€ ì˜µì…˜
    parser.add_argument("--best_model_metric", type=str, default="val_macro_f1", 
                       choices=["val_auc", "val_macro_f1", "avg_lang_auc", "avg_lang_macro_f1"],
                       help="ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€ (val_macro_f1: ì „ì²´ Macro F1, avg_lang_macro_f1: ì–¸ì–´ë³„ í‰ê·  Macro F1)")
    parser.add_argument("--target_languages", nargs="+", default=["English", "Spanish", "Mandarin"],
                       help="avg_lang_auc ëª¨ë“œì—ì„œ í‰ê· ì„ ê³„ì‚°í•  íƒ€ê²Ÿ ì–¸ì–´ë“¤")
    
    # Cross-lingual ëª¨ë“œ ì˜µì…˜
    parser.add_argument("--train_languages", nargs="+", default=["English", "Spanish", "Mandarin"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í›ˆë ¨ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    parser.add_argument("--test_languages", nargs="+", default=["Greek"],
                       help="Cross-lingual ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì–¸ì–´ë“¤")
    
    # ë°ì´í„° ë¶„í•  ë°©ì‹ ì˜µì…˜
    parser.add_argument("--split_by_patient", type=str, default="true", 
                       choices=["true", "false"],
                       help="ë°ì´í„° ë¶„í•  ë°©ì‹ (true: í™˜ì ë‹¨ìœ„, false: ìƒ˜í”Œ ë‹¨ìœ„)")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SigLIPConfig()
    training_config = TrainingConfig()
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    if args.data_dir:
        config.data_dir = args.data_dir
    if args.output_dir:
        config.output_dir = args.output_dir
        config.checkpoint_dir = f"{args.output_dir}/checkpoints"
    if args.model_name:
        config.model_name = args.model_name
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.num_epochs:
        config.num_epochs = args.num_epochs
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
    if args.loss_type:
        config.loss_type = args.loss_type
    if args.focal_alpha:
        config.focal_alpha = args.focal_alpha
    if args.focal_gamma:
        config.focal_gamma = args.focal_gamma
    config.auto_class_weights = args.auto_class_weights
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    if args.optimizer_type:
        config.optimizer_type = args.optimizer_type
    
    # True SigLIP2 ì„¤ì •
    config.ema_momentum = args.ema_momentum
    config.silc_weight = args.silc_weight
    config.sigmoid_weight = args.sigmoid_weight
    config.loca_weight = args.loca_weight
    config.classification_weight = args.classification_weight
    
    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ ê¸°ì¤€ ì„¤ì •
    config.best_model_metric = args.best_model_metric
    config.target_languages = args.target_languages
    config.split_by_patient = args.split_by_patient.lower() == "true"
    
    # Cross-lingual ì„¤ì •
    if args.parser == "cross_lingual":
        config.cross_lingual_mode = True
        config.train_languages = args.train_languages
        config.test_languages = args.test_languages
        config.languages = args.train_languages + args.test_languages
    else:
        config.cross_lingual_mode = False
    
    # ì–¸ì–´ íŒŒì„œ ì„¤ì •
    if args.parser == "all":
        if args.languages:
            config.languages = args.languages
        else:
            config.languages = ["English", "Greek", "Spanish", "Mandarin"]
    else:
        config.languages = [args.parser]
    
    print(f"ì„ íƒëœ ì–¸ì–´: {config.languages}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
    print(f"ì˜µí‹°ë§ˆì´ì €: {config.optimizer_type}")
    print(f"ì†ì‹¤ í•¨ìˆ˜: {config.loss_type}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.output_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    
    # ëª¨ë¸ í›ˆë ¨
    model, best_model_path = train_model(config, training_config)

if __name__ == "__main__":
    main()
